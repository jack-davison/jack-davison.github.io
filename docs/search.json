[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Jack Davison",
    "section": "",
    "text": "rvest\n\n\nwebscraping\n\n\n\n\nOvercoming the lack of a “filter” option on an experience booking website through web scraping.\n\n\n\n\n\n\nNov 13, 2022\n\n\nJack Davison\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntidytuesday\n\n\n\n\nA disucssion on data tidying and cleaning, with applications to a messy, unfamiliar data set.\n\n\n\n\n\n\nFeb 2, 2021\n\n\nJack Davison\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/reports.html",
    "href": "experience/reports.html",
    "title": "Dynamic Reporting",
    "section": "",
    "text": "Some examples of my work include:\n\nAnnual Reports (e.g., UK Hydrocarbons Network Annual Report)\nEpisode Reports (e.g., 2023 West London Particulate Episode Report)\n\nTools in which I am proficient that I use to develop these reports include:\n\n{quarto} (and formerly {rmarkdown}) to render the documents.\n{plotly} and {ggiraph} for interactive plots and figures.\n{leaflet} for interactive maps.\n{ggplot2} and various extensions for static figures/maps.\n{DT} for interactive tables.\n{gt} for static tables.\nmermaid for flowcharts and other diagrams.\n\nScreenshots from a dynamic report are shown in Figure 1.\n\n\n\n\n\n\n\n{ggplot2} figure.\n\n\n\n\n\n\n\nStyled {gt} table.\n\n\n\n\n\n\n\n\n\nInteractive {leaflet} map using the {leaftime} extension.\n\n\n\n\nFigure 1: Screenshots from the 2023 West London Particulate Episode Report."
  },
  {
    "objectID": "experience/research.html",
    "href": "experience/research.html",
    "title": "Research",
    "section": "",
    "text": "\\(Emission~Estimate~\\left(g\\right) = \\textbf{Emission~Factor}~\\left(g~km^{-1}\\right) \\times Measured~Activity~\\left(km\\right)\\)\nTypically these emission factors are based on data from a very limited range of vehicles, most commonly from lab tests but also from instrumented vehicles (Portable Emissions Monitoring Systems, PEMS). While these give good journey coverage, capturing everything from idling to bombing down the motorway, they give poor fleet coverage, only measuring a handful of “representative” vehicles.\nVehicle emission remote sensing is a technique for measuring a lot of vehicles. Think of it as a bit like a speed camera with a spectrometer attached, that being an instrument capable of measuring pollutant concentrations in the atmosphere. While the use of remote sensing rapidly populates a database of hundreds of thousands of tailpipe concentration measurements, only measuring a dispersing plume (rather than the sum total of all tailpipe emissions) means some robust statistical modelling is required to calculate representative emission factors.\nOne of the key outcomes from my research was the clear significance of the difference in emissions between vehicle manufacturers. Figure 1, taken from Davison et al. (2021), illustrates the distribution of different distance-specific emissions for different vehicle manufacturers and engine sizes. These differences are currently not directly accounted for in European emissions inventories.\n\n\n\nFigure 1: Distance-specific CO2 and NOx emissions (g km-1) for Euro 6 light duty vehicles. Each dot represents a unique manufacturer group-engine size combination, with size proportional to the number of observations included in its calculation. The diamonds represent the weighted mean for each engine size, and the horizontal lines the weighted mean for each vehicle category (Diesel Light Commercial Vehicle, Diesel Passenger Car, Gasoline Passenger Car). Taken from Davison et al. (2021).\n\n\nI also used statistical modelling techniques to examine other effects on vehicle emission concentrations. For example, quantile regression (via the {quantreg} package) was used to examine the skewed relationships between cumulative mileage and fuel-specific emissions in passenger cars. Figure 2, taken from Davison et al. (2022), shows that there are a small proportion of high-mileage gasoline Euro 3 passenger cars which are higher emitting than the average Euro 5/6 diesel cars.\n\n\n\nFigure 2: Plot showing the modelled linear deterioration of passenger cars from 0 to 160,000~km of cumulative mileage (a vehicle’s ``normal life’’ under Euro 6 legislation. Taken from Davison et al. (2022).\n\n\n\nAcademic Publications\nPublications I was involved in throughout my PhD are detailed below. The majority of these, not least my thesis (Davison 2022), are open source and therefore free to read for those outside of academia.\n\n\n\n\n\n\nReferences\n\nDavison, Jack. 2022. “New Approaches for Understanding Vehicle Emissions Using Remote Sensing.” University of York. https://etheses.whiterose.ac.uk/31313/.\n\n\nDavison, Jack, Yoann Bernard, Jens Borken-Kleefeld, Naomi J. Farren, Stefan Hausberger, Åke Sjödin, James E. Tate, Adam R. Vaughan, and David C. Carslaw. 2020. “Distance-Based Emission Factors from Vehicle Emission Remote Sensing Measurements.” Science of The Total Environment 739 (October): 139688. https://doi.org/10.1016/j.scitotenv.2020.139688.\n\n\nDavison, Jack, Rebecca A. Rose, Naomi J. Farren, Rebecca L. Wagner, Tim P. Murrells, and David C. Carslaw. 2021. “Verification of a National Emission Inventory and Influence of On-Road Vehicle Manufacturer-Level Emissions.” Environmental Science & Technology 55 (8): 4452–61. https://doi.org/10.1021/acs.est.0c08363.\n\n\nDavison, Jack, Rebecca A. Rose, Naomi J. Farren, Rebecca L. Wagner, Shona E. Wilde, Jasmine V. Wareham, and David C. Carslaw. 2022. “Gasoline and Diesel Passenger Car Emissions Deterioration Using on-Road Emission Measurements and Measured Mileage.” Atmospheric Environment: X 14 (April): 100162. https://doi.org/10.1016/j.aeaoa.2022.100162.\n\n\nFarren, Naomi J., Jack Davison, Rebecca A. Rose, Rebecca L. Wagner, and David C. Carslaw. 2020. “Underestimated Ammonia Emissions from Road Vehicles.” Environmental Science & Technology, December. https://doi.org/10.1021/acs.est.0c05839.\n\n\n———. 2021. “Characterisation of Ammonia Emissions from Gasoline and Gasoline Hybrid Passenger Cars.” Atmospheric Environment: X 11 (October): 100117. https://doi.org/10.1016/j.aeaoa.2021.100117.\n\n\nGrange, Stuart K., Naomi J. Farren, Adam R. Vaughan, Jack Davison, and David C. Carslaw. 2020. “Post-Dieselgate: Evidence of NOx Emission Reductions Using on-Road Remote Sensing.” Environmental Science & Technology Letters 7 (6): 382–87. https://doi.org/10.1021/acs.estlett.0c00188.\n\n\nWagner, Rebecca L., Naomi J. Farren, Jack Davison, Stuart Young, James R. Hopkins, Alastair C. Lewis, David C. Carslaw, and Marvin D. Shaw. 2021. “Application of a Mobile Laboratory Using a Selected-Ion Flow-Tube Mass Spectrometer (SIFT-MS) for Characterisation of Volatile Organic Compounds and Atmospheric Trace Gases.” Atmospheric Measurement Techniques 14 (9): 6083–6100. https://doi.org/10.5194/amt-14-6083-2021."
  },
  {
    "objectID": "experience/teaching.html",
    "href": "experience/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I have delivered training to learners from organisations such as:\n\nUK Department for the Environment, Food and Rural Afairs (Defra)\nUK Health Security Agency (UKHSA)\nUK Environment Agency (EA)\nThe Clean Air Society of Australia and New Zealand (CASANZ)\nPort Talbot Local Authority\nUK Centre for Ecology & Hydrology (UKCEH)\nThe Wolfson Atmospheric Chemistry Laboratories (WACL)\n\n\nApproach\nI am a proponent of live coding in teaching, meaning that learners can see RStudio used in an authentic way. Authentic examples showing real-world applications of R are important to ensure that the content is relevant to learner’s interests. Teaching is supported by extensive, reproducible learning materials written in {quarto} (formerly {rmarkdown}). Where possible on longer courses, case studies are written up using data provided by learners to get them started with their own data analysis projects.\nFigure 1 shows some example course materials produced for a recent course on using R for air quality modelling. An advanced lesson, it outlines a method to use {readr} and {purrr} to rapidly pull large amounts of kilometre grid-square modelled air quality data from the “UK AIR” website and then plot it using {ggplot2}. All of the content is reproducible, allowing learners to explore it further at a later date.\n\n\n\nFigure 1: An example of reproducible course materials, commonly produced when teaching R."
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Authoring dynamic documents using {quarto} and {rmarkdown}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD research into the remote sensing of vehicle emissions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperience teaching R and reproducible data analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack Davison",
    "section": "",
    "text": "Jack Davison is a data analyst and R developer working for an Environmental Consultancy in South Oxfordshire, United Kingdom. He was awarded a PhD in Atmospheric Chemistry from the Wolfson Atmospheric Chemistry Laboratories at the University of York, with research focusing on the calculating emission factors from large databases of road transport emissions. Jack is a collaborator on the {openair} project and the lead developer on the {openairmaps} package. He is a Posit (formerly RStudio) Certified Tidyverse Instructor and has taught R for environmental data analysis to a wide range of audiences, including learners from Defra, the UK Environment Agency, and the UK Health Security Agency."
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#what-are-the-issues",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#what-are-the-issues",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "What are the issues?",
    "text": "What are the issues?\nLet’s read in the data and give it a look over.\nThe bach_students data set indicates…\n\nThe percentage of students broken down by race/ethnicity, aged 25 and over who have attained a bachelor’s degree.\n\nBut in the form it is in, it is pretty untidy and unclean. Here are some issues I can identify:\n\nOur key variables are effectively year, race, total and standard error, but race is spread over multiple columns (remember - each column should represent a variable, not multiple columns representing one variable).\nAll of the columns should be numbers, but they have read in as characters.\nAsian people and Pacific Islanders are given twice - as a total of the two and individually.\nThe names of the columns have spaces in them, are wordy and therefore generally difficult to use.\nThe Total column represents the year. The name is therefore a bit misleading.\n\nA note on the emboldened item above; in my experience, data “in the wild” that has been put together in Excel typically has baggage like this as spreadsheets are sadly used for both data storage and presentation. This means that totals and the like are given alongside the data used to calculate them. I don’t feel bad about removing data like this as long as we are left with the tools to recalculate them. In this case, we are! If we want to regroup the Asian/Pacific Islander data it’d be as easy as renaming them both as the same thing using mutate() and if_else(), and then group_by() and summarise() to re-calculate the total. It may even be that we want to combine more racial groups using a fct_lump() function."
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#cleaning-and-tidying",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#cleaning-and-tidying",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "Cleaning and Tidying",
    "text": "Cleaning and Tidying\nLet’s start by making sure that all of the data is numeric. dplyr v1.0.0 gave us the across() function that makes this a breeze.\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  head(16)\n\n# A tibble: 16 × 19\n   Total `Total, percent of al…` `Standard Erro…` White1 `Standard Erro…` Black1\n   <dbl>                   <dbl>            <dbl>  <dbl>            <dbl>  <dbl>\n 1  1910                     2.7            NA      NA              NA      NA  \n 2  1920                     3.3            NA      NA              NA      NA  \n 3  1930                     3.9            NA      NA              NA      NA  \n 4  1940                     4.6            NA       4.9            NA       1.3\n 5  1950                     6.2            NA       6.6            NA       2.2\n 6  1960                     7.7            NA       8.1            NA       3.5\n 7  1970                    11              NA      11.6            NA       6.1\n 8  1975                    13.9            NA      14.9            NA       6.4\n 9  1980                    17              -0.16   18.4            -0.18    7.9\n10  1985                    19.4            -0.16   20.8            -0.19   11.1\n11  1986                    19.4            -0.16   20.9            -0.19   10.9\n12  1987                    19.9            -0.16   21.4            -0.19   10.8\n13  1988                    20.3            -0.16   21.8            -0.19   11.2\n14  1989                    21.1            -0.16   22.8            -0.19   11.7\n15  1990                    21.3            -0.16   23.1            -0.19   11.3\n16  1991                    21.4            -0.16   23.3            -0.19   11.5\n# … with 13 more variables: `Standard Errors - Black1` <dbl>, Hispanic <dbl>,\n#   `Standard Errors - Hispanic` <dbl>, `Total - Asian/Pacific Islander` <dbl>,\n#   `Standard Errors - Total - Asian/Pacific Islander` <dbl>,\n#   `Asian/Pacific Islander - Asian` <dbl>,\n#   `Standard Errors - Asian/Pacific Islander - Asian` <dbl>,\n#   `Asian/Pacific Islander - Pacific Islander` <dbl>,\n#   `Standard Errors - Asian/Pacific Islander - Pacific Islander` <dbl>, …\n\n\nWe could be tempted now to run janitor::clean_names(). For those unfamiliar, this function cleans the names of a data frame to make them easier to use - all lower-case, spaces replaced with underscores, and the like. Normally it’s a good idea to use it as soon as possible, but as I am going to restructure my data I am not going to use it right away. As race will be one of my columns, I’d probably want words like “White” and “Black” to remain capitalised, so if I reported them in a table or use them in a legend label they’d look more presentable.\nNext, we can start restructuring. A good trick with pivot_longer() is that it behaves much the same as functions like select() - if we list a column name preceded with a minus sign (-) it effectively tells the function “everything but this column, please!” I’m not going to specify column names for the names or values here as we’ll quickly get rid of them.\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>% \n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   <dbl> <chr>                                                             <dbl>\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White1\"                                                           NA  \n 4  1910 \"Standard Errors - White1\"                                         NA  \n 5  1910 \"Black1\"                                                           NA  \n 6  1910 \"Standard Errors - Black1\"                                         NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Total - Asian/Pacific Islander\"                                   NA  \n10  1910 \"Standard Errors - Total - Asian/Pacific Islander\"                 NA  \n11  1910 \"Asian/Pacific Islander - Asian\"                                   NA  \n12  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"                 NA  \n13  1910 \"Asian/Pacific Islander - Pacific Islander\"                        NA  \n14  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Islander\"      NA  \n15  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n16  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n\n\nNow we can remove that Asian/Pacific Islander total we spoke about earlier. We will use filter() alongside the str_detect() function and the logical operator !.\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   <dbl> <chr>                                                             <dbl>\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White1\"                                                           NA  \n 4  1910 \"Standard Errors - White1\"                                         NA  \n 5  1910 \"Black1\"                                                           NA  \n 6  1910 \"Standard Errors - Black1\"                                         NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Asian/Pacific Islander - Asian\"                                   NA  \n10  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"                 NA  \n11  1910 \"Asian/Pacific Islander - Pacific Islander\"                        NA  \n12  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Islander\"      NA  \n13  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 \"Two or more race\"                                                 NA  \n16  1910 \"Standard Errors - Two or more race\"                               NA  \n\n\nTo separate the total values from the standard errors we can use the separate() function and the \" - \" string. Issues once again come from the Asian and Pacific Islander data as they have a second \" - \" in them, but this can be straightforwardly removed. For whatever reason, “White” and “Black” are listed with the number one (1) after them, so we can get rid of this while we’re on the subject of cleaning strings.\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   <dbl> <chr>                                                             <dbl>\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White\"                                                            NA  \n 4  1910 \"Standard Errors - White\"                                          NA  \n 5  1910 \"Black\"                                                            NA  \n 6  1910 \"Standard Errors - Black\"                                          NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Asian\"                                                            NA  \n10  1910 \"Standard Errors - Asian\"                                          NA  \n11  1910 \"Pacific Islander\"                                                 NA  \n12  1910 \"Standard Errors - Pacific Islander\"                               NA  \n13  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 \"Two or more race\"                                                 NA  \n16  1910 \"Standard Errors - Two or more race\"                               NA  \n\n\nNow let’s add that separate() step, which we need to give some column names for the name column to turn into, the separating character (space-dash-space), and the direction to fill in (in this case left).\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\n  head(16)\n\n# A tibble: 16 × 4\n   Total stat            race                                            value\n   <dbl> <chr>           <chr>                                           <dbl>\n 1  1910 <NA>            \"Total, percent of all persons age 25 and over\"   2.7\n 2  1910 Standard Errors \"Total, percent of all persons age 25 and over\"  NA  \n 3  1910 <NA>            \"White\"                                          NA  \n 4  1910 Standard Errors \"White\"                                          NA  \n 5  1910 <NA>            \"Black\"                                          NA  \n 6  1910 Standard Errors \"Black\"                                          NA  \n 7  1910 <NA>            \"Hispanic\"                                       NA  \n 8  1910 Standard Errors \"Hispanic\"                                       NA  \n 9  1910 <NA>            \"Asian\"                                          NA  \n10  1910 Standard Errors \"Asian\"                                          NA  \n11  1910 <NA>            \"Pacific Islander\"                               NA  \n12  1910 Standard Errors \"Pacific Islander\"                               NA  \n13  1910 <NA>            \"American Indian/\\r\\nAlaska Native\"              NA  \n14  1910 Standard Errors \"American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 <NA>            \"Two or more race\"                               NA  \n16  1910 Standard Errors \"Two or more race\"                               NA  \n\n\nYou’ll notice that in the stats column we have some NA values that actually correspond to the “total” stat, so we’ll fill those in using a tidyr function, replace_na().\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\n  mutate(stat = replace_na(stat, \"Total\")) %>%\n  head(16)\n\n# A tibble: 16 × 4\n   Total stat            race                                            value\n   <dbl> <chr>           <chr>                                           <dbl>\n 1  1910 Total           \"Total, percent of all persons age 25 and over\"   2.7\n 2  1910 Standard Errors \"Total, percent of all persons age 25 and over\"  NA  \n 3  1910 Total           \"White\"                                          NA  \n 4  1910 Standard Errors \"White\"                                          NA  \n 5  1910 Total           \"Black\"                                          NA  \n 6  1910 Standard Errors \"Black\"                                          NA  \n 7  1910 Total           \"Hispanic\"                                       NA  \n 8  1910 Standard Errors \"Hispanic\"                                       NA  \n 9  1910 Total           \"Asian\"                                          NA  \n10  1910 Standard Errors \"Asian\"                                          NA  \n11  1910 Total           \"Pacific Islander\"                               NA  \n12  1910 Standard Errors \"Pacific Islander\"                               NA  \n13  1910 Total           \"American Indian/\\r\\nAlaska Native\"              NA  \n14  1910 Standard Errors \"American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 Total           \"Two or more race\"                               NA  \n16  1910 Standard Errors \"Two or more race\"                               NA  \n\n\nNow we can pivot_wider() to get the total values and the standard errors in their own columns. We’ll have to rename the existing Total column first, but that can be achieved using dplyr’s rename().\n\nbach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\n  mutate(stat = replace_na(stat, \"Total\")) %>%\n  rename(year = Total) %>%\n  pivot_wider(names_from = stat, values_from = value) %>%\n  head(8)\n\n# A tibble: 8 × 4\n   year race                                            Total `Standard Errors`\n  <dbl> <chr>                                           <dbl>             <dbl>\n1  1910 \"Total, percent of all persons age 25 and over\"   2.7                NA\n2  1910 \"White\"                                          NA                  NA\n3  1910 \"Black\"                                          NA                  NA\n4  1910 \"Hispanic\"                                       NA                  NA\n5  1910 \"Asian\"                                          NA                  NA\n6  1910 \"Pacific Islander\"                               NA                  NA\n7  1910 \"American Indian/\\r\\nAlaska Native\"              NA                  NA\n8  1910 \"Two or more race\"                               NA                  NA\n\n\nThe data is now tidy! We can do some additional cleaning steps now - there seems to be something odd going on with the American Indian/Alaska Native string, and the “Total” string is a bit wordy. Let’s sort that out, and finally throw in that janitor function I talked about right at the beginning.\n\ndf = bach_students %>%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %>%\n  pivot_longer(-Total) %>%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\n  mutate(stat = replace_na(stat, \"Total\")) %>%\n  rename(year = Total) %>%\n  pivot_wider(names_from = stat, values_from = value) %>%\n  janitor::clean_names() %>%\n  mutate(\n    race = str_remove_all(\n      race, \n      \", percent of all persons age 25 and over|\\r\\n\")\n  )\n\ndf %>% head(8)\n\n# A tibble: 8 × 4\n   year race                          total standard_errors\n  <dbl> <chr>                         <dbl>           <dbl>\n1  1910 Total                           2.7              NA\n2  1910 White                          NA                NA\n3  1910 Black                          NA                NA\n4  1910 Hispanic                       NA                NA\n5  1910 Asian                          NA                NA\n6  1910 Pacific Islander               NA                NA\n7  1910 American Indian/Alaska Native  NA                NA\n8  1910 Two or more race               NA                NA\n\n\nWe did it! The data is now clean and tidy and ready to use. Let’s do a bit of analysis just to demonstrate how straightforward it is to use now."
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#data-analysis",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#data-analysis",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "Data Analysis",
    "text": "Data Analysis\nWe can create some cool plots now we have access to this data. We can start with a simple timeseries.\n\ntheme_set(theme_light())\n\nplot_data = df %>%\n  drop_na() %>%\n  mutate(across(total:standard_errors, ~.x/100))\n\nplot_data %>%\n  filter(race != \"Total\") %>%\n  mutate(race = fct_reorder(race, total, max, na.rm = T),\n         race = fct_rev(race)) %>%\n  ggplot(aes(\n    year,\n    y = total,\n    ymax = total + standard_errors,\n    ymin = total - standard_errors,\n    group = race\n  )) +\n  geom_ribbon(aes(fill = race), alpha = .25) +\n  geom_line(aes(color = race)) +\n  geom_line(data = filter(plot_data, race == \"Total\"), size = 2) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Year\", y = NULL, color = \"Race\", fill = \"Race\",\n       title = \"Bachelor's Degree Attainment\",\n       subtitle = \"The percentage of the population who have achieved a\\nbachelor's degree in the US since 1980, split into racial groups.\\nThe bold line represents the total population.\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\nOr we could just focus in on the most recent data.\n\nplot_data_2 = plot_data %>%\n  filter(year == max(year)) %>%\n  mutate(race = fct_reorder(race, total))\n\ntot = plot_data_2 %>%\n  filter(race == \"Total\") %>% \n  pull(total)\n\nplot_data_2 %>%\n  mutate(flag = case_when(race == \"Total\" ~ \"T\",\n                          total > tot ~ \"Y\",\n                          total < tot ~ \"X\")) %>%\n  ggplot(aes(y = race, x = total, xmax = total+standard_errors, xmin = total-standard_errors, fill = flag)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = tot, size = 1) +\n  geom_pointrange(show.legend = F) +\n  scale_x_continuous(expand = expansion(mult = c(0,.1)), \n                     labels = scales::percent) +\n  labs(y = NULL, x = \"Bachelor's Degree Attainment in 2016\")\n\n\n\n\n\nplot_data_2 %>%\n  janitor::remove_constant() %>%\n  mutate(mutate(across(\n    where(is.numeric), ~ glue::glue(\"{abs(.x) * 100} %\")\n  ))) %>%\n  knitr::kable(col.names = c(\"Race\", \"Total\", \"Std Err.\")) %>%\n  kableExtra::kable_styling()\n\n\n\n Race \n    Total \n    Std Err. \n  \n\n\n Total \n    33.4 % \n    0.24 % \n  \n\n White \n    37.3 % \n    0.31 % \n  \n\n Black \n    23.5 % \n    0.46 % \n  \n\n Hispanic \n    16.4 % \n    0.4 % \n  \n\n Asian \n    56.4 % \n    0.89 % \n  \n\n Pacific Islander \n    27.5 % \n    2.92 % \n  \n\n American Indian/Alaska Native \n    16.8 % \n    1.39 % \n  \n\n Two or more race \n    30.6 % \n    1.52 % \n  \n\n\n\n\nSee how easy it is to use this data now? Incredible!"
  },
  {
    "objectID": "posts/2022-11-13-pastaEvangelists/index.html",
    "href": "posts/2022-11-13-pastaEvangelists/index.html",
    "title": "Scraping the Pasta Academy",
    "section": "",
    "text": "Purpose\nRecently, my partner and I were looking to book a cooking experience as a Christmas treat, and we settled on the excellently reviewed Pasta Academy in London. We were looking for a beginner class in early 2023, but we were immediately met with an issue — there was no way to filter the booking list!\nWe could have sat and go through the list manually, perhaps noting down each of the beginner classes when we found them. This sounded like a bit of a faff, so I instead thought I’d scrape all of the courses and put them into a table. Some might argue that writing an R script to do this is even more of a faff, but I did it anyway!\nThis blog post shows how I scraped the data from the Pasta Academy website using rvest and then went on to tidy it using the tidyverse. You may find this a nice introduction to web scraping in R, and a nice application of data skills in “real life”.\nWeb Scraping with {rvest}\n\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nWe’ll start by scraping the first page. To start using rvest, we first need to define a session. This simulates a user interacting with a website.\n\npasta_session <-\n  session(\"https://pastaevangelists.com/collections/pasta-academy?page=1\")\n\nWe’ll now need to pull each individual element of interest from the Pasta Academy website. One of the easiest ways of finding the correct HTML at which to aim your rvest functions is by using Chrome’s developer tools (CTRL-SHIFT-I on Windows) or your browser’s equivalent.\n\n\nFigure 1: A screenshot of the use of Chrome’s developer tools.\n\n\nOnce we have identified the correct HTML tags to use, we can use html_elements() to grab those items, and then html_text2() to extract the text from it. For example, the below line grabs each lesson title from the first page of the Pasta Academy website.\n\nhtml_elements(pasta_session, \".product-item-row__meta-title\") |> html_text2()\n\n [1] \"PASTA ACADEMY™ | TASTE OF NAPOLI |\\r\"                           \n [2] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                         \n [3] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                           \n [4] \"PASTA ACADEMY™ | TASTE OF AMALFI |\\r\"                           \n [5] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                         \n [6] \"PASTA ACADEMY™ | TASTE OF ROME |\\r\"                             \n [7] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                           \n [8] \"PASTA ACADEMY™ | TASTE OF PUGLIA |\\r\"                           \n [9] \"PASTA ACADEMY™ | LIMITED EDITION WINTER TRUFFLE MASTERCLASS |\\r\"\n[10] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                           \n[11] \"PASTA ACADEMY™ | TASTE OF PUGLIA |\\r\"                           \n[12] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                         \n[13] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                           \n[14] \"PASTA ACADEMY™ | TASTE OF VENICE |\\r\"                           \n[15] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                         \n[16] \"PASTA ACADEMY™ | TASTE OF SARDINIA |\\r\"                         \n[17] \"PASTA ACADEMY™ | TASTE OF ROME |\\r\"                             \n[18] \"PASTA ACADEMY™ | TASTE OF BOLOGNA |\\r\"                          \n[19] \"PASTA ACADEMY™ | LIMITED EDITION SEAFOOD PASTA MASTERCLASS |\\r\" \n[20] \"PASTA ACADEMY™ | LIMITED EDITION SEAFOOD PASTA MASTERCLASS |\\r\" \n\n\nAll we need to do is repeat this until we have all of the information we want. In this case, all I need is the course title, the date/time, the price, and whether or not it is sold out. The button text is a useful proxy for that last item, as it reads “SEE INFO” if a course is fully booked and “BOOK NOW” if there are spaces left.\n\nname <- html_elements(pasta_session, \".product-item-row__meta-title\") |> html_text2()\n\ndate <- html_elements(pasta_session, \".product-item-row__meta-title-sub\") |> html_text2()\n\nprice <- html_elements(pasta_session, \"span.price\") |> html_text2()\n\nbutton <- html_elements(pasta_session, \".product-item-row__cta-btn\") |> html_text2()\n\ndplyr::tibble(\n  name = name, \n  date = date,\n  price = price, \n  button = button\n) |>\n  dplyr::glimpse()\n\nRows: 20\nColumns: 4\n$ name   <chr> \"PASTA ACADEMY™ | TASTE OF NAPOLI |\\r\", \"PASTA ACADEMY™ | MORNI…\n$ date   <chr> \"\\r Friday February 24th, 2023, 18:30\\r\", \"\\r Saturday February…\n$ price  <chr> \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£6…\n$ button <chr> \"BOOK NOW\", \"SEE INFO\", \"SEE INFO\", \"SEE INFO\", \"SEE INFO\", \"BO…\n\n\nNow all four pieces of information can be scraped from one page, we can use purrr to scrape this information from all of the pages. We’ll write a function which takes one argument, id, which is used to select the specific page of the Pasta Academy website.\n\nscrape_pasta <- function(id){\n  x <- session(str_glue(\"https://pastaevangelists.com/collections/pasta-academy?page={id}\"))\n  \n  tibble(\n    name = html_elements(x, \".product-item-row__meta-title\") |> html_text2(),\n    date = html_elements(x, \".product-item-row__meta-title-sub\") |> html_text2(),\n    price = html_elements(x, \"span.price\") |> html_text2(),\n    fully_booked = html_elements(x, \".product-item-row__cta-btn\") |> html_text2()\n  ) |>\n    mutate(page_id = id)\n}\n\nraw_pasta <- map_dfr(1:19, scrape_pasta)\n\nTidying Data\nWe wouldn’t eat raw pasta, and we won’t want to work with raw_pasta as it currently exists. Lets use the tidyverse to tidy this data up a bit.\nFirst, lets get rid of some of the dodgy formatting — we’ll drop the \\r, vertical bars, question marks, and the PASTA ACADEMY branding from all character columns.\n\npasta <- \n  raw_pasta |>\n  mutate(across(where(is.character), \n                ~ str_remove_all(.x, \"PASTA ACADEMY™|\\r|\\\\||\\\\?\") |> \n                  str_squish()))\npasta\n\n# A tibble: 146 × 5\n   name                                       date         price fully…¹ page_id\n   <chr>                                      <chr>        <chr> <chr>     <int>\n 1 TASTE OF NAPOLI                            Friday Febr… £65.… BOOK N…       1\n 2 MORNING BEGINNERS                          Saturday Fe… £65.… SEE IN…       1\n 3 BEGINNERS CLASS                            Saturday Fe… £65.… SEE IN…       1\n 4 TASTE OF AMALFI                            Saturday Fe… £65.… SEE IN…       1\n 5 MORNING BEGINNERS                          Sunday Febr… £65.… SEE IN…       1\n 6 TASTE OF ROME                              Sunday Febr… £65.… BOOK N…       1\n 7 BEGINNERS CLASS                            Sunday Febr… £65.… SEE IN…       1\n 8 TASTE OF PUGLIA                            Monday Febr… £65.… SEE IN…       1\n 9 LIMITED EDITION WINTER TRUFFLE MASTERCLASS Tuesday Feb… £65.… BOOK N…       1\n10 BEGINNERS CLASS                            Wednesday M… £65.… SEE IN…       1\n# … with 136 more rows, and abbreviated variable name ¹​fully_booked\n\n\nAn easier step — lets format the price as numeric data, and fully_booked as logical. Lets also make name all lower-case.\n\npasta <-\n  pasta |> \n  mutate(name = tolower(name),\n         price = parse_number(price),\n         fully_booked = if_else(fully_booked == \"SEE INFO\", T, F))\n\npasta\n\n# A tibble: 146 × 5\n   name                                       date         price fully…¹ page_id\n   <chr>                                      <chr>        <dbl> <lgl>     <int>\n 1 taste of napoli                            Friday Febr…    65 FALSE         1\n 2 morning beginners                          Saturday Fe…    65 TRUE          1\n 3 beginners class                            Saturday Fe…    65 TRUE          1\n 4 taste of amalfi                            Saturday Fe…    65 TRUE          1\n 5 morning beginners                          Sunday Febr…    65 TRUE          1\n 6 taste of rome                              Sunday Febr…    65 FALSE         1\n 7 beginners class                            Sunday Febr…    65 TRUE          1\n 8 taste of puglia                            Monday Febr…    65 TRUE          1\n 9 limited edition winter truffle masterclass Tuesday Feb…    65 FALSE         1\n10 beginners class                            Wednesday M…    65 TRUE          1\n# … with 136 more rows, and abbreviated variable name ¹​fully_booked\n\n\nIt’d be useful to have the date as a properly formatted date-time column. Sadly, this column is not consistently formatted. The two issues are:\n\nSometimes the time has a comma before it, but not always\nSometimes the year is present, but not always\n\nTo deal with this, we’ll take the following steps:\n\nExtract the time & year from the date using regex.\nFill any missing years. This might not be perfect if there are missing years between December and January, but for our purposes we can live with this and can cross-reference with page_id if we need to double check.\nReformat the date by performing string transformations.\nParse the date as a date-time using lubridate.\n\nIn practice, this looks like this:\n\npasta <-\n  pasta |>\n  \n  # extract year/time\n  mutate(\n    time = str_extract(date, \"[0-9][0-9]:[0-9][0-9]\"),\n    year = str_extract(date, \"[0-9][0-9][0-9][0-9]\")\n  ) |>\n  \n  # fill year\n  fill(year, .direction = \"down\") |>\n  \n  # reformat date \n  mutate(date = str_remove_all(date, time) |>\n           str_remove_all(year) |>\n           str_remove(\", \") |>\n           str_squish() |> \n           str_remove(\", am\")) |> \n  separate(date, c(\"day\", \"date\"), sep = \" \", extra = \"merge\") |> \n  unite(date, date, year, time, sep = \" \") |> \n  mutate(date = str_remove(date, \",\")) |> \n  \n  # parse date as date-time\n  mutate(date = lubridate::parse_date_time(date, \n                                           orders = c(\"BdY HM\", \"dBY HM\"))) |>\n  \n  # drop date\n  select(-day)\n\npasta\n\n# A tibble: 146 × 5\n   name                                date                price fully…¹ page_id\n   <chr>                               <dttm>              <dbl> <lgl>     <int>\n 1 taste of napoli                     2023-02-24 18:30:00    65 FALSE         1\n 2 morning beginners                   2023-02-25 10:00:00    65 TRUE          1\n 3 beginners class                     2023-02-25 13:00:00    65 TRUE          1\n 4 taste of amalfi                     2023-02-25 18:30:00    65 TRUE          1\n 5 morning beginners                   2023-02-26 10:00:00    65 TRUE          1\n 6 taste of rome                       2023-02-26 13:00:00    65 FALSE         1\n 7 beginners class                     2023-02-26 18:30:00    65 TRUE          1\n 8 taste of puglia                     2023-02-27 18:30:00    65 TRUE          1\n 9 limited edition winter truffle mas… 2023-02-28 18:30:00    65 FALSE         1\n10 beginners class                     2023-03-01 18:30:00    65 TRUE          1\n# … with 136 more rows, and abbreviated variable name ¹​fully_booked\n\n\nUsing the data\nNow that the data is in a tidy format, lets find out when the available beginner courses are being held. Evenings are also our preference:\n\npotential_classes <- \n  pasta |> \n  filter(str_detect(name, \"beg\"),\n         lubridate::hour(date) > 14,\n         !fully_booked)\n\npotential_classes\n\n# A tibble: 12 × 5\n   name            date                price fully_booked page_id\n   <chr>           <dttm>              <dbl> <lgl>          <int>\n 1 beginners class 2023-03-14 18:30:00    65 FALSE              2\n 2 beginners class 2023-03-15 18:30:00    65 FALSE              2\n 3 beginners class 2023-03-21 18:30:00    65 FALSE              2\n 4 beginners class 2023-04-02 18:30:00    65 FALSE              3\n 5 beginners class 2023-04-12 18:30:00    65 FALSE              4\n 6 beginners class 2023-04-24 18:30:00    65 FALSE              5\n 7 beginners class 2023-05-05 18:30:00    65 FALSE              5\n 8 beginners class 2023-05-16 18:30:00    65 FALSE              6\n 9 beginners class 2023-05-20 18:30:00    65 FALSE              6\n10 beginners class 2023-05-27 18:30:00    65 FALSE              6\n11 beginners class 2023-06-09 18:30:00    65 FALSE              7\n12 beginners class 2023-06-25 18:30:00    65 FALSE              8\n\n\nAnd there they are!\nJust as we now have the data to hand, lets see what some common themes are in the classes. Disregarding words that aren’t particularly unique or descriptive like “taste”, “beginners”, “class”, “morning”, we can learn that there seems to be a lot inspired by the cuisines of Puglia, Rome, and Sardinia.\n\ncounts <-\n  pasta |> \n  tidytext::unnest_tokens(word, name) |> \n  anti_join(tidytext::get_stopwords()) |> \n  count(word, sort = T)\n\ncounts\n\n# A tibble: 32 × 2\n   word            n\n   <chr>       <int>\n 1 taste          81\n 2 beginners      54\n 3 class          28\n 4 morning        27\n 5 edition        12\n 6 limited        12\n 7 masterclass    11\n 8 puglia         11\n 9 rome           10\n10 sardinia       10\n# … with 22 more rows\n\n\nFigure 2 visualises the above data using ggiraph, showing the most and least common words in these pasta making class names. Hover over the bars to read the exact values!\n\nlibrary(ggiraph)\n\nplt <-\n  counts |>\n  mutate(word = fct_reorder(word, n)) |>\n  ggplot(aes(y = word, x = n)) +\n  geom_col_interactive(aes(fill = n, tooltip = n), show.legend = FALSE) +\n  theme_classic() +\n  scale_x_continuous(expand = expansion()) +\n  labs(y = NULL, x = \"Count (n)\")\n\ngirafe(ggobj = plt)\n\n\nFigure 2: Frequency of different words in Pasta Academy class names."
  }
]