[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "“Put it on a map!” Developments in air quality data analysis\n\n\n\n\n\nA public talk given as part of SatRdays London 2023.\n\n\n\n\n\n\nApr 23, 2023\n\n\nSatRdays London\n\n\n\n\n\n\n  \n\n\n\n\nNew Approaches for Understanding Vehicle Emissions Using Remote Sensing\n\n\n\n\n\nPresented as part of the Kathleen Mary Stott Prizewinners Seminar.\n\n\n\n\n\n\nOct 6, 2021\n\n\nDepartment of Chemistry, University of York\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2021_kmschemistry.html",
    "href": "talks/2021_kmschemistry.html",
    "title": "New Approaches for Understanding Vehicle Emissions Using Remote Sensing",
    "section": "",
    "text": "Access\n\nSlides were produced in PowerPoint and are available on request.\nContent was centred around my PhD thesis (Davison 2022).\n\n\n\nAbstract\nN/A\n\n\nContext\nI was one of three PhD student recipients of the 2021 Kathleen Mary Stott Prize, awarded by the Department of Chemistry, University of York. My research can be read about on my research page or in my publicly available thesis (Davison 2022).\n\n\n\n\n\nReferences\n\nDavison, Jack. 2022. “New Approaches for Understanding Vehicle Emissions Using Remote Sensing.” University of York. https://etheses.whiterose.ac.uk/31313/."
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html",
    "href": "posts/2023-02-20-tinyCategories/index.html",
    "title": "Visualising Uncommon Factors",
    "section": "",
    "text": "library(readr) # read rectangular data\nlibrary(dplyr) # data manipulation\nlibrary(tidyr) # data tidying\nlibrary(ggplot2) # plotting\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#normalise",
    "href": "posts/2023-02-20-tinyCategories/index.html#normalise",
    "title": "Visualising Uncommon Factors",
    "section": "Normalise",
    "text": "Normalise\nOne way to overcome the unreadability of some of the smaller categories is to normalise them somehow. For example, we could normalise each of the sectors to their earliest value (in this case, their 1990 value) to have a better feel for the trend. We could have alternatively normalised them to their max() or mean() values.\nAdvantage: All of the sectors are now readable.\nDisadvantage: We’ve lost the “absolute” values and can now only see the trend and “relative” values.\nHow: We add an extra mutate() step which divides all of the values in each group by their first() value.\n\ncopper |&gt;\n  mutate(value = value / first(value), .by = Sectors) |&gt;\n  ggplot(aes(x = year, y = value, color = Sectors)) +\n  geom_line() +\n  geom_point() +\n  labs(x = NULL, y = \"Change in Annual UK Copper Emissions,\\nNormalised to 1990\", color = NULL) +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"top\")\n\n\n\nFigure 2: Each of the sectors have been normalised relative to their first value in 1990."
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#lump",
    "href": "posts/2023-02-20-tinyCategories/index.html#lump",
    "title": "Visualising Uncommon Factors",
    "section": "Lump",
    "text": "Lump\nTo overcome the disadvantage above (and return to a bar chart) we could use forcats to “lump” some of the smaller categories together into one “Other” category.\nAdvantage: Each of the legend items are now readable and distinct - no more tiny little bars that can’t be discerned right at the bottom. Absolute values are shown.\nDisadvantage: Not all of the sectors have a legend item now. Some readers may skip the caption or body text and not acknowledge what “Other” is composed of. We’ve also lost the specific values for the individual “Other” sectors (not that they could really be read before!)\nHow: We can use a function from the forcats::fct_lump() family to “lump” together the smaller sectors. In this case we use forcats::fct_lump_lowfreq(), which makes sure the “Other” category is always the smallest one. Optionally, we can extract the sectors that make up “Other” and list them in the plot caption (or the body of our report).\n\ncopper2 &lt;-\n  mutate(copper,\n         sector_lumped = forcats::fct_lump_lowfreq(Sectors, w = value, other_level = \"Other*\"))\n\nother_cats &lt;-\n  filter(copper2, sector_lumped == \"Other*\") |&gt;\n  distinct(Sectors) |&gt;\n  pull() |&gt;\n  paste(collapse = \", \")\n\nggplot(copper2, aes(x = year, y = value)) +\n  geom_col(aes(fill = sector_lumped)) +\n  coord_cartesian(expand = FALSE) +\n  theme(legend.position = \"top\") +\n  labs(x = NULL, y = \"Annual UK Copper Emissions (kt)\",\n       fill = NULL,\n       caption = stringr::str_wrap(paste0(\"*\", other_cats))) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"top\")\n\n\n\nFigure 3: Lumping the small categories makes the plot easier to read, but we’ve lost the individual values from the ‘Other’ sectors."
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#scale-transform",
    "href": "posts/2023-02-20-tinyCategories/index.html#scale-transform",
    "title": "Visualising Uncommon Factors",
    "section": "Scale Transform",
    "text": "Scale Transform\nIf we want a bar chart and to retain all the individual sectors, we could choose to use a scale transform. A common scale transform is the log-transform, but there are plenty out there. For example, we could use a square-root axis.\nAdvantage: Each of the individual sectors are now visually distinct from one another.\nDisadvantage: Scale transformed axes are harder to read. Many people aren’t familiar with the concept at all - though some would question if that always really matters.\nHow: ggplot2 makes it easy to transform scales. All of the continuous scales_*_*() functions have the “trans” argument which can transform the axes however we like. There are even short-cuts for the x and y axes, like scale_y_sqrt() and scale_y_log10().\n\nggplot(copper, aes(x = year, y = value)) +\n  geom_col(aes(fill = Sectors)) +\n  coord_cartesian(expand = FALSE) +\n  labs(x = NULL, y = \"Annual UK Copper Emissions (kt)\", fill = NULL) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_y_sqrt(breaks = c(0, 0.1, 0.25, 0.5, 1, 2)) +\n  theme(legend.position = \"top\")\n\n\n\nFigure 4: The y-axis is now on a transformed scale; all the categories stand out, but it isn’t necessarily easy to read."
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#zoom",
    "href": "posts/2023-02-20-tinyCategories/index.html#zoom",
    "title": "Visualising Uncommon Factors",
    "section": "Zoom",
    "text": "Zoom\nIf we’re happy using a ggplot2 extension package, ggforce allows us a different way to display smaller values. We could “zoom in” on specific parts of our chart, making it much easier to view those smaller bars in our stacked barchart.\nAdvantage: “Best of both worlds” - we get our original plot as well as a version from which readers can see the smaller categories.\nDisadvantage: The plot is now nearly twice as big. We must also now make sure that readers are clear what each panel represents to avoid confusion.\nHow: The ggforce::facet_zoom() function is a special faceting function which is designed to do exactly what we’re after here.\n\nggplot(copper, aes(x = year, y = value)) +\n  geom_col(aes(fill = Sectors)) +\n  labs(x = NULL, y = \"Annual UK Copper Emissions (kt)\", fill = NULL) +\n  theme(legend.position = \"top\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  ggforce::facet_zoom(y = value &lt;= 0.02,\n                      zoom.size = .5,\n                      show.area = T)\n\n\n\nFigure 5: Zooming in on a specific part of the plot lets us have the best of both worlds!"
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#interact",
    "href": "posts/2023-02-20-tinyCategories/index.html#interact",
    "title": "Visualising Uncommon Factors",
    "section": "Interact",
    "text": "Interact\nIf we’re in the position to do so, we could create an interactive plot.\nAdvantage: Smaller sectors can now be read by turning off the bigger ones, or “zooming in” on the plot. Tooltips can also reveal the precise value of each sector when the reader hovers over the corresponding bar.\nDisadvantage: Restricted to HTML; cannot be inserted into an academic article or powerpoint presentation. Needs an amount of explanation so readers are aware that they can interact with the figure.\nHow: There are plenty of different ways to create interactive plots in R, including plotly, dygraphs and ggiraph. Below a plotly graphic is shown.\n\nlibrary(plotly)\n\nplot_ly(copper, colors = \"Dark2\") |&gt;\n  add_bars(x = ~ year,\n           y = ~ value,\n           color = ~ Sectors) |&gt;\n  layout(\n    barmode = \"stack\",\n    yaxis = list(title = \"Annual UK Copper Emissions (kt)\"),\n    legend = list(orientation = 'h')\n  )\n\n\nFigure 6: An interactive plot. Try clicking on the legend or dragging around the plot area."
  },
  {
    "objectID": "posts/2023-02-20-tinyCategories/index.html#tabulate",
    "href": "posts/2023-02-20-tinyCategories/index.html#tabulate",
    "title": "Visualising Uncommon Factors",
    "section": "Tabulate",
    "text": "Tabulate\nA left-field alternative to everything we’ve done so far is to abandon creating a chart altogether and just tabulate our data! In a table, each value takes up the exact same amount of space, so there’s no such thing as a sector too small to really make out.\nAdvantage: Every single value can be read. With an interactive table the data is even searchable.\nDisadvantage: It’s not a chart any more. It is also much harder to tell a story with a table like this (e.g., the trend is much harder to identify with a list of numbers).\nHow: There are a lot of table packages in R, such as gt, reactable and DT. Here we use DT to create an interactive, searchable table.\n\ncopper |&gt;\n  mutate(Sectors = factor(Sectors)) |&gt;\n  DT::datatable(rownames = FALSE, filter = \"top\") |&gt;\n  DT::formatSignif(columns = 3, digits = 5)\n\n\nFigure 7: This isn’t a plot any more, but it does the job!"
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#what-are-the-issues",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#what-are-the-issues",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "What are the issues?",
    "text": "What are the issues?\nLet’s read in the data and give it a look over.\nThe bach_students data set indicates…\n\nThe percentage of students broken down by race/ethnicity, aged 25 and over who have attained a bachelor’s degree.\n\nBut in the form it is in, it is pretty untidy and unclean. Here are some issues I can identify:\n\nOur key variables are effectively year, race, total and standard error, but race is spread over multiple columns (remember - each column should represent a variable, not multiple columns representing one variable).\nAll of the columns should be numbers, but they have read in as characters.\nAsian people and Pacific Islanders are given twice - as a total of the two and individually.\nThe names of the columns have spaces in them, are wordy and therefore generally difficult to use.\nThe Total column represents the year. The name is therefore a bit misleading.\n\nA note on the emboldened item above; in my experience, data “in the wild” that has been put together in Excel typically has baggage like this as spreadsheets are sadly used for both data storage and presentation. This means that totals and the like are given alongside the data used to calculate them. I don’t feel bad about removing data like this as long as we are left with the tools to recalculate them. In this case, we are! If we want to regroup the Asian/Pacific Islander data it’d be as easy as renaming them both as the same thing using mutate() and if_else(), and then group_by() and summarise() to re-calculate the total. It may even be that we want to combine more racial groups using a fct_lump() function."
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#cleaning-and-tidying",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#cleaning-and-tidying",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "Cleaning and Tidying",
    "text": "Cleaning and Tidying\nLet’s start by making sure that all of the data is numeric. dplyr v1.0.0 gave us the across() function that makes this a breeze.\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  head(16)\n\n# A tibble: 16 × 19\n   Total `Total, percent of al…` `Standard Erro…` White1 `Standard Erro…` Black1\n   &lt;dbl&gt;                   &lt;dbl&gt;            &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n 1  1910                     2.7            NA      NA              NA      NA  \n 2  1920                     3.3            NA      NA              NA      NA  \n 3  1930                     3.9            NA      NA              NA      NA  \n 4  1940                     4.6            NA       4.9            NA       1.3\n 5  1950                     6.2            NA       6.6            NA       2.2\n 6  1960                     7.7            NA       8.1            NA       3.5\n 7  1970                    11              NA      11.6            NA       6.1\n 8  1975                    13.9            NA      14.9            NA       6.4\n 9  1980                    17              -0.16   18.4            -0.18    7.9\n10  1985                    19.4            -0.16   20.8            -0.19   11.1\n11  1986                    19.4            -0.16   20.9            -0.19   10.9\n12  1987                    19.9            -0.16   21.4            -0.19   10.8\n13  1988                    20.3            -0.16   21.8            -0.19   11.2\n14  1989                    21.1            -0.16   22.8            -0.19   11.7\n15  1990                    21.3            -0.16   23.1            -0.19   11.3\n16  1991                    21.4            -0.16   23.3            -0.19   11.5\n# … with 13 more variables: `Standard Errors - Black1` &lt;dbl&gt;, Hispanic &lt;dbl&gt;,\n#   `Standard Errors - Hispanic` &lt;dbl&gt;, `Total - Asian/Pacific Islander` &lt;dbl&gt;,\n#   `Standard Errors - Total - Asian/Pacific Islander` &lt;dbl&gt;,\n#   `Asian/Pacific Islander - Asian` &lt;dbl&gt;,\n#   `Standard Errors - Asian/Pacific Islander - Asian` &lt;dbl&gt;,\n#   `Asian/Pacific Islander - Pacific Islander` &lt;dbl&gt;,\n#   `Standard Errors - Asian/Pacific Islander - Pacific Islander` &lt;dbl&gt;, …\n\n\nWe could be tempted now to run janitor::clean_names(). For those unfamiliar, this function cleans the names of a data frame to make them easier to use - all lower-case, spaces replaced with underscores, and the like. Normally it’s a good idea to use it as soon as possible, but as I am going to restructure my data I am not going to use it right away. As race will be one of my columns, I’d probably want words like “White” and “Black” to remain capitalised, so if I reported them in a table or use them in a legend label they’d look more presentable.\nNext, we can start restructuring. A good trick with pivot_longer() is that it behaves much the same as functions like select() - if we list a column name preceded with a minus sign (-) it effectively tells the function “everything but this column, please!” I’m not going to specify column names for the names or values here as we’ll quickly get rid of them.\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;% \n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;dbl&gt;\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White1\"                                                           NA  \n 4  1910 \"Standard Errors - White1\"                                         NA  \n 5  1910 \"Black1\"                                                           NA  \n 6  1910 \"Standard Errors - Black1\"                                         NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Total - Asian/Pacific Islander\"                                   NA  \n10  1910 \"Standard Errors - Total - Asian/Pacific Islander\"                 NA  \n11  1910 \"Asian/Pacific Islander - Asian\"                                   NA  \n12  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"                 NA  \n13  1910 \"Asian/Pacific Islander - Pacific Islander\"                        NA  \n14  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Islander\"      NA  \n15  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n16  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n\n\nNow we can remove that Asian/Pacific Islander total we spoke about earlier. We will use filter() alongside the str_detect() function and the logical operator !.\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;dbl&gt;\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White1\"                                                           NA  \n 4  1910 \"Standard Errors - White1\"                                         NA  \n 5  1910 \"Black1\"                                                           NA  \n 6  1910 \"Standard Errors - Black1\"                                         NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Asian/Pacific Islander - Asian\"                                   NA  \n10  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"                 NA  \n11  1910 \"Asian/Pacific Islander - Pacific Islander\"                        NA  \n12  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Islander\"      NA  \n13  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 \"Two or more race\"                                                 NA  \n16  1910 \"Standard Errors - Two or more race\"                               NA  \n\n\nTo separate the total values from the standard errors we can use the separate() function and the \" - \" string. Issues once again come from the Asian and Pacific Islander data as they have a second \" - \" in them, but this can be straightforwardly removed. For whatever reason, “White” and “Black” are listed with the number one (1) after them, so we can get rid of this while we’re on the subject of cleaning strings.\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %&gt;%\n  head(16)\n\n# A tibble: 16 × 3\n   Total name                                                              value\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;dbl&gt;\n 1  1910 \"Total, percent of all persons age 25 and over\"                     2.7\n 2  1910 \"Standard Errors - Total, percent of all persons age 25 and over\"  NA  \n 3  1910 \"White\"                                                            NA  \n 4  1910 \"Standard Errors - White\"                                          NA  \n 5  1910 \"Black\"                                                            NA  \n 6  1910 \"Standard Errors - Black\"                                          NA  \n 7  1910 \"Hispanic\"                                                         NA  \n 8  1910 \"Standard Errors - Hispanic\"                                       NA  \n 9  1910 \"Asian\"                                                            NA  \n10  1910 \"Standard Errors - Asian\"                                          NA  \n11  1910 \"Pacific Islander\"                                                 NA  \n12  1910 \"Standard Errors - Pacific Islander\"                               NA  \n13  1910 \"American Indian/\\r\\nAlaska Native\"                                NA  \n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 \"Two or more race\"                                                 NA  \n16  1910 \"Standard Errors - Two or more race\"                               NA  \n\n\nNow let’s add that separate() step, which we need to give some column names for the name column to turn into, the separating character (space-dash-space), and the direction to fill in (in this case left).\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %&gt;%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %&gt;%\n  head(16)\n\n# A tibble: 16 × 4\n   Total stat            race                                            value\n   &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;                                           &lt;dbl&gt;\n 1  1910 &lt;NA&gt;            \"Total, percent of all persons age 25 and over\"   2.7\n 2  1910 Standard Errors \"Total, percent of all persons age 25 and over\"  NA  \n 3  1910 &lt;NA&gt;            \"White\"                                          NA  \n 4  1910 Standard Errors \"White\"                                          NA  \n 5  1910 &lt;NA&gt;            \"Black\"                                          NA  \n 6  1910 Standard Errors \"Black\"                                          NA  \n 7  1910 &lt;NA&gt;            \"Hispanic\"                                       NA  \n 8  1910 Standard Errors \"Hispanic\"                                       NA  \n 9  1910 &lt;NA&gt;            \"Asian\"                                          NA  \n10  1910 Standard Errors \"Asian\"                                          NA  \n11  1910 &lt;NA&gt;            \"Pacific Islander\"                               NA  \n12  1910 Standard Errors \"Pacific Islander\"                               NA  \n13  1910 &lt;NA&gt;            \"American Indian/\\r\\nAlaska Native\"              NA  \n14  1910 Standard Errors \"American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 &lt;NA&gt;            \"Two or more race\"                               NA  \n16  1910 Standard Errors \"Two or more race\"                               NA  \n\n\nYou’ll notice that in the stats column we have some NA values that actually correspond to the “total” stat, so we’ll fill those in using a tidyr function, replace_na().\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %&gt;%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %&gt;%\n  mutate(stat = replace_na(stat, \"Total\")) %&gt;%\n  head(16)\n\n# A tibble: 16 × 4\n   Total stat            race                                            value\n   &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;                                           &lt;dbl&gt;\n 1  1910 Total           \"Total, percent of all persons age 25 and over\"   2.7\n 2  1910 Standard Errors \"Total, percent of all persons age 25 and over\"  NA  \n 3  1910 Total           \"White\"                                          NA  \n 4  1910 Standard Errors \"White\"                                          NA  \n 5  1910 Total           \"Black\"                                          NA  \n 6  1910 Standard Errors \"Black\"                                          NA  \n 7  1910 Total           \"Hispanic\"                                       NA  \n 8  1910 Standard Errors \"Hispanic\"                                       NA  \n 9  1910 Total           \"Asian\"                                          NA  \n10  1910 Standard Errors \"Asian\"                                          NA  \n11  1910 Total           \"Pacific Islander\"                               NA  \n12  1910 Standard Errors \"Pacific Islander\"                               NA  \n13  1910 Total           \"American Indian/\\r\\nAlaska Native\"              NA  \n14  1910 Standard Errors \"American Indian/\\r\\nAlaska Native\"              NA  \n15  1910 Total           \"Two or more race\"                               NA  \n16  1910 Standard Errors \"Two or more race\"                               NA  \n\n\nNow we can pivot_wider() to get the total values and the standard errors in their own columns. We’ll have to rename the existing Total column first, but that can be achieved using dplyr’s rename().\n\nbach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %&gt;%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %&gt;%\n  mutate(stat = replace_na(stat, \"Total\")) %&gt;%\n  rename(year = Total) %&gt;%\n  pivot_wider(names_from = stat, values_from = value) %&gt;%\n  head(8)\n\n# A tibble: 8 × 4\n   year race                                            Total `Standard Errors`\n  &lt;dbl&gt; &lt;chr&gt;                                           &lt;dbl&gt;             &lt;dbl&gt;\n1  1910 \"Total, percent of all persons age 25 and over\"   2.7                NA\n2  1910 \"White\"                                          NA                  NA\n3  1910 \"Black\"                                          NA                  NA\n4  1910 \"Hispanic\"                                       NA                  NA\n5  1910 \"Asian\"                                          NA                  NA\n6  1910 \"Pacific Islander\"                               NA                  NA\n7  1910 \"American Indian/\\r\\nAlaska Native\"              NA                  NA\n8  1910 \"Two or more race\"                               NA                  NA\n\n\nThe data is now tidy! We can do some additional cleaning steps now - there seems to be something odd going on with the American Indian/Alaska Native string, and the “Total” string is a bit wordy. Let’s sort that out, and finally throw in that janitor function I talked about right at the beginning.\n\ndf = bach_students %&gt;%\n  mutate(across(.cols = where(is.character), \n                .fns = parse_number)) %&gt;%\n  pivot_longer(-Total) %&gt;%\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %&gt;%\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %&gt;%\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %&gt;%\n  mutate(stat = replace_na(stat, \"Total\")) %&gt;%\n  rename(year = Total) %&gt;%\n  pivot_wider(names_from = stat, values_from = value) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(\n    race = str_remove_all(\n      race, \n      \", percent of all persons age 25 and over|\\r\\n\")\n  )\n\ndf %&gt;% head(8)\n\n# A tibble: 8 × 4\n   year race                          total standard_errors\n  &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;           &lt;dbl&gt;\n1  1910 Total                           2.7              NA\n2  1910 White                          NA                NA\n3  1910 Black                          NA                NA\n4  1910 Hispanic                       NA                NA\n5  1910 Asian                          NA                NA\n6  1910 Pacific Islander               NA                NA\n7  1910 American Indian/Alaska Native  NA                NA\n8  1910 Two or more race               NA                NA\n\n\nWe did it! The data is now clean and tidy and ready to use. Let’s do a bit of analysis just to demonstrate how straightforward it is to use now."
  },
  {
    "objectID": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#data-analysis",
    "href": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/index.html#data-analysis",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "section": "Data Analysis",
    "text": "Data Analysis\nWe can create some cool plots now we have access to this data. We can start with a simple timeseries.\n\ntheme_set(theme_light())\n\nplot_data = df %&gt;%\n  drop_na() %&gt;%\n  mutate(across(total:standard_errors, ~.x/100))\n\nplot_data %&gt;%\n  filter(race != \"Total\") %&gt;%\n  mutate(race = fct_reorder(race, total, max, na.rm = T),\n         race = fct_rev(race)) %&gt;%\n  ggplot(aes(\n    year,\n    y = total,\n    ymax = total + standard_errors,\n    ymin = total - standard_errors,\n    group = race\n  )) +\n  geom_ribbon(aes(fill = race), alpha = .25) +\n  geom_line(aes(color = race)) +\n  geom_line(data = filter(plot_data, race == \"Total\"), size = 2) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Year\", y = NULL, color = \"Race\", fill = \"Race\",\n       title = \"Bachelor's Degree Attainment\",\n       subtitle = \"The percentage of the population who have achieved a\\nbachelor's degree in the US since 1980, split into racial groups.\\nThe bold line represents the total population.\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\nOr we could just focus in on the most recent data.\n\nplot_data_2 = plot_data %&gt;%\n  filter(year == max(year)) %&gt;%\n  mutate(race = fct_reorder(race, total))\n\ntot = plot_data_2 %&gt;%\n  filter(race == \"Total\") %&gt;% \n  pull(total)\n\nplot_data_2 %&gt;%\n  mutate(flag = case_when(race == \"Total\" ~ \"T\",\n                          total &gt; tot ~ \"Y\",\n                          total &lt; tot ~ \"X\")) %&gt;%\n  ggplot(aes(y = race, x = total, xmax = total+standard_errors, xmin = total-standard_errors, fill = flag)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = tot, size = 1) +\n  geom_pointrange(show.legend = F) +\n  scale_x_continuous(expand = expansion(mult = c(0,.1)), \n                     labels = scales::percent) +\n  labs(y = NULL, x = \"Bachelor's Degree Attainment in 2016\")\n\n\n\n\n\nplot_data_2 %&gt;%\n  janitor::remove_constant() %&gt;%\n  mutate(mutate(across(\n    where(is.numeric), ~ glue::glue(\"{abs(.x) * 100} %\")\n  ))) %&gt;%\n  knitr::kable(col.names = c(\"Race\", \"Total\", \"Std Err.\")) %&gt;%\n  kableExtra::kable_styling()\n\n\n\nRace\nTotal\nStd Err.\n\n\n\nTotal\n33.4 %\n0.24 %\n\n\nWhite\n37.3 %\n0.31 %\n\n\nBlack\n23.5 %\n0.46 %\n\n\nHispanic\n16.4 %\n0.4 %\n\n\nAsian\n56.4 %\n0.89 %\n\n\nPacific Islander\n27.5 %\n2.92 %\n\n\nAmerican Indian/Alaska Native\n16.8 %\n1.39 %\n\n\nTwo or more race\n30.6 %\n1.52 %\n\n\n\n\n\nSee how easy it is to use this data now? Incredible!"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Data Visualisation\n\n\nCreating effective static and interactive data visualisation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Reporting\n\n\nAuthoring dynamic documents using {quarto} and {rmarkdown}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Package Development\n\n\nDeveloping open-source tools for the R community.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nPhD research into the remote sensing of vehicle emissions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nExperience teaching R and reproducible data analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/research.html",
    "href": "experience/research.html",
    "title": "Research",
    "section": "",
    "text": "My PhD thesis (Davison 2022) focused on the calculation of representative emission factors from vehicle emission remote sensing data. An emission factor is a value used to estimate a total emission from a sector (e.g., transport) from a more measurable activity (e.g., total kilometres driven).\n\\(Emission~Estimate~\\left(g\\right) = \\textbf{Emission~Factor}~\\left(g~km^{-1}\\right) \\times Measured~Activity~\\left(km\\right)\\)\nTypically these emission factors are based on data from a very limited range of vehicles, most commonly from lab tests but also from instrumented vehicles (Portable Emissions Monitoring Systems, PEMS). While these give good journey coverage, capturing everything from idling to bombing down the motorway, they give poor fleet coverage, only measuring a handful of “representative” vehicles.\nVehicle emission remote sensing is a technique for measuring a lot of vehicles. Think of it as a bit like a speed camera with a spectrometer attached, that being an instrument capable of measuring pollutant concentrations in the atmosphere. While the use of remote sensing rapidly populates a database of hundreds of thousands of tailpipe concentration measurements, only measuring a dispersing plume (rather than the sum total of all tailpipe emissions) means some robust statistical modelling is required to calculate representative emission factors.\nOne of the key outcomes from my research was the clear significance of the difference in emissions between vehicle manufacturers. Figure 1, taken from Davison et al. (2021), illustrates the distribution of different distance-specific emissions for different vehicle manufacturers and engine sizes. These differences are currently not directly accounted for in European emissions inventories.\n\n\n\nFigure 1: Distance-specific CO2 and NOx emissions (g km-1) for Euro 6 light duty vehicles. Each dot represents a unique manufacturer group-engine size combination, with size proportional to the number of observations included in its calculation. The diamonds represent the weighted mean for each engine size, and the horizontal lines the weighted mean for each vehicle category (Diesel Light Commercial Vehicle, Diesel Passenger Car, Gasoline Passenger Car). Taken from Davison et al. (2021).\n\n\nI also used statistical modelling techniques to examine other effects on vehicle emission concentrations. For example, quantile regression (via the {quantreg} package) was used to examine the skewed relationships between cumulative mileage and fuel-specific emissions in passenger cars. Figure 2, taken from Davison et al. (2022), shows that there are a small proportion of high-mileage gasoline Euro 3 passenger cars which are higher emitting than the average Euro 5/6 diesel cars.\n\n\n\nFigure 2: Plot showing the modelled linear deterioration of passenger cars from 0 to 160,000~km of cumulative mileage (a vehicle’s ``normal life’’ under Euro 6 legislation. Taken from Davison et al. (2022).\n\n\n\nAcademic Publications\nPublications I was involved in throughout my PhD are detailed below. The majority of these, not least my thesis (Davison 2022), are open source and therefore free to read for those outside of academia.\n\n\n\nDavison, Jack. 2022. “New Approaches for Understanding Vehicle Emissions Using Remote Sensing.” University of York. https://etheses.whiterose.ac.uk/31313/.\n\n\nDavison, Jack, Yoann Bernard, Jens Borken-Kleefeld, Naomi J. Farren, Stefan Hausberger, Åke Sjödin, James E. Tate, Adam R. Vaughan, and David C. Carslaw. 2020. “Distance-Based Emission Factors from Vehicle Emission Remote Sensing Measurements.” Science of The Total Environment 739 (October): 139688. https://doi.org/10.1016/j.scitotenv.2020.139688.\n\n\nDavison, Jack, Rebecca A. Rose, Naomi J. Farren, Rebecca L. Wagner, Tim P. Murrells, and David C. Carslaw. 2021. “Verification of a National Emission Inventory and Influence of On-Road Vehicle Manufacturer-Level Emissions.” Environmental Science & Technology 55 (8): 4452–61. https://doi.org/10.1021/acs.est.0c08363.\n\n\nDavison, Jack, Rebecca A. Rose, Naomi J. Farren, Rebecca L. Wagner, Shona E. Wilde, Jasmine V. Wareham, and David C. Carslaw. 2022. “Gasoline and Diesel Passenger Car Emissions Deterioration Using on-Road Emission Measurements and Measured Mileage.” Atmospheric Environment: X 14 (April): 100162. https://doi.org/10.1016/j.aeaoa.2022.100162.\n\n\nFarren, Naomi J., Jack Davison, Rebecca A. Rose, Rebecca L. Wagner, and David C. Carslaw. 2020. “Underestimated Ammonia Emissions from Road Vehicles.” Environmental Science & Technology, December. https://doi.org/10.1021/acs.est.0c05839.\n\n\n———. 2021. “Characterisation of Ammonia Emissions from Gasoline and Gasoline Hybrid Passenger Cars.” Atmospheric Environment: X 11 (October): 100117. https://doi.org/10.1016/j.aeaoa.2021.100117.\n\n\nGrange, Stuart K., Naomi J. Farren, Adam R. Vaughan, Jack Davison, and David C. Carslaw. 2020. “Post-Dieselgate: Evidence of NOx Emission Reductions Using on-Road Remote Sensing.” Environmental Science & Technology Letters 7 (6): 382–87. https://doi.org/10.1021/acs.estlett.0c00188.\n\n\nWagner, Rebecca L., Naomi J. Farren, Jack Davison, Stuart Young, James R. Hopkins, Alastair C. Lewis, David C. Carslaw, and Marvin D. Shaw. 2021. “Application of a Mobile Laboratory Using a Selected-Ion Flow-Tube Mass Spectrometer (SIFT-MS) for Characterisation of Volatile Organic Compounds and Atmospheric Trace Gases.” Atmospheric Measurement Techniques 14 (9): 6083–6100. https://doi.org/10.5194/amt-14-6083-2021."
  },
  {
    "objectID": "experience/pkgdev.html",
    "href": "experience/pkgdev.html",
    "title": "R Package Development",
    "section": "",
    "text": "As well as authoring many private R packages to streamline data analysis pipelines for myself and colleagues, I have also developed publicly available packages, many of which focused on air quality data analysis.\n\nLead Developer\n\n{openairmaps}\n\n\n\nI am the lead developer on the {openairmaps} package, designed to combine the air quality analysis techniques of {openair} with the interactivity of {leaflet}. Any {openair} “polar” directional analysis plotting function can be used as a marker on a {leaflet} map, allowing for easier triangulation of potential pollutant sources. Other functionality includes visualising UK AQ networks, interactive mapping of HYSPLIT trajectories, and static equivalents of the polar maps powered using {ggmap}.\n\n\n{ggopenair}\n\n\n\nI am developing {ggopenair}, a {ggplot2} implementation of {openair}. {openair} is written in {lattice} which, unlike {ggplot2}, is no longer under active development. {ggopenair} will also allow users more power to modify their plots “after-the-fact”, and benefit from the extensive library of {ggplot2} extensions like {patchwork}.\n\n\n\nCollaborator\n\n{openair}\nThe {openair} package has been around for over a decade, and is used around the world in the public and private sectors to analyse air quality data.\n\n\n{worldmet}\nThe {worldmet} package gives easy access to world meteorological data via accessing NOAA databases."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Are {tidyverse} function names getting longer?\n\n\n\n\n\n\n\ntidyverse\n\n\nplotly\n\n\n\n\nAre we approaching sentence-length {tidyverse} functions? quarto::read_blog_carefully_with_eyes() to find out.\n\n\n\n\n\n\nMar 4, 2023\n\n\nJack Davison\n\n\n\n\n\n\n  \n\n\n\n\nVisualising Uncommon Factors\n\n\n\n\n\n\n\nvisualisation\n\n\nggplot2\n\n\nplotly\n\n\nDT\n\n\n\n\nAvoiding incomprehensible legend items in your proportion charts through tabulating, lumping or interactive visualisations.\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJack Davison\n\n\n\n\n\n\n  \n\n\n\n\nScraping the Pasta Academy\n\n\n\n\n\n\n\nrvest\n\n\nwebscraping\n\n\n\n\nOvercoming the lack of a “filter” option on an experience booking website through web scraping.\n\n\n\n\n\n\nNov 13, 2022\n\n\nJack Davison\n\n\n\n\n\n\n  \n\n\n\n\nAn Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)\n\n\n\n\n\n\n\ntidytuesday\n\n\n\n\nA disucssion on data tidying and cleaning, with applications to a messy, unfamiliar data set.\n\n\n\n\n\n\nFeb 2, 2021\n\n\nJack Davison\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/dataviz.html",
    "href": "experience/dataviz.html",
    "title": "Data Visualisation",
    "section": "",
    "text": "I am passionate about data visualisation and communication in all of its forms. This page details some of the projects I’ve been part of.\n\nTidyTuesday\n#TidyTuesday is a weekly data challenge which tests the R community to tidy and visualise an unseen dataset, which could be related to anything from astronauts to zoo animals! Figure 1 is a collage of a few of my favourite contributions, with my full repo found at https://github.com/jack-davison/TidyTuesday.\n\n\n\nFigure 1: Some examples of my TidyTuesday contributions.\n\n\n\n\nDuBois Challenge\nThe DuBois Challenge is a challenge to recreate historic, hand drawn data visualisations using modern tools. The visualisations are specifically the work of data-viz pioneer W.E.B. DuBois, known for his subversive, geometric, ahead-of-his-time designs.\nI took part in the DuBois Challenge as part of #TidyTuesday (above), but my deepest examination of DuBois’ work was in my submission for the Posit Table Contest 2022, Tabulating DuBois, wherein I attempted to translate the work of DuBois to a more tabular medium with {gt}.\n\n\n\nFigure 2: An example table from Tabulating DuBois."
  },
  {
    "objectID": "experience/reports.html",
    "href": "experience/reports.html",
    "title": "Dynamic Reporting",
    "section": "",
    "text": "I have authored interactive documents for a wide range of projects and clients, from developing dynamic annual network summaries for the UK Government to efficiently writing up reports in response to unpredictable air pollution episodes.\nSome examples of my work include:\n\nAnnual Reports (e.g., UK Hydrocarbons Network Annual Report)\nEpisode Reports (e.g., 2023 West London Particulate Episode Report)\n\nTools in which I am proficient that I use to develop these reports include:\n\n{quarto} (and formerly {rmarkdown}) to render the documents.\n{plotly} and {ggiraph} for interactive plots and figures.\n{leaflet} for interactive maps.\n{ggplot2} and various extensions for static figures/maps.\n{DT} for interactive tables.\n{gt} for static tables.\nmermaid for flowcharts and other diagrams.\n\nScreenshots from a dynamic report are shown in Figure 1.\n\n\n\n\n\n\n\n{ggplot2} figure.\n\n\n\n\n\n\n\nStyled {gt} table.\n\n\n\n\n\n\n\n\n\nInteractive {leaflet} map using the {leaftime} extension.\n\n\n\n\nFigure 1: Screenshots from the 2023 West London Particulate Episode Report."
  },
  {
    "objectID": "experience/teaching.html",
    "href": "experience/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I am a Posit (formerly RStudio) Certified Tidyverse Instructor with over 5 years experience teaching reproducible data analysis in R to a variety of audiences. Typically these courses centre around the analysis of environmental data, specifically long term monitoring data.\nI have delivered training to learners from organisations such as:\n\nUK Department for the Environment, Food and Rural Afairs (Defra)\nUK Health Security Agency (UKHSA)\nUK Environment Agency (EA)\nThe Clean Air Society of Australia and New Zealand (CASANZ)\nPort Talbot Local Authority\nUK Centre for Ecology & Hydrology (UKCEH)\nThe Wolfson Atmospheric Chemistry Laboratories (WACL)\n\n\nApproach\nI am a proponent of live coding in teaching, meaning that learners can see RStudio used in an authentic way. Authentic examples showing real-world applications of R are important to ensure that the content is relevant to learner’s interests. Teaching is supported by extensive, reproducible learning materials written in {quarto} (formerly {rmarkdown}). Where possible on longer courses, case studies are written up using data provided by learners to get them started with their own data analysis projects.\nFigure 1 shows some example course materials produced for a recent course on using R for air quality modelling. An advanced lesson, it outlines a method to use {readr} and {purrr} to rapidly pull large amounts of kilometre grid-square modelled air quality data from the “UK AIR” website and then plot it using {ggplot2}. All of the content is reproducible, allowing learners to explore it further at a later date.\n\n\n\nFigure 1: An example of reproducible course materials, commonly produced when teaching R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack Davison",
    "section": "",
    "text": "I am a senior consultant and data analyst working for an Environmental Consultancy in South Oxfordshire in the United Kingdom.\nI was awarded a PhD in Atmospheric Chemistry from the Wolfson Atmospheric Chemistry Laboratories at the University of York, with research focusing on the calculating emission factors from large databases of road transport emissions.\nI’m a collaborator on the {openair} project and the lead developer on the {openairmaps} R package.\nI’m also a Posit (formerly RStudio) Certified Tidyverse Instructor and have taught R for environmental data analysis to a wide range of audiences, including learners from Defra, the UK Environment Agency, and the UK Health Security Agency."
  },
  {
    "objectID": "posts/2022-11-13-pastaEvangelists/index.html",
    "href": "posts/2022-11-13-pastaEvangelists/index.html",
    "title": "Scraping the Pasta Academy",
    "section": "",
    "text": "Note\n\n\n\nAt time of writing this blog, the Pasta Academy did not have a filter on their website. They have since got their act together and implemented one, making much of this code no longer useful! It is, however, still interesting, and a nice introduction to applying rvest to solve a real (at the time) problem.\n\n\nPurpose\nRecently, my partner and I were looking to book a cooking experience as a Christmas treat, and we settled on the excellently reviewed Pasta Academy in London. We were looking for a beginner class in early 2023, but we were immediately met with an issue — there was no way to filter the booking list!\nWe could have sat and go through the list manually, perhaps noting down each of the beginner classes when we found them. This sounded like a bit of a faff, so I instead thought I’d scrape all of the courses and put them into a table. Some might argue that writing an R script to do this is even more of a faff, but I did it anyway!\nThis blog post shows how I scraped the data from the Pasta Academy website using rvest and then went on to tidy it using the tidyverse. You may find this a nice introduction to web scraping in R, and a nice application of data skills in “real life”.\nWeb Scraping with {rvest}\n\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nWe’ll start by scraping the first page. To start using rvest, we first need to define a session. This simulates a user interacting with a website.\n\npasta_session &lt;-\n  session(\"https://pastaevangelists.com/collections/pasta-academy?page=1\")\n\nWe’ll now need to pull each individual element of interest from the Pasta Academy website. One of the easiest ways of finding the correct HTML at which to aim your rvest functions is by using Chrome’s developer tools (CTRL-SHIFT-I on Windows) or your browser’s equivalent.\n\n\nFigure 1: A screenshot of the use of Chrome’s developer tools.\n\nOnce we have identified the correct HTML tags to use, we can use html_elements() to grab those items, and then html_text2() to extract the text from it. For example, the below line grabs each lesson title from the first page of the Pasta Academy website.\n\nhtml_elements(pasta_session, \".product-item-row__meta-title\") |&gt; html_text2()\n\n [1] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n [2] \"PASTA ACADEMY™ | TASTE OF PUGLIA |\\r\"                          \n [3] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                        \n [4] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n [5] \"PASTA ACADEMY™ | TASTE OF VENICE |\\r\"                          \n [6] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                        \n [7] \"PASTA ACADEMY™ | TASTE OF SARDINIA |\\r\"                        \n [8] \"PASTA ACADEMY™ | TASTE OF ROME |\\r\"                            \n [9] \"PASTA ACADEMY™ | TASTE OF BOLOGNA |\\r\"                         \n[10] \"PASTA ACADEMY™ | LIMITED EDITION SEAFOOD PASTA MASTERCLASS |\\r\"\n[11] \"PASTA ACADEMY™ | LIMITED EDITION SEAFOOD PASTA MASTERCLASS |\\r\"\n[12] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                        \n[13] \"PASTA ACADEMY™ | LIMITED EDITION SEAFOOD PASTA MASTERCLASS |\\r\"\n[14] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n[15] \"PASTA ACADEMY™ | MORNING BEGINNERS |\\r\"                        \n[16] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n[17] \"PASTA ACADEMY™ | TASTE OF SARDINIA |\\r\"                        \n[18] \"PASTA ACADEMY™ | TASTE OF TUSCANY |\\r\"                         \n[19] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n[20] \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\"                          \n\n\nAll we need to do is repeat this until we have all of the information we want. In this case, all I need is the course title, the date/time, the price, and whether or not it is sold out. The button text is a useful proxy for that last item, as it reads “SEE INFO” if a course is fully booked and “BOOK NOW” if there are spaces left.\n\nname &lt;- html_elements(pasta_session, \".product-item-row__meta-title\") |&gt; html_text2()\n\ndate &lt;- html_elements(pasta_session, \".product-item-row__meta-title-sub\") |&gt; html_text2()\n\nprice &lt;- html_elements(pasta_session, \"span.price\") |&gt; html_text2()\n\nbutton &lt;- html_elements(pasta_session, \".product-item-row__cta-btn\") |&gt; html_text2()\n\ndplyr::tibble(\n  name = name, \n  date = date,\n  price = price, \n  button = button\n) |&gt;\n  dplyr::glimpse()\n\nRows: 20\nColumns: 4\n$ name   &lt;chr&gt; \"PASTA ACADEMY™ | BEGINNERS CLASS |\\r\", \"PASTA ACADEMY™ | TASTE…\n$ date   &lt;chr&gt; \"\\r Wednesday March 1st, 2023, 18:30\\r\", \"\\r Friday March 3rd, …\n$ price  &lt;chr&gt; \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£65.00\\r\", \"£6…\n$ button &lt;chr&gt; \"SEE INFO\", \"SEE INFO\", \"SEE INFO\", \"SEE INFO\", \"SEE INFO\", \"SE…\n\n\nNow all four pieces of information can be scraped from one page, we can use purrr to scrape this information from all of the pages. We’ll write a function which takes one argument, id, which is used to select the specific page of the Pasta Academy website.\n\nscrape_pasta &lt;- function(id){\n  x &lt;- session(str_glue(\"https://pastaevangelists.com/collections/pasta-academy?page={id}\"))\n  \n  tibble(\n    name = html_elements(x, \".product-item-row__meta-title\") |&gt; html_text2(),\n    date = html_elements(x, \".product-item-row__meta-title-sub\") |&gt; html_text2(),\n    price = html_elements(x, \"span.price\") |&gt; html_text2(),\n    fully_booked = html_elements(x, \".product-item-row__cta-btn\") |&gt; html_text2()\n  ) |&gt;\n    mutate(page_id = id)\n}\n\nraw_pasta &lt;- map_dfr(1:19, scrape_pasta)\n\nTidying Data\nWe wouldn’t eat raw pasta, and we won’t want to work with raw_pasta as it currently exists. Lets use the tidyverse to tidy this data up a bit.\nFirst, lets get rid of some of the dodgy formatting — we’ll drop the \\r, vertical bars, question marks, and the PASTA ACADEMY branding from all character columns.\n\npasta &lt;- \n  raw_pasta |&gt;\n  mutate(across(where(is.character), \n                ~ str_remove_all(.x, \"PASTA ACADEMY™|\\r|\\\\||\\\\?\") |&gt; \n                  str_squish()))\npasta\n\n# A tibble: 145 × 5\n   name                                      date          price fully…¹ page_id\n   &lt;chr&gt;                                     &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n 1 BEGINNERS CLASS                           Wednesday Ma… £65.… SEE IN…       1\n 2 TASTE OF PUGLIA                           Friday March… £65.… SEE IN…       1\n 3 MORNING BEGINNERS                         Saturday Mar… £65.… SEE IN…       1\n 4 BEGINNERS CLASS                           Saturday Mar… £65.… SEE IN…       1\n 5 TASTE OF VENICE                           Saturday Mar… £65.… SEE IN…       1\n 6 MORNING BEGINNERS                         Sunday March… £65.… SEE IN…       1\n 7 TASTE OF SARDINIA                         Sunday March… £65.… SEE IN…       1\n 8 TASTE OF ROME                             Sunday March… £65.… SEE IN…       1\n 9 TASTE OF BOLOGNA                          Monday March… £65.… SEE IN…       1\n10 LIMITED EDITION SEAFOOD PASTA MASTERCLASS Tuesday Marc… £85.… BOOK N…       1\n# … with 135 more rows, and abbreviated variable name ¹​fully_booked\n\n\nAn easier step — lets format the price as numeric data, and fully_booked as logical. Lets also make name all lower-case.\n\npasta &lt;-\n  pasta |&gt; \n  mutate(name = tolower(name),\n         price = parse_number(price),\n         fully_booked = if_else(fully_booked == \"SEE INFO\", T, F))\n\npasta\n\n# A tibble: 145 × 5\n   name                                      date          price fully…¹ page_id\n   &lt;chr&gt;                                     &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;     &lt;int&gt;\n 1 beginners class                           Wednesday Ma…    65 TRUE          1\n 2 taste of puglia                           Friday March…    65 TRUE          1\n 3 morning beginners                         Saturday Mar…    65 TRUE          1\n 4 beginners class                           Saturday Mar…    65 TRUE          1\n 5 taste of venice                           Saturday Mar…    65 TRUE          1\n 6 morning beginners                         Sunday March…    65 TRUE          1\n 7 taste of sardinia                         Sunday March…    65 TRUE          1\n 8 taste of rome                             Sunday March…    65 TRUE          1\n 9 taste of bologna                          Monday March…    65 TRUE          1\n10 limited edition seafood pasta masterclass Tuesday Marc…    85 FALSE         1\n# … with 135 more rows, and abbreviated variable name ¹​fully_booked\n\n\nIt’d be useful to have the date as a properly formatted date-time column. Sadly, this column is not consistently formatted. The two issues are:\n\nSometimes the time has a comma before it, but not always\nSometimes the year is present, but not always\n\nTo deal with this, we’ll take the following steps:\n\nExtract the time & year from the date using regex.\nFill any missing years. This might not be perfect if there are missing years between December and January, but for our purposes we can live with this and can cross-reference with page_id if we need to double check.\nReformat the date by performing string transformations.\nParse the date as a date-time using lubridate.\n\nIn practice, this looks like this:\n\npasta &lt;-\n  pasta |&gt;\n  \n  # extract year/time\n  mutate(\n    time = str_extract(date, \"[0-9][0-9]:[0-9][0-9]\"),\n    year = str_extract(date, \"[0-9][0-9][0-9][0-9]\")\n  ) |&gt;\n  \n  # fill year\n  fill(year, .direction = \"down\") |&gt;\n  \n  # reformat date \n  mutate(date = str_remove_all(date, time) |&gt;\n           str_remove_all(year) |&gt;\n           str_remove(\", \") |&gt;\n           str_squish() |&gt; \n           str_remove(\", am\")) |&gt; \n  separate(date, c(\"day\", \"date\"), sep = \" \", extra = \"merge\") |&gt; \n  unite(date, date, year, time, sep = \" \") |&gt; \n  mutate(date = str_remove(date, \",\")) |&gt; \n  \n  # parse date as date-time\n  mutate(date = lubridate::parse_date_time(date, \n                                           orders = c(\"BdY HM\", \"dBY HM\"))) |&gt;\n  \n  # drop date\n  select(-day)\n\npasta\n\n# A tibble: 145 × 5\n   name                                date                price fully…¹ page_id\n   &lt;chr&gt;                               &lt;dttm&gt;              &lt;dbl&gt; &lt;lgl&gt;     &lt;int&gt;\n 1 beginners class                     2023-03-01 18:30:00    65 TRUE          1\n 2 taste of puglia                     2023-03-03 18:30:00    65 TRUE          1\n 3 morning beginners                   2023-03-04 10:00:00    65 TRUE          1\n 4 beginners class                     2023-03-04 13:00:00    65 TRUE          1\n 5 taste of venice                     2023-03-04 18:30:00    65 TRUE          1\n 6 morning beginners                   2023-03-05 10:00:00    65 TRUE          1\n 7 taste of sardinia                   2023-03-05 13:00:00    65 TRUE          1\n 8 taste of rome                       2023-03-05 18:30:00    65 TRUE          1\n 9 taste of bologna                    2023-03-06 18:30:00    65 TRUE          1\n10 limited edition seafood pasta mast… 2023-03-07 18:30:00    85 FALSE         1\n# … with 135 more rows, and abbreviated variable name ¹​fully_booked\n\n\nUsing the data\nNow that the data is in a tidy format, lets find out when the available beginner courses are being held. Evenings are also our preference:\n\npotential_classes &lt;- \n  pasta |&gt; \n  filter(str_detect(name, \"beg\"),\n         lubridate::hour(date) &gt; 14,\n         !fully_booked)\n\npotential_classes\n\n# A tibble: 12 × 5\n   name            date                price fully_booked page_id\n   &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;lgl&gt;          &lt;int&gt;\n 1 beginners class 2023-03-14 18:30:00    65 FALSE              1\n 2 beginners class 2023-03-15 18:30:00    65 FALSE              1\n 3 beginners class 2023-03-21 18:30:00    65 FALSE              2\n 4 beginners class 2023-04-02 18:30:00    65 FALSE              3\n 5 beginners class 2023-04-12 18:30:00    65 FALSE              3\n 6 beginners class 2023-04-24 18:30:00    65 FALSE              4\n 7 beginners class 2023-05-05 18:30:00    65 FALSE              5\n 8 beginners class 2023-05-16 18:30:00    65 FALSE              5\n 9 beginners class 2023-05-20 18:30:00    65 FALSE              6\n10 beginners class 2023-05-27 18:30:00    65 FALSE              6\n11 beginners class 2023-06-09 18:30:00    65 FALSE              7\n12 beginners class 2023-06-25 18:30:00    65 FALSE              8\n\n\nAnd there they are!\nJust as we now have the data to hand, lets see what some common themes are in the classes. Disregarding words that aren’t particularly unique or descriptive like “taste”, “beginners”, “class”, “morning”, we can learn that there seems to be a lot inspired by the cuisines of Puglia, Rome, and Sardinia.\n\ncounts &lt;-\n  pasta |&gt; \n  tidytext::unnest_tokens(word, name) |&gt; \n  anti_join(tidytext::get_stopwords()) |&gt; \n  count(word, sort = T)\n\ncounts\n\n# A tibble: 30 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 taste          77\n 2 beginners      58\n 3 morning        33\n 4 class          26\n 5 edition        11\n 6 limited        11\n 7 masterclass    10\n 8 puglia         10\n 9 sardinia       10\n10 rome            9\n# … with 20 more rows\n\n\nFigure 2 visualises the above data using ggiraph, showing the most and least common words in these pasta making class names. Hover over the bars to read the exact values!\n\nlibrary(ggiraph)\n\nplt &lt;-\n  counts |&gt;\n  mutate(word = fct_reorder(word, n)) |&gt;\n  ggplot(aes(y = word, x = n)) +\n  geom_col_interactive(aes(fill = n, tooltip = n), show.legend = FALSE) +\n  theme_classic() +\n  scale_x_continuous(expand = expansion()) +\n  labs(y = NULL, x = \"Count (n)\")\n\ngirafe(ggobj = plt)\n\n\nFigure 2: Frequency of different words in Pasta Academy class names."
  },
  {
    "objectID": "posts/2023-03-04-tidyverseLength/index.html",
    "href": "posts/2023-03-04-tidyverseLength/index.html",
    "title": "Are {tidyverse} function names getting longer?",
    "section": "",
    "text": "Note\n\n\n\nNone of this is meant as an insult to the tidyverse developers, of course, who have done excellent work making R one of the top languages for data analysis. This is all in good fun!\n\n\nScrolling through #rstats Twitter recently I noticed a lot of conversation about the recent tidyr deprecations being overly wordy. After all, separate() being superseded by separate_wider_position() does feel almost comically long!\n\n\nThis made me think, however - which tidyverse packages have the wordiest functions? And are functions always shorter than the functions that supersede them? Let’s use R to find out.\nData\nFirst, we need to load the whole tidyverse. I’m not talking about the “core” tidyverse - we need the whole thing!\n\n(pkgs &lt;- tidyverse::tidyverse_packages(include_self = FALSE))\n\n [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"        \n[21] \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n[25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"        \n[29] \"xml2\"         \n\n\nLet’s use purrr to rapidly load them all. We’ll also need plotly and ggiraph for data visualisation.\n\npurrr::walk(pkgs, ~ library(.x, character.only = TRUE))\n\nlibrary(ggiraph)\nlibrary(plotly)\n\nWe can now easily extract a list of functions in a package using the ls() function.\n\nls(\"package:broom\")\n\n[1] \"augment\"         \"augment_columns\" \"bootstrap\"       \"confint_tidy\"   \n[5] \"finish_glance\"   \"fix_data_frame\"  \"glance\"          \"tidy\"           \n[9] \"tidy_irlba\"     \n\n\nThe lifecycle package can even tell us which functions are superseded or not for many tidyverse packages.\n\nlifecycle::pkg_lifecycle_statuses(package = \"ggplot2\")\n\n    package            fun  lifecycle\n4   ggplot2           aes_ deprecated\n5   ggplot2     aes_string deprecated\n6   ggplot2          aes_q deprecated\n8   ggplot2       aes_auto deprecated\n37  ggplot2      coord_map superseded\n38  ggplot2 coord_quickmap superseded\n166 ggplot2         gg_dep deprecated\n\n\nThis process can then be tidied by writing a function and once again using purrr.\n\nlist_functions_tbl &lt;- function(pkg){\n  life &lt;- lifecycle::pkg_lifecycle_statuses(package = pkg) %&gt;%\n    select(-package)\n  ls(str_glue(\"package:{pkg}\")) %&gt;%\n    tibble() %&gt;%\n    set_names(\"fun\") %&gt;%\n    mutate(pkg = {{pkg}},\n           fun.len = str_length(fun)) %&gt;%\n    left_join(life, by = join_by(fun))\n}\n\nfunctions &lt;-\n  purrr::map(pkgs, list_functions_tbl) %&gt;%\n  list_rbind() %&gt;%\n  relocate(lifecycle, .after = pkg)\n\nslice_sample(functions, n = 10)\n\n# A tibble: 10 × 4\n   fun               pkg         lifecycle fun.len\n   &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;       &lt;int&gt;\n 1 reclass_date      lubridate   &lt;NA&gt;           12\n 2 position_identity ggplot2     &lt;NA&gt;           17\n 3 unpack            tidyr       &lt;NA&gt;            6\n 4 hash_file_sha1    cli         &lt;NA&gt;           14\n 5 sql               dbplyr      &lt;NA&gt;            3\n 6 theme_linedraw    ggplot2     &lt;NA&gt;           14\n 7 str_extract_all   stringr     &lt;NA&gt;           15\n 8 drive_api_key     googledrive &lt;NA&gt;           13\n 9 second&lt;-          lubridate   &lt;NA&gt;            8\n10 str_like          stringr     &lt;NA&gt;            8\n\n\nHere we have the function lengths, but it may also be interesting to see the number of constituent words that make up each function. For example, separate() is one word, whereas separate_wider_position() is three. We can achieve this using tidytext.\n\ncounts &lt;-\n  functions %&gt;%\n  mutate(fun2 = snakecase::to_snake_case(fun) %&gt;% str_replace_all(\"_\", \" \")) %&gt;%\n  tidytext::unnest_tokens(word, fun2, drop = FALSE) %&gt;%\n  count(pkg, fun, name = \"fun.words\")\n\nfunctions &lt;-\n  left_join(functions, counts, by = join_by(pkg, fun))\n\nThe last thing we’ll do is mark the “core” tidyverse so we can easily distinguish a key package like dplyr from a more niche one like xml2. Note that I’m including lubridate in the “core” tidyverse as it is due to be added in an upcoming update.\n\nfunctions &lt;-\n  functions %&gt;%\n  mutate(pkg.core = pkg %in% c(\"ggplot2\", \"dplyr\", \"tidyr\", \"readr\", \"purrr\", \"tibble\", \"stringr\", \"forcats\", \"lubridate\"),\n         .after = pkg)\n\nFunction Lengths\nNow we have our data in order, lets investigate the distributions of some of these data. First, let’s consider the function character length distributions of the different packages in Figure 1.\n\nfunctions %&gt;%\n  mutate(pkg = fct_reorder(pkg, fun.len, median),\n         color = if_else(pkg.core, \"Core\", \"Non-Core\")) %&gt;%\n  plot_ly(x = ~pkg, y = ~fun.len, colors = \"Dark2\") %&gt;%\n  add_boxplot(color = ~color) %&gt;%\n  layout(yaxis = list(title = \"Function Length\"),\n         xaxis = list(title = \"Package\"))\n\n\nFigure 1: The distributions of different function name lengths in tidyverse packages. Hover over each boxplot to see the summary statistics they represent.\n\n\nThe package with the longest median function length is rstudioapi with a median value of 14 characters. The shortest is crayon with a median of 7. Both of these packages are not “core” and, furthermore, they’re not particularly common to call directly! Looking at just the “core” tidyverse, ggplot2 has the highest median function length and lubridate has the lowest. Not particularly surprising when we compare mammoths like scale_linewidth_continuous() to small fries like ymd()!\nTable 1 shows the longest and shortest 10 functions in the tidyverse. The longest functions are from googlesheets4, although these are exported internal vctrs methods. The longest named function that anyone is likely to call commonly is the aforementioned scale_linewidth_continuous() at a massive 26 characters long. On the other end, the shortest function in the tidyverse is dplyr::n() at only one character.\n\nbind_rows(\n  \"longest\" = slice_max(functions, n = 10, order_by = fun.len),\n  \"shortest\" = slice_min(functions, n = 10, order_by = fun.len),\n  .id = \"type\"\n) %&gt;%\n  knitr::kable()\n\n\n\nTable 1: The longest and shortest function names in the tidyverse by characters.\n\n\n\n\n\n\n\n\n\n\ntype\nfun\npkg\npkg.core\nlifecycle\nfun.len\nfun.words\n\n\n\nlongest\nvec_ptype2.googlesheets4_formula\ngooglesheets4\nFALSE\nNA\n32\n6\n\n\nlongest\nvec_cast.googlesheets4_formula\ngooglesheets4\nFALSE\nNA\n30\n5\n\n\nlongest\ncli_progress_builtin_handlers\ncli\nFALSE\nNA\n29\n4\n\n\nlongest\ngetRStudioPackageDependencies\nrstudioapi\nFALSE\nNA\n29\n5\n\n\nlongest\nregisterCommandStreamCallback\nrstudioapi\nFALSE\nNA\n29\n4\n\n\nlongest\nsupports_star_without_alias\ndbplyr\nFALSE\nNA\n27\n4\n\n\nlongest\nlauncherPlacementConstraint\nrstudioapi\nFALSE\nNA\n27\n3\n\n\nlongest\nansi_has_hyperlink_support\ncli\nFALSE\nNA\n26\n4\n\n\nlongest\nscale_linewidth_continuous\nggplot2\nTRUE\nNA\n26\n3\n\n\nlongest\nscale_continuous_identity\nggplot2\nTRUE\nNA\n25\n3\n\n\nlongest\nscale_linetype_continuous\nggplot2\nTRUE\nNA\n25\n3\n\n\nlongest\nunregisterCommandCallback\nrstudioapi\nFALSE\nNA\n25\n3\n\n\nshortest\nn\ndplyr\nTRUE\nNA\n1\n1\n\n\nshortest\nno\ncli\nFALSE\nNA\n2\n1\n\n\nshortest\ndo\ndplyr\nTRUE\nsuperseded\n2\n1\n\n\nshortest\nid\ndplyr\nTRUE\ndefunct\n2\n1\n\n\nshortest\nam\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nhm\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nms\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nmy\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\npm\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\ntz\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nym\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nyq\nlubridate\nTRUE\nNA\n2\n1\n\n\nshortest\nor\nmagrittr\nFALSE\nNA\n2\n1\n\n\nshortest\n!!\nrlang\nFALSE\nNA\n2\nNA\n\n\nshortest\n:=\nrlang\nFALSE\nNA\n2\nNA\n\n\nshortest\nll\nrlang\nFALSE\nNA\n2\n1\n\n\nshortest\nUQ\nrlang\nFALSE\ndeprecated\n2\n1\n\n\n\n\n\n\nLooking at the distribution of the number of words in Figure 2, we can see that most tidyverse functions are made up of 2 words (e.g., separate_wider()). The wordiest two functions, however, have a massive 5 words. These are the relatively new fct_na_level_to_value() and fct_na_value_to_level() from forcats, both of which superseded fct_explicit_na() which itself was three words long.\nThe least wordy tidyverse functions actually have no words at all, those being: %+%, %&gt;%, %–%, %!&gt;%, %$%, %&lt;&gt;%, %@%, %||%, !!, !!!, %@%&lt;-, %|%, %&lt;~%, :=. As you can tell, they’re mostly magrittr pipes and rlang syntax stuff.\n\nfunctions %&gt;%\n  count(fun.words, pkg.core) %&gt;%\n  mutate(color = if_else(pkg.core, \"Core\", \"Non-Core\"),\n         fun.words = if_else(is.na(fun.words), 0, fun.words)) %&gt;%\n  plot_ly(x = ~factor(fun.words), y = ~n, colors = \"Dark2\") %&gt;%\n  add_bars(color = ~color) %&gt;%\n  layout(xaxis = list(title = \"Number of Words\"))\n\n\nFigure 2: Counts of the number of words in tidyverse functions. A function with 0 words has no English characters (e.g., %&gt;%).\n\n\nSuperseded Functions\nAll of that was interesting, but lets examine the original point of this post - are superseded functions shorter than new ones?\nFirst, lets make sure that we’re only looking at packages that are actually classified using lifecycle. Of the remaining packages, all of the NA values represent stable functions.\n\nfunctions2 &lt;-\n  filter(functions, any(!is.na(lifecycle)), .by = pkg) %&gt;%\n  mutate(lifecycle = replace_na(lifecycle, \"stable\"))\n\nWe’ll visuaise all of the lifecycle stages available, although the three key ones we should be looking at are:\n\nStable - functions that are in current use,\nSuperseded - functions that have newer versions that are now recommended (e.g., separate()), and\nExperimental - the newest functions that are in development (e.g., separate_wider_delim()).\n\nFigure 3 shows the distributions of different function lengths, similar to Figure 1 but with the function lifecycle instead of its package. Deprecated and superseded functions have similar mean lengths of around about 9.5, compared to stable functions’ 11.2 and experimental functions’ 14.2! This does imply that new functions are indeed getting longer - at least on average!\n\nfunctions2 %&gt;%\n  mutate(lifecycle = fct_reorder(lifecycle, fun.len, mean)) %&gt;%\n  mutate(fun.avglen = mean(fun.len, na.rm = TRUE), .by = lifecycle) %&gt;%\n  plot_ly(x = ~ lifecycle, y = ~ fun.len) %&gt;%\n  add_boxplot(name = \"Boxplot\", color = I(\"grey75\")) %&gt;%\n  add_lines(\n    y = ~ round(fun.avglen, 2),\n    legendgroup = \"marker\",\n    showlegend = FALSE,\n    color = I(\"red\")\n  ) %&gt;%\n  add_markers(\n    y = ~ round(fun.avglen, 2),\n    name = \"Mean Marker\",\n    legendgroup = \"marker\",\n    color = I(\"red\")\n  ) %&gt;%\n  layout(yaxis = list(title = \"Function Length\"))\n\n\nFigure 3: The distributions of function lengths at different lifecycle stages.\n\n\nA similar observation is seen in Figure 4, which shows that a whopping 52% of experimental tidyverse experimental functions are three words long, plus an extra 12% at four words. Conversely, only 10% of superseded functions are three words long and none are four words.\n\nfunctions2 %&gt;%\n  count(lifecycle, fun.words) %&gt;%\n  mutate(fun.words = replace_na(fun.words, 0),\n         lifecycle = fct_reorder(lifecycle, fun.words, median)) %&gt;%\n  mutate(n = n / sum(n), .by = lifecycle) %&gt;%\n  plot_ly(x = ~ lifecycle, y = ~ n) %&gt;%\n  add_bars(color = ~ factor(fun.words)) %&gt;%\n  layout(barmode = \"stack\",\n         yaxis = list(tickformat = '.0%'))\n\n\nFigure 4: The percentage of tidyverse functions at different lifecycle stages and numbers of words.\n\n\nNow to make this truly robust we should be comparing individual superseded functions with the functions that replaced them. To do this, I wrote out all of the superseded functions to a CSV and searched through the documentation to determine their replacements. A few notes before we continue:\n\nSome superseded functions simply weren’t replaced with anything in particular. For example, dplyr::with_groups() was replaced by the by/.by argument in functions like summarise() so won’t be analysed here. Many dplyr functions were effectively replaced by across() so are also ignored.\nA lot of readr functions were moved to meltr so aren’t considered here.\nThe purrr::map_dfr() family was replaced by a combination of map() and list_rbind(). It wouldn’t be fair to compare these because of course two functions will be longer than one!\ntidyr possesses unnest_legacy() and nest_legacy() which are listed as superseded but, in reality, they were never labelled “legacy” originally, so these are discounted.\nSome functions were also replaced by multiple functions. For example, do() is replaced by either reframe(), nest_by() or pick(). In that case, all three are used.\n\n\nsupers &lt;- \n  read_csv(here::here(\"posts/2023-03-04-tidyverseLength/superseded.csv\")) %&gt;%\n  left_join(select(functions, -pkg.core, -lifecycle)) %&gt;%\n  left_join(\n    select(functions, -pkg.core, -lifecycle),\n    by = join_by(oldfun == fun, pkg),\n    suffix = c(\"_new\", \"_old\")\n  ) %&gt;%\n  drop_na(fun) %&gt;%\n  filter(fun != \"across\",\n         !str_detect(oldfun, \"legacy\"))\n\nslice_sample(supers, n = 10)\n\n# A tibble: 10 × 7\n   oldfun      pkg   fun                  fun.len_new fun.word…¹ fun.l…² fun.w…³\n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                      &lt;int&gt;      &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 simplify    purrr list_simplify                 13          2       8       1\n 2 env_bury    rlang set_env                        7          2       8       2\n 3 flatten_dfc purrr list_cbind                    10          2      11       2\n 4 transpose   purrr list_transpose                14          2       9       1\n 5 flatten_lgl purrr list_c                         6          2      11       2\n 6 gather      tidyr pivot_longer                  12          2       6       1\n 7 top_frac    dplyr slice_min                      9          2       8       2\n 8 do          dplyr pick                           4          1       2       1\n 9 sample_frac dplyr slice_sample                  12          2      11       2\n10 separate    tidyr separate_wider_delim          20          3       8       1\n# … with abbreviated variable names ¹​fun.words_new, ²​fun.len_old,\n#   ³​fun.words_old\n\n\nNow, finally, lets compare the old and new! Figure 5 shows the function lengths of specific superseded functions and the functions they were replaced with. Most functions marked superseded that were directly replaced by one or more functions are in the dplyr, purrr and tidyr packages, and many of them do indeed get longer! The function that increased in length the most was separate(), which became the massive separate_wider_position(), gaining 15 additional characters. The function that decreased in length the most was coord_quickmap(), which was replaced by the petite coord_sf(), losing 6 characters. In this dataset, the median change between the old and new functions was an increase in 3 characters.\n\nplt &lt;- \n  supers %&gt;%\n  mutate(\n    id = row_number(),\n    diff = fun.len_new - fun.len_old,\n    diff = if_else(diff &gt; 0, paste(\"+\", diff, sep = \"\"), paste(diff)),\n    tooltip = str_glue(\"`{oldfun}()` to `{fun}()` ({diff})\"),\n    color = if_else(fun.len_new &gt; fun.len_old, \"Increase\", \"Decrease\")\n  ) %&gt;%\n  pivot_longer(c(fun.len_new, fun.len_old)) %&gt;%\n  mutate(\n    name = case_match(name,\n                      \"fun.len_new\" ~ \"New\",\n                      \"fun.len_old\" ~ \"Old\"),\n    name = factor(name, c(\"Old\", \"New\"))\n  ) %&gt;%\n  ggplot(aes(x = name, y = value, color = color)) +\n  geom_line_interactive(aes(\n    group = id,\n    tooltip = tooltip,\n    data_id = tooltip\n  ),\n  hover_nearest = TRUE) +\n  geom_point_interactive(\n    aes(data_id = tooltip),\n    shape = 21,\n    fill = \"white\",\n    stroke = 1\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_x_discrete(expand = expansion()) +\n  expand_limits(y = 0) +\n  theme_minimal() +\n  theme(\n    panel.spacing = unit(.7, \"cm\"),\n    legend.position = \"top\",\n    aspect.ratio = 1\n  ) +\n  facet_wrap(vars(pkg), scales = \"free_x\") +\n  labs(x = NULL, y = \"Function Length\", color = NULL)\n\ngirafe(ggobj = plt,\n       options = list(\n         opts_hover(\"\"),\n         opts_hover_inv(\"opacity:0.25\")\n       ))\n\n\nFigure 5: The change in the length of superseded function names. Hover over the lines to identify the specific functions.\n\n\nConclusion\nThis blog post was on a bit of a silly topic, but I’m hopeful that it was a useful demonstration of various tidyverse functions and the plotly package for interactive data visualisation. The key conclusion that I’m taking is that, while there is a lot of variation, superseded functions do tend to be shorter than the functions that replace them! Yesterday we had separate(), today we have separate_wider_position(), and tomorrow separate_wider_integer_position_ignore_toofew_removecols() may be on the cards!"
  },
  {
    "objectID": "talks/2023_satRdayslondon.html",
    "href": "talks/2023_satRdayslondon.html",
    "title": "“Put it on a map!” Developments in air quality data analysis",
    "section": "",
    "text": "Access\n\nSlides are hosted at https://jack-davison.quarto.pub/satrdays-london-2023/#/section.\nSlides were created using RevealJS/Quarto, with the source code found at https://github.com/jack-davison/satRdaysLondon2023-openairmaps.\nA recording will soon be made available.\nAbstract\n\nAn understanding of air quality is crucial as it can have significant public health, environmental and economic effects. However, air quality data is complex, constantly changing in space and time, and influenced by a myriad of factors such as meteorology and human activity. This makes air quality analysis challenging, and communicating the results of this analysis more challenging still!\nJust over a decade ago, the openair package was authored to provide an open‐source toolkit to help air quality practitioners get the most out of their data, and is still used widely in academia, consultancy and industry today. While openair itself has not changed hugely in recent years, much thought has been put into extending it through leveraging more recent tools and packages.\nIn this talk I will discuss how we have recently married leaflet and openair to create effective, interactive air quality maps. In particular, I’ll discuss the development of the openairmaps package – a toolset which makes it easy to create interactive “directional analysis” maps to help explore the geospatial context of pollution monitoring data.\n\nContext\n“SatRdays” is a low-cost, not-for-profit, locally organised R conference series. This talk was given at SatRdays London 2023, and was mainly centred around the openairmaps R package, with wider framing around updating and using “legacy” packages."
  }
]