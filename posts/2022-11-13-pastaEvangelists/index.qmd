---
title: "Scraping the Pasta Academy"
description: |
  Overcoming the lack of a "filter" option on an experience booking website through web scraping.
image: pasta.jpg
categories: [rvest, webscraping]
author: "Jack Davison"
date: 2022-11-13
twitter-card:
  image:  "pasta.jpg"
  card-style: summary_large_image
execute: 
  warning: false
  message: false
  error: false
  freeze: true
editor_options: 
  chunk_output_type: console
---

![](pasta.jpg)

# Purpose

Recently, my partner and I were looking to book a cooking experience as a Christmas treat, and we settled on the excellently reviewed [Pasta Academy](https://pastaevangelists.com/collections/pasta-academy) in London. We were looking for a beginner class in early 2023, but we were immediately met with an issue --- there was no way to filter the booking list!

We could have sat and go through the list manually, perhaps noting down each of the beginner classes when we found them. This sounded like a bit of a faff, so I instead thought I'd scrape all of the courses and put them into a table. Some might argue that writing an R script to do this is *even more* of a faff, but I did it anyway!

This blog post shows how I scraped the data from the Pasta Academy website using `{rvest}` and then went on to tidy it using the `{tidyverse}`. You may find this a nice introduction to web scraping in R, and a nice application of data skills in "real life".

# Web Scraping with `{rvest}`

```{r}
#| label: libs
#| message: false
#| warning: false
library(rvest)
library(tidyverse)
```

We'll start by scraping the first page. To start using `{rvest}`, we first need to define a session. This simulates a user interacting with a website.

```{r}
#| label: session
pasta_session <-
  session("https://pastaevangelists.com/collections/pasta-academy?page=1")
```

We'll now need to pull each individual element of interest from the Pasta Academy website. One of the easiest ways of finding the correct HTML at which to aim your `{rvest}` functions is by using Chrome's developer tools (CTRL-SHIFT-I on Windows) or your browser's equivalent.

![A screenshot of the use of Chrome's developer tools.](developer-tools.png){#fig-devtools}

Once we have identified the correct HTML tags to use, we can use `html_elements()` to grab those items, and then `html_text2()` to extract the text from it. For example, the below line grabs each lesson title from the first page of the Pasta Academy website.

```{r}
#| label: elements
html_elements(pasta_session, ".product-item-row__meta-title") |> html_text2()
```

All we need to do is repeat this until we have all of the information we want. In this case, all I need is the course title, the date/time, the price, and whether or not it is sold out. The button text is a useful proxy for that last item, as it reads "SEE INFO" if a course is fully booked and "BOOK NOW" if there are spaces left.

```{r}
#| label: all-elements
name <- html_elements(pasta_session, ".product-item-row__meta-title") |> html_text2()

date <- html_elements(pasta_session, ".product-item-row__meta-title-sub") |> html_text2()

price <- html_elements(pasta_session, "span.price") |> html_text2()

button <- html_elements(pasta_session, ".product-item-row__cta-btn") |> html_text2()

dplyr::tibble(
  name = name, 
  date = date,
  price = price, 
  button = button
) |>
  dplyr::glimpse()
```

Now all four pieces of information can be scraped from one page, we can use `{purrr}` to scrape this information from *all* of the pages. We'll write a function which takes one argument, `id`, which is used to select the specific page of the Pasta Academy website.

```{r}
#| label: purrr
scrape_pasta <- function(id){
  x <- session(str_glue("https://pastaevangelists.com/collections/pasta-academy?page={id}"))
  
  tibble(
    name = html_elements(x, ".product-item-row__meta-title") |> html_text2(),
    date = html_elements(x, ".product-item-row__meta-title-sub") |> html_text2(),
    price = html_elements(x, "span.price") |> html_text2(),
    fully_booked = html_elements(x, ".product-item-row__cta-btn") |> html_text2()
  ) |>
    mutate(page_id = id)
}

raw_pasta <- map_dfr(1:19, scrape_pasta)
```

# Tidying Data

We wouldn't eat raw pasta, and we won't want to work with `raw_pasta` as it currently exists. Lets use the `{tidyverse}` to tidy this data up a bit.

First, lets get rid of some of the dodgy formatting --- we'll drop the `\r`, vertical bars, question marks, and the PASTA ACADEMY branding from all character columns. 

```{r}
#| label: strremove
pasta <- 
  raw_pasta |>
  mutate(across(where(is.character), 
                ~ str_remove_all(.x, "PASTA ACADEMYâ„¢|\r|\\||\\?") |> 
                  str_squish()))
pasta
```

An easier step --- lets format the `price` as numeric data, and `fully_booked` as logical. Lets also make `name` all lower-case.

```{r}
#| label: fmtAll
pasta <-
  pasta |> 
  mutate(name = tolower(name),
         price = parse_number(price),
         fully_booked = if_else(fully_booked == "SEE INFO", T, F))

pasta
```

It'd be useful to have the date as a properly formatted date-time column. Sadly, this column is not consistently formatted. The two issues are:

* Sometimes the time has a comma before it, but not always
* Sometimes the year is present, but not always

To deal with this, we'll take the following steps:

1.    Extract the time & year from the `date` using regex.
2.    Fill any missing years. This might not be perfect if there are missing years between December and January, but for our purposes we can live with this and can cross-reference with `page_id` if we need to double check.
3.    Reformat the `date` by performing string transformations.
4.    Parse the `date` as a date-time using `{lubridate}`.

In practice, this looks like this:

```{r}
#| label: fmtDate
pasta <-
  pasta |>
  
  # extract year/time
  mutate(
    time = str_extract(date, "[0-9][0-9]:[0-9][0-9]"),
    year = str_extract(date, "[0-9][0-9][0-9][0-9]")
  ) |>
  
  # fill year
  fill(year, .direction = "down") |>
  
  # reformat date 
  mutate(date = str_remove_all(date, time) |>
           str_remove_all(year) |>
           str_remove(", ") |>
           str_squish() |> 
           str_remove(", am")) |> 
  separate(date, c("day", "date"), sep = " ", extra = "merge") |> 
  unite(date, date, year, time, sep = " ") |> 
  mutate(date = str_remove(date, ",")) |> 
  
  # parse date as date-time
  mutate(date = lubridate::parse_date_time(date, 
                                           orders = c("BdY HM", "dBY HM"))) |>
  
  # drop date
  select(-day)

pasta
```

# Using the data

Now that the data is in a tidy format, lets find out when the available beginner courses are being held. Evenings are also our preference:

```{r}
#| label: classes
potential_classes <- 
  pasta |> 
  filter(str_detect(name, "beg"),
         lubridate::hour(date) > 14,
         !fully_booked)

potential_classes
```

And there they are!

Just as we now have the data to hand, lets see what some common themes are in the classes. Disregarding words that aren't particularly unique or descriptive like "taste", "beginners", "class", "morning", we can learn that there seems to be a lot inspired by the cuisines of Puglia, Rome, and Sardinia.

```{r}
counts <-
  pasta |> 
  tidytext::unnest_tokens(word, name) |> 
  anti_join(tidytext::get_stopwords()) |> 
  count(word, sort = T)

counts
```

@fig-bars visualises the above data using `{ggiraph}`, showing the most and least common words in these pasta making class names. Hover over the bars to read the exact values!

```{r}
#| label: fig-bars
#| fig-cap: "Frequency of different words in Pasta Academy class names."
library(ggiraph)

plt <-
  counts |>
  mutate(word = fct_reorder(word, n)) |>
  ggplot(aes(y = word, x = n)) +
  geom_col_interactive(aes(fill = n, tooltip = n), show.legend = FALSE) +
  theme_classic() +
  scale_x_continuous(expand = expansion()) +
  labs(y = NULL, x = "Count (n)")

girafe(ggobj = plt)
```

