[
  {
    "path": "posts/2021-03-16-tidytuesday-2021-week-12-steam-users/",
    "title": "Teaching Myself to Web Scrape (TidyTuesday 2021 Week 12: Steam Users)",
    "description": "Augmenting a lacking TidyTuesday data set with data from the web.",
    "author": [
      {
        "name": "Jack Davison",
        "url": {}
      }
    ],
    "date": "2021-03-16",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nOur Data\r\nInitial Analysis\r\n\r\nWeb Scraping\r\nThe gentlest introduction to scraping HTML tables\r\nThe main event\r\nFurther Analysis\r\n\r\nConcluding Remarks\r\n\r\nIntroduction\r\nOur Data\r\nThis week’s #TidyTuesday data set is quite a simple one. Let’s give it a look:\r\n\r\n\r\nShow code\r\n\r\nglimpse(games)\r\n\r\n\r\nRows: 83,631\r\nColumns: 7\r\n$ gamename      <chr> \"Counter-Strike: Global Offensive\", \"Dota 2\", ~\r\n$ year          <dbl> 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021~\r\n$ month         <chr> \"February\", \"February\", \"February\", \"February\"~\r\n$ avg           <dbl> 741013.24, 404832.13, 198957.52, 120982.64, 11~\r\n$ gain          <dbl> -2196.42, -27839.52, -2289.67, 49215.90, -2437~\r\n$ peak          <dbl> 1123485, 651615, 447390, 196799, 224276, 13362~\r\n$ avg_peak_perc <chr> \"65.9567%\", \"62.1275%\", \"44.4707%\", \"61.4752%\"~\r\n\r\nWe can see that there is effectively 4 pieces of information in this data set - the name of the video game, the date (a combination of year and month), and the monthly average and peak number of players. The other two columns could be recalculated from the other columns, but are nice to have.\r\nThere is other data that I’d maybe have wanted in an ideal world - the game publisher/designer may be one, game review ratings at every month, the game genre, etc. But we aren’t always in an ideal world and this is what we have got!\r\nLet’s first do some initial cleaning. There are two issues with this data set:\r\navg_peak_perc is a character owing to the percentage sign - we’ll parse the number from this. I’m not actually going to use this column, but its nice to do so everything is in order.\r\nyear and month can be combined into one column, which we’ll make sure is a date. The lubridate package has the my() function which we’ll use alongside glue::glue().\r\n\r\n\r\nShow code\r\n\r\ngames = games %>%\r\n  mutate(avg_peak_perc = parse_number(avg_peak_perc)/100,\r\n         yearmon = lubridate::my(glue::glue(\"{month}/{year}\")),\r\n         .keep = \"unused\") %>%\r\n  relocate(yearmon, .after = gamename)\r\n\r\nglimpse(games)\r\n\r\n\r\nRows: 83,631\r\nColumns: 6\r\n$ gamename      <chr> \"Counter-Strike: Global Offensive\", \"Dota 2\", ~\r\n$ yearmon       <date> 2021-02-01, 2021-02-01, 2021-02-01, 2021-02-0~\r\n$ avg           <dbl> 741013.24, 404832.13, 198957.52, 120982.64, 11~\r\n$ gain          <dbl> -2196.42, -27839.52, -2289.67, 49215.90, -2437~\r\n$ peak          <dbl> 1123485, 651615, 447390, 196799, 224276, 13362~\r\n$ avg_peak_perc <dbl> 0.659567, 0.621275, 0.444707, 0.614752, 0.5249~\r\n\r\nInitial Analysis\r\nNow there are two ways we could go with this - we could look at lots of different games and compare between them, or we could have a deeper look at a single game. There are 1258 games in this data set, but whenever I plot the timeseries I am desperate to aggregate them in some way, and without genre or publisher or the like I keep getting stuck.\r\nInstead, let’s look at a single game - Dota 2. It is updated regularly and I’m familiar-ish with it. I stopped playing after I started university because the games were too long and the community too toxic, but I have a relatively good grasp of what Dota 2 is all about. Let’s plot up the data.\r\n\r\n\r\nShow code\r\n\r\nggthemr(palette = \"chalk\", text_size = 11, layout = \"minimal\", type = \"outer\")\r\n\r\ngames %>%\r\n  filter(gamename == \"Dota 2\") %>%\r\n  ggplot(aes(x = yearmon, y = avg)) +\r\n  geom_line() +\r\n  geom_point() +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  labs(y = \"Monthly Concurrent Dota 2 Players\", x = \"Date\")\r\n\r\n\r\n\r\n\r\nSo the average player count seems to peak in 2016 and has been gently decreasing since, with a little burst of activity in 2019-ish. The immediate question I have is - what is prompting the increases and decreases in player count? Updates are a natural possibility, but we don’t have data on that to hand… or do we?\r\nWeb Scraping\r\nThe gentlest introduction to scraping HTML tables\r\nThe Dota 2 Gamepedia (wiki) has a wonderful table of all of the major versions of the game. You can see it here. Our problem, however, is that it is a HTML table and not inside of R - yet!\r\nWeb scraping is something that I’ve always assumed would be extremely difficult, but it turns out that it couldn’t be simpler with the rvest package. I’ll demonstrate how I pulled that table from the internet and to my R session without having to manually copy it out at all.\r\nFirst, I’m going to use read_html(), which is imported from the xml2 package. We can see the output of this below.\r\n\r\n\r\nShow code\r\n\r\nlibrary(rvest)\r\n\r\nread_html(\"https://dota2.gamepedia.com/Game_Versions\")\r\n\r\n\r\n{html_document}\r\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\r\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; cha ...\r\n[2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 n ...\r\n\r\nNow if I’m entirely honest, my ability to read HTML is not the best - I can get by with basic stuff, but actual websites have so much going on it can get pretty complicated. But we don’t actually have to manually scan through this output, we can pull specific pieces out of it using rvest::html_nodes() provided with the argument “table”.\r\n\r\n\r\nShow code\r\n\r\nhtml_nodes(read_html(\"https://dota2.gamepedia.com/Game_Versions\"), \"table\")\r\n\r\n\r\n{xml_nodeset (5)}\r\n[1] <table class=\"wikitable\" style=\"text-align:center\" width=\"75%\"> ...\r\n[2] <table><tbody><tr style=\"vertical-align:top;\">\\n<td width=\"120p ...\r\n[3] <table><tbody><tr style=\"vertical-align:top;\">\\n<td width=\"120p ...\r\n[4] <table class=\"notanavbox\" cellspacing=\"0\" style=\";\"><tbody><tr> ...\r\n[5] <table cellspacing=\"0\" class=\"nowraplinks hlist mw-collapsible\" ...\r\n\r\nSo we’re still at the stage where we’ve got a load of unformatted HTML, but at least we now have a list of tables on this web page. I can only visibly see two things that are recognisable as tables myself, so there’ll be some underlying HTML that is using a table structure for something that’s not really a table.\r\nAnyway, our next step is to use another rvest function, html_table(), to format our table into a usable tibble in R. My plan was to use the robust scientific process of trying each one in turn until I find the correct one, and I was delighted to find it was the first identified table in this list of five.\r\n\r\n\r\nShow code\r\n\r\nhtml_table(html_nodes(read_html(\"https://dota2.gamepedia.com/Game_Versions\"), \"table\")[1]) %>%\r\n  glimpse()\r\n\r\n\r\nList of 1\r\n $ : tibble [75 x 3] (S3: tbl_df/tbl/data.frame)\r\n  ..$ Version   : chr [1:75] \"7.28c\" \"7.28b\" \"7.28a\" \"7.28\" ...\r\n  ..$ Highlights: chr [1:75] \"Nerfed\" \"Nerfed      \\nBuffed                              \\nRebalanced\" \"Nerfed                \\nBuffed\" \"Added new heroes: HoodwinkNerfed              \\nBuffed                                                   \\nReba\"| __truncated__ ...\r\n  ..$ Patch Date: chr [1:75] \"2021-02-19\" \"2021-01-10\" \"2020-12-22\" \"2020-12-17\" ...\r\n\r\nMy heart swells with pride - we’ve just pulled a HTML table from the internet and got it into R. This isn’t quite the end of the story - there is a second table for updates before version 7.0.0, and we’ll need to do some data tidying/cleaning - but this new challenge is behind me.\r\nThe main event\r\nLet’s do it “properly” now, in a “proper” workflow.\r\n\r\n\r\nShow code\r\n\r\nrecent_updates = html_nodes(read_html(\"https://dota2.gamepedia.com/Game_Versions\"),\r\n                            \"table\")[1] %>%\r\n  html_table() %>%\r\n  .[[1]] %>%\r\n  janitor::clean_names()\r\n\r\nolder_updates = html_nodes(read_html(\"https://dota2.gamepedia.com/Game_Versions/6.70_to_6.88f\"),\r\n           \"table\") %>%\r\n  html_table() %>%\r\n  .[[1]] %>%\r\n  janitor::clean_names() %>%\r\n  .[-1,] %>%\r\n  select(-patch_date_2)\r\n  \r\nupdates = bind_rows(recent_updates, older_updates) %>%\r\n  mutate(patch_date = lubridate::ymd(patch_date),\r\n         yearmon = lubridate::floor_date(patch_date, unit = \"month\"))\r\n\r\nversion_updates = updates %>%\r\n  mutate(version = parse_number(version)) %>%\r\n  with_groups(version, \r\n              ~filter(.x, patch_date == min(patch_date))) %>% \r\n  distinct(version, yearmon)\r\n\r\n\r\n\r\nI’ll explain what is going on here:\r\nI use the workflow above to read in both tables.\r\nI use janitor::clean_names() to make the column headers easier to use (the function puts them all in snake_case).\r\nIn the case of the pre-v7.0.0 updates, Dota 1 and 2 were being updated concurrently, so there’s a second row of headers specifying which game is being talked about. In practice this means that the first line of the table generated by html_table() isn’t really meaningful and can be dropped, and as we’re considering Dota 2 only I can bin the second patch_date column as that refers to Dota 1.\r\nI merge both of these tibbles together using dplyr::bind_rows(), and use the lubridate package once more to ensure that the patch_date column is a datetime and ensure that there is a yearmon column to merge with our existing data.\r\nThere are 143 updates in our data set. I reduce this to 36 by only considering “full” updates (e.g. v7.0.5) where typically heroes and items are added or reworked, rather than the alphabetically labelled updates (e.g. v7.0.5a) which are typically limited to balance fixes. I use the experimental dplyr::with_groups() function to achieve this.\r\nWe did a lot there! It’s thankfully now easy to merge this with our original Dota 2 data. I’m going to add a new logical column called popular, which is effectively whether the update led to an increase or decrease in player counts.\r\n\r\n\r\nShow code\r\n\r\ndota2 = games %>%\r\n  filter(gamename == \"Dota 2\") %>%\r\n  left_join(version_updates, by = \"yearmon\") %>%\r\n  arrange(yearmon) %>%\r\n  mutate(popular = if_else(!is.na(version), lead(gain) > 0, NA))\r\n\r\nglimpse(dota2)\r\n\r\n\r\nRows: 112\r\nColumns: 8\r\n$ gamename      <chr> \"Dota 2\", \"Dota 2\", \"Dota 2\", \"Dota 2\", \"Dota ~\r\n$ yearmon       <date> 2012-07-01, 2012-08-01, 2012-09-01, 2012-10-0~\r\n$ avg           <dbl> 52721.05, 55768.61, 61867.68, 75965.44, 101077~\r\n$ gain          <dbl> NA, 3047.56, 6099.07, 14097.77, 25111.99, 2084~\r\n$ peak          <dbl> 75041, 108689, 118724, 171860, 169631, 213521,~\r\n$ avg_peak_perc <dbl> 0.702563, 0.513103, 0.521105, 0.442019, 0.5958~\r\n$ version       <dbl> NA, NA, NA, 6.76, NA, NA, NA, NA, NA, NA, NA, ~\r\n$ popular       <lgl> NA, NA, NA, TRUE, NA, NA, NA, NA, NA, NA, NA, ~\r\n\r\nFurther Analysis\r\nI’m going to start by recreating our earlier plot, but this time I’m going to encode our version updates data. I’ll also pretty it up a little!\r\n\r\n\r\nShow code\r\n\r\ndota2 %>%\r\n  ggplot(aes(x = yearmon, y = avg)) +\r\n  geom_line() +\r\n  geom_point(aes(shape = popular), fill = \"#3c3c3c\", color = \"#3c3c3c\", size = 3) +\r\n  geom_point(aes(color = popular, fill = popular, shape = popular), size = 2) +\r\n  scale_color_manual(values = c(\"#DB5461\", \"#2AF5FF\"), na.value = \"#D8D5DB\") +\r\n  scale_fill_manual(values = c(\"#DB5461\", \"#2AF5FF\"), na.value = \"#D8D5DB\") +\r\n  scale_shape_manual(values = c(25, 24), na.value = 21) +\r\n  scale_y_continuous(labels = scales::comma, expand = expansion(mult = c(0,.1)), sec.axis = sec_axis(~., labels = scales::comma)) +\r\n  expand_limits(y = 0) +\r\n  labs(x = NULL, y = NULL, \r\n       title = \"Monthly Average Concurrent Dota 2 players\",\r\n       subtitle = \"Months with updates are marked using triangles.\\nA red downwards triangle indicates an unpopular update, where the player numbers decreased.\\nA blue upwards triangle indicates a popular update, where the player numbers increased.\") +\r\n  theme(panel.grid.major.y = element_line(linetype = 2, color = \"grey40\"),\r\n        axis.line.y = element_blank(),\r\n        legend.position = \"none\", \r\n        plot.title.position = \"plot\")\r\n\r\n\r\n\r\n\r\nSo there’s a fair scatter of popular and unpopular updates throughout the history of the game. Initially all of the updates were popular, but we can rationalise that as initial hype as the game grew and people migrated from Dota 1. After that there were a series of unpopular updates until 2017, where the popularity of updates started almost alternating.\r\nLet’s make a more basic plot to illustrate this.\r\n\r\n\r\nShow code\r\n\r\ndota2 %>%\r\n  mutate(l_gain = lead(gain)) %>%\r\n  drop_na(version) %>%\r\n  ggplot(aes(x = version, y = l_gain)) +\r\n  geom_segment(color = swatch()[1], aes(yend = 0, xend = version)) +\r\n  geom_point(aes(color = l_gain > 0)) +\r\n  geom_hline(yintercept = 0) +\r\n  labs(x = \"Version\", y = \"Gain in average monthly players\") +\r\n  scale_color_manual(values = c(\"#DB5461\", \"#2AF5FF\"), na.value = \"#D8D5DB\") +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\nI’m now wondering which updates had the biggest effect on the player count? Let’s try pulling out the top, say, five.\r\n\r\n\r\nShow code\r\n\r\nbig_updates = dota2 %>%\r\n  mutate(l_gain = lead(gain),\r\n         a_gain = abs(l_gain)) %>% \r\n  drop_na(version) %>%\r\n  arrange(desc(a_gain)) %>%\r\n  head(5) %>%\r\n  select(l_gain, version)\r\n\r\nbig_updates\r\n\r\n\r\n# A tibble: 5 x 2\r\n   l_gain version\r\n    <dbl>   <dbl>\r\n1  89163.    7.21\r\n2  56153.    7.25\r\n3  54287.    6.84\r\n4 -48014.    7.09\r\n5  43985.    7.13\r\n\r\nThe Dota 2 wiki unhelpfully has images rather than text in the table to describe the updates. If it did have text, now would be the time to semi_join() this onto our updates data and read what was included in these versions, but I’ll just have to read them myself instead.\r\nv7.21 is an interesting leader because nothing really stands out about it. I found a Reddit thread that suggests v7.20 reworked a load of the heroes and 7.21 rolled back some of the changes.\r\nv7.25 has the usual array of hero updates, but also changed how All Pick (the “default” mode to pick heroes at the beginning of a match) worked.\r\nv6.84 is more what I expected the top 2 to be - a load of reworks and additions to items and heroes. A seemingly pretty big update.\r\nv7.09 confuses me most. Seemingly coinciding the biggest drop in players in this analysis, the only highlights were a buff to the animal courier that carries hero items and a rework to an early game item.\r\nv7.13 again there is little to report.\r\nHmm.\r\nConcluding Remarks\r\nThe main thing I’ve taken from this activity is that web scraping isn’t necessarily as scary as I thought it was, at least for HTML that already exists as a table.\r\nAs far as the analysis goes, I did a bit of tidying, cleaning and joining and tried to identify which updates might have caused the biggest changes in player numbers. My initial hypothesis going in was that the most dramatic changes would happen alongside big updates where heroes are added or reworked, but that doesn’t seem to be the case.\r\nI think there are key weaknesses in the way I conducted this very basic analysis:\r\nUnavoidably, player counts aren’t just related to updates. Interest in games wax and wane for lots of reasons, including attitudes to the company and what isn’t being updated as much as what is. It’s tricky to really include this in analysis without a lot of further reading.\r\nTo join the data easily without letting updates “bunch up” I rounded the update date down to the nearest month. There is an obvious weakness here in that an update that came on the 25th of January isn’t going to have an effect for the 1-24th. If I wanted to really get more insight out of this I’d have to carefully consider how far through each month each update was.\r\nAs the data is monthly, it is hard to really capture the effect of a game update. It is intuitive that player numbers may spike the day or even the week after a big update, and then either settle down if nothing that exciting changed or perhaps stay high/low depending on the opinion of the player-base. We don’t quite see that fine detail in monthly data.\r\nIf I wanted to spend more time on this, I think it’d be interesting to do a bit more:\r\nCould you predict the “popularity” of an update (how big/small the gain is) based on features of the update (could just be verbs - “Added”, “Removed”, “Buffed”, “Nerfed”, “Reworked”, etc.) - what kind of update has the biggest effect?\r\nIs there a better way to understand gain vs just month-to-month? Is there a way to smooth it a bit so you get more of the context of each update? e.g. it is possible that some of my “most/least popular” updates were just in the wake of the actual big updates and claimed the glory.\r\nDota 1 isn’t on Steam, so I doubt the data is out there, but I wonder if when the same or similar updates were being made to both whether they were received the same way by both communities. Someone would have to be quite canny with time series to remove the effect of players migrating from one to the other though.\r\nRegardless, we’ve done a bit of web scraping, and that’s the main thing.\r\nHappy #TidyTuesday!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-16-tidytuesday-2021-week-12-steam-users/tidytuesday-2021-week-12-steam-users_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-03-16T17:09:10+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/",
    "title": "An Ode to Data Tidying (TidyTuesday 2021 Week 6: HBCU Enrolment)",
    "description": "A disucssion on data tidying and cleaning, with applications to a messy, unfamiliar data set.",
    "author": [
      {
        "name": "Jack Davison",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "tidytuesday",
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction (or, “What do we mean by Tidy?”)\r\nThe HBCU Data\r\nWhat are the issues?\r\nCleaning and Tidying\r\nData Analysis\r\n\r\nConclusion\r\n\r\nIntroduction (or, “What do we mean by Tidy?”)\r\nThis week’s TidyTuesday is related to HBCU Enrolment. I will have to start by admitting my ignorance - I had no real idea what “HBCU” actually meant, which I attribute to being from the UK. However, datasets on American culture or politics that I’m not very familiar with has never stopped me before! So I sat down to give it a look. To my horror - or, perhaps, delight - the dataset was not particularly tidy. I will admit, there is something delightful about tackling a particularly untidy data set and turning it into something beautiful and easy to work with.\r\nWhat do I mean by tidy data? The tidyr vingette defines it as follows:\r\n\r\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.\r\n\r\nA variable contains all values that measure some unique feature - height, weight, length, temperature. An observation ties together the different values into one observational unit - a date, an object, a person. For the sake of example, say we have a data set of secondary school children who have all sat their end of year exams in several subjects that are all scored out of 100. We may be tempted to lay the data out in a wide format, as I have below:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\n\r\ntab = data.frame(student = c(\"Amy\", \"Barry\", \"Colin\"),\r\n                 english = round(rnorm(n = 3, mean = 50, sd = 20)),\r\n                 maths = round(rnorm(n = 3, mean = 50, sd = 20)),\r\n                 science = round(rnorm(n = 3, mean = 50, sd = 20)))\r\n\r\nknitr::kable(tab) %>%\r\n  kableExtra::kable_styling()\r\n\r\n\r\n\r\nstudent\r\n\r\n\r\nenglish\r\n\r\n\r\nmaths\r\n\r\n\r\nscience\r\n\r\n\r\nAmy\r\n\r\n\r\n40\r\n\r\n\r\n68\r\n\r\n\r\n38\r\n\r\n\r\nBarry\r\n\r\n\r\n53\r\n\r\n\r\n52\r\n\r\n\r\n64\r\n\r\n\r\nColin\r\n\r\n\r\n48\r\n\r\n\r\n56\r\n\r\n\r\n33\r\n\r\n\r\nHowever, is this tidy? Our observations are the students, yes, but our variables are not student, maths, english and history. The “real” variables should be student, subject and mark.\r\n\r\n\r\nShow code\r\n\r\ntab %>%\r\n  pivot_longer(-student, names_to = \"subject\", values_to = \"mark\") %>%\r\n  knitr::kable() %>%\r\n  kableExtra::kable_styling()\r\n\r\n\r\n\r\nstudent\r\n\r\n\r\nsubject\r\n\r\n\r\nmark\r\n\r\n\r\nAmy\r\n\r\n\r\nenglish\r\n\r\n\r\n40\r\n\r\n\r\nAmy\r\n\r\n\r\nmaths\r\n\r\n\r\n68\r\n\r\n\r\nAmy\r\n\r\n\r\nscience\r\n\r\n\r\n38\r\n\r\n\r\nBarry\r\n\r\n\r\nenglish\r\n\r\n\r\n53\r\n\r\n\r\nBarry\r\n\r\n\r\nmaths\r\n\r\n\r\n52\r\n\r\n\r\nBarry\r\n\r\n\r\nscience\r\n\r\n\r\n64\r\n\r\n\r\nColin\r\n\r\n\r\nenglish\r\n\r\n\r\n48\r\n\r\n\r\nColin\r\n\r\n\r\nmaths\r\n\r\n\r\n56\r\n\r\n\r\nColin\r\n\r\n\r\nscience\r\n\r\n\r\n33\r\n\r\n\r\nThere are benefits to having data in this format. For example, we could group_by() the subject and summarise() a mean and standard deviation. We could filter() for the students who scored less than 50 in a subject who may need extra help. If we had another data set of the subject teachers and the students they teach, we could left_join() these on to see which teachers are helping the students to score the top marks. All of this would be much, much more difficult in the wide format we had the data in before.\r\nNote that there are some situations where we might want the data wide - for example, what if we wanted to plot english marks vs maths marks? Or perhaps perform correlations between all of the different subjects? In the former case we can easily pivot_wider() just for this job. In the latter case we could also pivot_wider(), but there is also the widyr package that can just do it from a tidy format anyway!\r\nI like to draw a distinction between data tidying and data cleaning - two similar words, but I think have subtly different uses. If I was to tidy my desk, I’d reorganise what’s on it - file my papers away, collect all my stationary, untangle my cables; all things to help make my work easier and more efficient. If I was to clean my desk, I’d remove rubbish, give it a wipe down, remove general detritus; all things that improve the quality of my work area. Tidying relocates things, cleaning removes things.\r\nData is much the same. Tidying, to me, is about restructuring - getting the data in the best format for easy use. On the other hand, cleaning is about quality - removing empty columns and rows, getting rid of worthless or inaccurate information, trimming off parts that are no longer useful in its new format.\r\nThe HBCU Data\r\nWhat are the issues?\r\nLet’s read in the data and give it a look over.\r\nThe bach_students data set indicates…\r\n\r\nThe percentage of students broken down by race/ethnicity, aged 25 and over who have attained a bachelor’s degree.\r\n\r\nBut in the form it is in, it is pretty untidy and unclean. Here are some issues I can identify:\r\nOur key variables are effectively year, race, total and standard error, but race is spread over multiple columns (remember - each column should represent a variable, not multiple columns representing one variable).\r\nAll of the columns should be numbers, but they have read in as characters.\r\nAsian people and Pacific Islanders are given twice - as a total of the two and individually.\r\nThe names of the columns have spaces in them, are wordy and therefore generally difficult to use.\r\nThe Total column represents the year. The name is therefore a bit misleading.\r\nA note on the emboldened item above; in my experience, data “in the wild” that has been put together in Excel typically has baggage like this as spreadsheets are sadly used for both data storage and presentation. This means that totals and the like are given alongside the data used to calculate them. I don’t feel bad about removing data like this as long as we are left with the tools to recalculate them. In this case, we are! If we want to regroup the Asian/Pacific Islander data it’d be as easy as renaming them both as the same thing using mutate() and if_else(), and then group_by() and summarise() to re-calculate the total. It may even be that we want to combine more racial groups using a fct_lump() function.\r\nCleaning and Tidying\r\nLet’s start by making sure that all of the data is numeric. dplyr v1.0.0 gave us the across() function that makes this a breeze.\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 19\r\n   Total `Total, percent~ `Standard Error~ White1 `Standard Error~\r\n   <dbl>            <dbl>            <dbl>  <dbl>            <dbl>\r\n 1  1910              2.7            NA      NA              NA   \r\n 2  1920              3.3            NA      NA              NA   \r\n 3  1930              3.9            NA      NA              NA   \r\n 4  1940              4.6            NA       4.9            NA   \r\n 5  1950              6.2            NA       6.6            NA   \r\n 6  1960              7.7            NA       8.1            NA   \r\n 7  1970             11              NA      11.6            NA   \r\n 8  1975             13.9            NA      14.9            NA   \r\n 9  1980             17              -0.16   18.4            -0.18\r\n10  1985             19.4            -0.16   20.8            -0.19\r\n11  1986             19.4            -0.16   20.9            -0.19\r\n12  1987             19.9            -0.16   21.4            -0.19\r\n13  1988             20.3            -0.16   21.8            -0.19\r\n14  1989             21.1            -0.16   22.8            -0.19\r\n15  1990             21.3            -0.16   23.1            -0.19\r\n16  1991             21.4            -0.16   23.3            -0.19\r\n# ... with 14 more variables: Black1 <dbl>, `Standard Errors -\r\n#   Black1` <dbl>, Hispanic <dbl>, `Standard Errors -\r\n#   Hispanic` <dbl>, `Total - Asian/Pacific Islander` <dbl>,\r\n#   `Standard Errors - Total - Asian/Pacific Islander` <dbl>,\r\n#   `Asian/Pacific Islander - Asian` <dbl>, `Standard Errors -\r\n#   Asian/Pacific Islander - Asian` <dbl>, `Asian/Pacific Islander -\r\n#   Pacific Islander` <dbl>, `Standard Errors - Asian/Pacific\r\n#   Islander - Pacific Islander` <dbl>, `American Indian/\\r\\nAlaska\r\n#   Native` <dbl>, `Standard Errors - American Indian/\\r\\nAlaska\r\n#   Native` <dbl>, `Two or more race` <dbl>, `Standard Errors - Two\r\n#   or more race` <dbl>\r\n\r\nWe could be tempted now to run janitor::clean_names(). For those unfamiliar, this function cleans the names of a data frame to make them easier to use - all lower-case, spaces replaced with underscores, and the like. Normally it’s a good idea to use it as soon as possible, but as I am going to restructure my data I am not going to use it right away. As race will be one of my columns, I’d probably want words like “White” and “Black” to remain capitalised, so if I reported them in a table or use them in a legend label they’d look more presentable.\r\nNext, we can start restructuring. A good trick with pivot_longer() is that it behaves much the same as functions like select() - if we list a column name preceded with a minus sign (-) it effectively tells the function “everything but this column, please!” I’m not going to specify column names for the names or values here as we’ll quickly get rid of them.\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>% \r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 3\r\n   Total name                                                    value\r\n   <dbl> <chr>                                                   <dbl>\r\n 1  1910 \"Total, percent of all persons age 25 and over\"           2.7\r\n 2  1910 \"Standard Errors - Total, percent of all persons age 2~  NA  \r\n 3  1910 \"White1\"                                                 NA  \r\n 4  1910 \"Standard Errors - White1\"                               NA  \r\n 5  1910 \"Black1\"                                                 NA  \r\n 6  1910 \"Standard Errors - Black1\"                               NA  \r\n 7  1910 \"Hispanic\"                                               NA  \r\n 8  1910 \"Standard Errors - Hispanic\"                             NA  \r\n 9  1910 \"Total - Asian/Pacific Islander\"                         NA  \r\n10  1910 \"Standard Errors - Total - Asian/Pacific Islander\"       NA  \r\n11  1910 \"Asian/Pacific Islander - Asian\"                         NA  \r\n12  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"       NA  \r\n13  1910 \"Asian/Pacific Islander - Pacific Islander\"              NA  \r\n14  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Is~  NA  \r\n15  1910 \"American Indian/\\r\\nAlaska Native\"                      NA  \r\n16  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"    NA  \r\n\r\nNow we can remove that Asian/Pacific Islander total we spoke about earlier. We will use filter() alongside the str_detect() function and the logical operator !.\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 3\r\n   Total name                                                    value\r\n   <dbl> <chr>                                                   <dbl>\r\n 1  1910 \"Total, percent of all persons age 25 and over\"           2.7\r\n 2  1910 \"Standard Errors - Total, percent of all persons age 2~  NA  \r\n 3  1910 \"White1\"                                                 NA  \r\n 4  1910 \"Standard Errors - White1\"                               NA  \r\n 5  1910 \"Black1\"                                                 NA  \r\n 6  1910 \"Standard Errors - Black1\"                               NA  \r\n 7  1910 \"Hispanic\"                                               NA  \r\n 8  1910 \"Standard Errors - Hispanic\"                             NA  \r\n 9  1910 \"Asian/Pacific Islander - Asian\"                         NA  \r\n10  1910 \"Standard Errors - Asian/Pacific Islander - Asian\"       NA  \r\n11  1910 \"Asian/Pacific Islander - Pacific Islander\"              NA  \r\n12  1910 \"Standard Errors - Asian/Pacific Islander - Pacific Is~  NA  \r\n13  1910 \"American Indian/\\r\\nAlaska Native\"                      NA  \r\n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"    NA  \r\n15  1910 \"Two or more race\"                                       NA  \r\n16  1910 \"Standard Errors - Two or more race\"                     NA  \r\n\r\nTo separate the total values from the standard errors we can use the separate() function and the \" - \" string. Issues once again come from the Asian and Pacific Islander data as they have a second \" - \" in them, but this can be straightforwardly removed. For whatever reason, “White” and “Black” are listed with the number one (1) after them, so we can get rid of this while we’re on the subject of cleaning strings.\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 3\r\n   Total name                                                    value\r\n   <dbl> <chr>                                                   <dbl>\r\n 1  1910 \"Total, percent of all persons age 25 and over\"           2.7\r\n 2  1910 \"Standard Errors - Total, percent of all persons age 2~  NA  \r\n 3  1910 \"White\"                                                  NA  \r\n 4  1910 \"Standard Errors - White\"                                NA  \r\n 5  1910 \"Black\"                                                  NA  \r\n 6  1910 \"Standard Errors - Black\"                                NA  \r\n 7  1910 \"Hispanic\"                                               NA  \r\n 8  1910 \"Standard Errors - Hispanic\"                             NA  \r\n 9  1910 \"Asian\"                                                  NA  \r\n10  1910 \"Standard Errors - Asian\"                                NA  \r\n11  1910 \"Pacific Islander\"                                       NA  \r\n12  1910 \"Standard Errors - Pacific Islander\"                     NA  \r\n13  1910 \"American Indian/\\r\\nAlaska Native\"                      NA  \r\n14  1910 \"Standard Errors - American Indian/\\r\\nAlaska Native\"    NA  \r\n15  1910 \"Two or more race\"                                       NA  \r\n16  1910 \"Standard Errors - Two or more race\"                     NA  \r\n\r\nNow let’s add that separate() step, which we need to give some column names for the name column to turn into, the separating character (space-dash-space), and the direction to fill in (in this case left).\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\r\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 4\r\n   Total stat           race                                     value\r\n   <dbl> <chr>          <chr>                                    <dbl>\r\n 1  1910 <NA>           \"Total, percent of all persons age 25 a~   2.7\r\n 2  1910 Standard Erro~ \"Total, percent of all persons age 25 a~  NA  \r\n 3  1910 <NA>           \"White\"                                   NA  \r\n 4  1910 Standard Erro~ \"White\"                                   NA  \r\n 5  1910 <NA>           \"Black\"                                   NA  \r\n 6  1910 Standard Erro~ \"Black\"                                   NA  \r\n 7  1910 <NA>           \"Hispanic\"                                NA  \r\n 8  1910 Standard Erro~ \"Hispanic\"                                NA  \r\n 9  1910 <NA>           \"Asian\"                                   NA  \r\n10  1910 Standard Erro~ \"Asian\"                                   NA  \r\n11  1910 <NA>           \"Pacific Islander\"                        NA  \r\n12  1910 Standard Erro~ \"Pacific Islander\"                        NA  \r\n13  1910 <NA>           \"American Indian/\\r\\nAlaska Native\"       NA  \r\n14  1910 Standard Erro~ \"American Indian/\\r\\nAlaska Native\"       NA  \r\n15  1910 <NA>           \"Two or more race\"                        NA  \r\n16  1910 Standard Erro~ \"Two or more race\"                        NA  \r\n\r\nYou’ll notice that in the stats column we have some NA values that actually correspond to the “total” stat, so we’ll fill those in using a tidyr function, replace_na().\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\r\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\r\n  mutate(stat = replace_na(stat, \"Total\")) %>%\r\n  head(16)\r\n\r\n\r\n# A tibble: 16 x 4\r\n   Total stat           race                                     value\r\n   <dbl> <chr>          <chr>                                    <dbl>\r\n 1  1910 Total          \"Total, percent of all persons age 25 a~   2.7\r\n 2  1910 Standard Erro~ \"Total, percent of all persons age 25 a~  NA  \r\n 3  1910 Total          \"White\"                                   NA  \r\n 4  1910 Standard Erro~ \"White\"                                   NA  \r\n 5  1910 Total          \"Black\"                                   NA  \r\n 6  1910 Standard Erro~ \"Black\"                                   NA  \r\n 7  1910 Total          \"Hispanic\"                                NA  \r\n 8  1910 Standard Erro~ \"Hispanic\"                                NA  \r\n 9  1910 Total          \"Asian\"                                   NA  \r\n10  1910 Standard Erro~ \"Asian\"                                   NA  \r\n11  1910 Total          \"Pacific Islander\"                        NA  \r\n12  1910 Standard Erro~ \"Pacific Islander\"                        NA  \r\n13  1910 Total          \"American Indian/\\r\\nAlaska Native\"       NA  \r\n14  1910 Standard Erro~ \"American Indian/\\r\\nAlaska Native\"       NA  \r\n15  1910 Total          \"Two or more race\"                        NA  \r\n16  1910 Standard Erro~ \"Two or more race\"                        NA  \r\n\r\nNow we can pivot_wider() to get the total values and the standard errors in their own columns. We’ll have to rename the existing Total column first, but that can be achieved using dplyr’s rename().\r\n\r\n\r\nShow code\r\n\r\ntuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\r\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\r\n  mutate(stat = replace_na(stat, \"Total\")) %>%\r\n  rename(year = Total) %>%\r\n  pivot_wider(names_from = stat, values_from = value) %>%\r\n  head(8)\r\n\r\n\r\n# A tibble: 8 x 4\r\n   year race                                    Total `Standard Error~\r\n  <dbl> <chr>                                   <dbl>            <dbl>\r\n1  1910 \"Total, percent of all persons age 25 ~   2.7               NA\r\n2  1910 \"White\"                                  NA                 NA\r\n3  1910 \"Black\"                                  NA                 NA\r\n4  1910 \"Hispanic\"                               NA                 NA\r\n5  1910 \"Asian\"                                  NA                 NA\r\n6  1910 \"Pacific Islander\"                       NA                 NA\r\n7  1910 \"American Indian/\\r\\nAlaska Native\"      NA                 NA\r\n8  1910 \"Two or more race\"                       NA                 NA\r\n\r\nThe data is now tidy! We can do some additional cleaning steps now - there seems to be something odd going on with the American Indian/Alaska Native string, and the “Total” string is a bit wordy. Let’s sort that out, and finally throw in that janitor function I talked about right at the beginning.\r\n\r\n\r\nShow code\r\n\r\ndf = tuesdata$bach_students %>%\r\n  mutate(across(everything(), as.numeric)) %>%\r\n  pivot_longer(-Total) %>%\r\n  filter(!str_detect(name, \"Total - Asian/Pacific Islander\")) %>%\r\n  mutate(name = str_remove(name, \"Asian/Pacific Islander - |1\")) %>%\r\n  separate(name, into = c(\"stat\",\"race\"), sep = \" - \", fill = \"left\") %>%\r\n  mutate(stat = replace_na(stat, \"Total\")) %>%\r\n  rename(year = Total) %>%\r\n  pivot_wider(names_from = stat, values_from = value) %>%\r\n  janitor::clean_names() %>%\r\n  mutate(\r\n    race = str_remove_all(\r\n      race, \r\n      \", percent of all persons age 25 and over|\\r\\n\")\r\n  )\r\n\r\ndf %>% head(8)\r\n\r\n\r\n# A tibble: 8 x 4\r\n   year race                          total standard_errors\r\n  <dbl> <chr>                         <dbl>           <dbl>\r\n1  1910 Total                           2.7              NA\r\n2  1910 White                          NA                NA\r\n3  1910 Black                          NA                NA\r\n4  1910 Hispanic                       NA                NA\r\n5  1910 Asian                          NA                NA\r\n6  1910 Pacific Islander               NA                NA\r\n7  1910 American Indian/Alaska Native  NA                NA\r\n8  1910 Two or more race               NA                NA\r\n\r\nWe did it! The data is now clean and tidy and ready to use. Let’s do a bit of analysis just to demonstrate how straightforward it is to use now.\r\nData Analysis\r\nWe can create some cool plots now we have access to this data. We can start with a simple timeseries.\r\n\r\n\r\nShow code\r\n\r\nggthemr::ggthemr(\"flat\", text_size = 14)\r\n\r\nplot_data = df %>%\r\n  drop_na() %>%\r\n  mutate(across(total:standard_errors, ~.x/100))\r\n\r\nplot_data %>%\r\n  filter(race != \"Total\") %>%\r\n  mutate(race = fct_reorder(race, total, max, na.rm = T),\r\n         race = fct_rev(race)) %>%\r\n  ggplot(aes(\r\n    year,\r\n    y = total,\r\n    ymax = total + standard_errors,\r\n    ymin = total - standard_errors,\r\n    group = race\r\n  )) +\r\n  geom_ribbon(aes(fill = race), alpha = .25) +\r\n  geom_line(aes(color = race)) +\r\n  geom_line(data = plot_data %>% filter(race == \"Total\"), size = 2, color = ggthemr::swatch()[1]) +\r\n  scale_color_manual(values = ggthemr::swatch()[-1]) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(x = \"Year\", y = NULL, color = \"Race\", fill = \"Race\",\r\n       title = \"Bachelor's Degree Attainment\",\r\n       subtitle = \"The percentage of the population who have achieved a\\nbachelor's degree in the US since 1980, split into racial groups.\\nThe bold line represents the total population.\") +\r\n  theme(plot.title.position = \"plot\")\r\n\r\n\r\n\r\n\r\nOr we could just focus in on the most recent data.\r\n\r\n\r\nShow code\r\n\r\nplot_data_2 = plot_data %>%\r\n  filter(year == max(year)) %>%\r\n  mutate(race = fct_reorder(race, total))\r\n\r\ntot = plot_data_2 %>%\r\n  filter(race == \"Total\") %>% \r\n  pull(total)\r\n\r\nplot_data_2 %>%\r\n  mutate(flag = case_when(race == \"Total\" ~ \"T\",\r\n                          total > tot ~ \"Y\",\r\n                          total < tot ~ \"X\")) %>%\r\n  ggplot(aes(y = race, x = total, xmax = total+standard_errors, xmin = total-standard_errors, fill = flag)) +\r\n  geom_col(show.legend = F) +\r\n  geom_vline(xintercept = tot, color = ggthemr::swatch()[1], size = 1) +\r\n  geom_pointrange(show.legend = F) +\r\n  scale_x_continuous(expand = expansion(mult = c(0,.1)), labels = scales::percent) +\r\n  scale_fill_manual(values = ggthemr::swatch()[c(1,2,3)]) +\r\n  labs(y = NULL, x = \"Bachelor's Degree Attainment in 2016\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nplot_data_2 %>%\r\n  janitor::remove_constant() %>%\r\n  mutate(mutate(across(where(is.numeric), ~glue::glue(\"{abs(.x) * 100} %\")))) %>%\r\n  knitr::kable(col.names = c(\"Race\", \"Total\", \"Std Err.\")) %>%\r\n  kableExtra::kable_styling()\r\n\r\n\r\n\r\nRace\r\n\r\n\r\nTotal\r\n\r\n\r\nStd Err.\r\n\r\n\r\nTotal\r\n\r\n\r\n33.4 %\r\n\r\n\r\n0.24 %\r\n\r\n\r\nWhite\r\n\r\n\r\n37.3 %\r\n\r\n\r\n0.31 %\r\n\r\n\r\nBlack\r\n\r\n\r\n23.5 %\r\n\r\n\r\n0.46 %\r\n\r\n\r\nHispanic\r\n\r\n\r\n16.4 %\r\n\r\n\r\n0.4 %\r\n\r\n\r\nAsian\r\n\r\n\r\n56.4 %\r\n\r\n\r\n0.89 %\r\n\r\n\r\nPacific Islander\r\n\r\n\r\n27.5 %\r\n\r\n\r\n2.92 %\r\n\r\n\r\nAmerican Indian/Alaska Native\r\n\r\n\r\n16.8 %\r\n\r\n\r\n1.39 %\r\n\r\n\r\nTwo or more race\r\n\r\n\r\n30.6 %\r\n\r\n\r\n1.52 %\r\n\r\n\r\nSee how easy it is to use this data now? Incredible!\r\nConclusion\r\nIn this post we explored the ideas of data tidying and cleaning using a messy, unfamiliar data set. We showed how the tidyverse packages can help us achieve this goal easily and quickly, and demonstrated how easy it is to then go on and perform some basic data analysis on our newly tidied data.\r\nI use this sort of process all the time, especially when data has been ripped from Excel spreadsheets that are trying to store and display data at the same time - a recipe for disaster when it comes to reading into R!\r\nHopefully this post was interesting for newer R users, and I look forward to the visualisations that will come from this week’s TidyTuesday data sets!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-02-tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning/tidytuesday-2021-week-6-hbcu-enrollment-an-ode-to-data-cleaning_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-02-03T12:23:57+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-05-tidytuesday-2021-week-2-global-transit-costs/",
    "title": "TidyTuesday 2021 Week 2: Global Transit Costs",
    "description": "A Leaflet map of global transit costs on a per-city basis.",
    "author": [
      {
        "name": "Jack Davison",
        "url": {}
      }
    ],
    "date": "2021-01-05",
    "categories": [
      "tidytuesday"
    ],
    "contents": "\r\nThis week for TidyTuesday I produced a static map of distance-based road transit costs on a per-city basis. You can see it on my Twitter and the code will be pushed to my GitHub.\r\n\r\nTo compliment this, I thought I’d also try to go a step further and try to learn how to use Leaflet. I’ve long admired Leaflet and thought it would be a really hard job, but it’s actually really easy! The code and map are provided below, and you’ll note that the vast majority is just me cleaning and augmenting the data - only the last bit is the Leaflet code.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytuesdayR)\r\nlibrary(countrycode)\r\nlibrary(rnaturalearth)\r\nlibrary(leaflet)\r\nlibrary(htmltools)\r\n\r\nworld = ne_countries(scale = \"medium\", returnclass = \"sf\") %>%\r\n  filter(!continent %in% c(\"Antarctica\", \"Seven seas (open ocean)\"))\r\n\r\ncontinents = countrycode::codelist %>%\r\n  select(ecb, country.name.en, continent) %>%\r\n  rename(country = country.name.en)\r\n\r\ncity_locs = readr::read_csv(\"worldcities.csv\") %>%\r\n  select(city, country, lat, lng) %>%\r\n  mutate(country = if_else(country == \"Korea, South\", \"South Korea\", country))\r\n\r\ndata = tt_load(\"2021-01-05\")$transit_cost %>%\r\n  filter(!is.na(city)) %>%\r\n  rename(ecb = country) %>%\r\n  mutate(ecb = if_else(ecb == \"UK\", \"GB\", ecb)) %>%\r\n  left_join(continents, by = \"ecb\") %>%\r\n  left_join(city_locs, by = c(\"city\", \"country\")) %>%\r\n  filter(!is.na(lat)) %>%\r\n  mutate(start_year = as.numeric(start_year)) %>%\r\n  filter(start_year > 2000)\r\n\r\n\r\n\r\n    Downloading file 1 of 1: `transit_cost.csv`\r\n\r\navgs = data %>% \r\n  select(city, country, continent, lat, lng, cost_km_millions) %>%\r\n  group_by(city, country, continent, lat, lng) %>%\r\n  summarise(avg = median(cost_km_millions)) %>%\r\n  ungroup() %>%\r\n  as.data.frame()\r\n\r\nleaflet(avgs) %>%\r\n  addTiles() %>%\r\n  addCircles(data = avgs, lng = ~lng, lat = ~lat,\r\n             radius = ~round(avg,0) * 200) %>%\r\n  addMarkers(lng = ~lng, lat = ~lat, \r\n             clusterOptions = markerClusterOptions(),\r\n             popup = ~paste0(\"<b>\", city, \"<\/b>, \",country,\"<br>\",\r\n                             \"$\",round(avg,0),\" million / km\")\r\n  )\r\n\r\n\r\n\r\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"http://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"http://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addCircles\",\"args\":[[52.35,39.93,37.9794,-36.85,12.9699,13.75,41.3825,39.905,52.5167,43.2622,42.3188,42.6528,50.8467,44.4,47.4983,-34.5997,35.1,30.0561,43.9,22.3762,28.1987,30.6636,13.0825,18.7889,29.55,50.9422,55.6786,28.66,23.7161,25.3,23.0475,25.2697,50.1136,33.5903,26.0769,27.9814,46.2,20.6767,23.1288,26.5794,28.45,53.55,30.25,21.0245,31.8639,60.1756,10.8167,41.01,-6.2146,21.5428,22.6167,49.0167,50,9.9667,3.1478,25.0433,29.375,50.45,31.5497,36.0617,51.35,-12.05,38.7452,51.5072,34.1139,47.0523,45.76,40.4189,14.5958,21.4225,-37.8136,19.4333,45.4669,55.7558,18.9667,28.6842,32.05,22.8192,40.6943,56.3269,49.4528,59.9111,45.4247,9,48.8566,-31.9522,41.1495,50.0833,-0.2186,24.65,41.8931,59.95,37.7562,37.3019,-33.45,47.6211,37.5833,31.1667,41.8039,22.535,1.3,42.6975,59.3294,31.304,33.6333,-33.865,24.15,22.9833,25.0478,37.8733,41.3,35.7,39.1467,35.6897,43.7417,43.6045,45.0667,49.25,48.2083,52.2167,27.9991,30.5872,24.4797,34.7492],[4.9166,32.85,23.7161,174.7833,77.598,100.5167,2.1769,116.3914,13.3833,-2.9533,-71.0846,-78.7555,4.3517,26.0833,19.0408,-58.3819,129.0403,31.2394,125.2,112.6877,112.9709,104.0667,80.275,98.9833,106.5069,6.9578,12.5635,77.23,90.3961,51.5333,113.7493,55.3094,8.6797,130.4019,119.2917,116.3577,6.15,-103.3475,113.259,106.7078,77.02,10,120.1675,105.8412,117.2808,24.9342,106.6333,28.9603,106.8451,39.1728,120.3,8.4,36.2292,76.2833,101.6953,102.7061,47.98,30.5236,74.3436,103.8318,12.3833,-77.0333,-9.1604,-0.1275,-118.4068,8.3059,4.84,-3.6919,120.9772,39.8261,144.9631,-99.1333,9.19,37.6178,72.8333,115.8872,118.7667,108.315,-73.9249,44.0075,11.0778,10.7528,-75.695,-79.5,2.3522,115.8589,-8.6108,14.4167,-78.5097,46.71,12.4828,30.3167,-122.443,-121.8486,-70.6667,-122.3244,127,121.4667,123.4258,114.054,103.8,23.3241,18.0686,120.6164,116.9683,151.2094,120.6667,120.1833,121.5319,112.5425,69.2667,51.4167,117.2056,139.6922,-79.3733,1.444,7.7,-123.1,16.3731,21.0333,120.6561,114.2881,118.0819,113.6605],[83000,23800,26200,171000,41400,35400,25200,44200,54200,12200,60200,60200,53200,54000,96800,46400,27600,null,29000,36800,36800,36800,47800,27600,32200,67600,45000,38000,107800,145200,30600,60800,36000,73400,39400,39400,24200,31000,38800,31600,32600,35600,36200,57800,37200,17000,107000,22000,37400,21400,71000,79000,85200,41400,69800,30200,38000,49600,44600,35400,69400,63400,28000,144000,113800,20000,32000,18000,81400,40600,168600,50800,29000,44200,38400,38800,39400,36200,358400,36000,59600,28400,27800,40800,47000,30200,13400,49800,33400,43000,38000,59000,123800,143800,33600,68800,20800,32400,32800,45600,131400,25600,27600,33000,33000,50600,47200,47600,46800,35400,31600,40000,48000,64200,93000,25800,24800,83400,33200,39800,18600,36800,35800,29200],null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null,null]},{\"method\":\"addMarkers\",\"args\":[[52.35,39.93,37.9794,-36.85,12.9699,13.75,41.3825,39.905,52.5167,43.2622,42.3188,42.6528,50.8467,44.4,47.4983,-34.5997,35.1,30.0561,43.9,22.3762,28.1987,30.6636,13.0825,18.7889,29.55,50.9422,55.6786,28.66,23.7161,25.3,23.0475,25.2697,50.1136,33.5903,26.0769,27.9814,46.2,20.6767,23.1288,26.5794,28.45,53.55,30.25,21.0245,31.8639,60.1756,10.8167,41.01,-6.2146,21.5428,22.6167,49.0167,50,9.9667,3.1478,25.0433,29.375,50.45,31.5497,36.0617,51.35,-12.05,38.7452,51.5072,34.1139,47.0523,45.76,40.4189,14.5958,21.4225,-37.8136,19.4333,45.4669,55.7558,18.9667,28.6842,32.05,22.8192,40.6943,56.3269,49.4528,59.9111,45.4247,9,48.8566,-31.9522,41.1495,50.0833,-0.2186,24.65,41.8931,59.95,37.7562,37.3019,-33.45,47.6211,37.5833,31.1667,41.8039,22.535,1.3,42.6975,59.3294,31.304,33.6333,-33.865,24.15,22.9833,25.0478,37.8733,41.3,35.7,39.1467,35.6897,43.7417,43.6045,45.0667,49.25,48.2083,52.2167,27.9991,30.5872,24.4797,34.7492],[4.9166,32.85,23.7161,174.7833,77.598,100.5167,2.1769,116.3914,13.3833,-2.9533,-71.0846,-78.7555,4.3517,26.0833,19.0408,-58.3819,129.0403,31.2394,125.2,112.6877,112.9709,104.0667,80.275,98.9833,106.5069,6.9578,12.5635,77.23,90.3961,51.5333,113.7493,55.3094,8.6797,130.4019,119.2917,116.3577,6.15,-103.3475,113.259,106.7078,77.02,10,120.1675,105.8412,117.2808,24.9342,106.6333,28.9603,106.8451,39.1728,120.3,8.4,36.2292,76.2833,101.6953,102.7061,47.98,30.5236,74.3436,103.8318,12.3833,-77.0333,-9.1604,-0.1275,-118.4068,8.3059,4.84,-3.6919,120.9772,39.8261,144.9631,-99.1333,9.19,37.6178,72.8333,115.8872,118.7667,108.315,-73.9249,44.0075,11.0778,10.7528,-75.695,-79.5,2.3522,115.8589,-8.6108,14.4167,-78.5097,46.71,12.4828,30.3167,-122.443,-121.8486,-70.6667,-122.3244,127,121.4667,123.4258,114.054,103.8,23.3241,18.0686,120.6164,116.9683,151.2094,120.6667,120.1833,121.5319,112.5425,69.2667,51.4167,117.2056,139.6922,-79.3733,1.444,7.7,-123.1,16.3731,21.0333,120.6561,114.2881,118.0819,113.6605],null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},[\"<b>Amsterdam<\\/b>, Netherlands<br>$415 million / km\",\"<b>Ankara<\\/b>, Turkey<br>$119 million / km\",\"<b>Athens<\\/b>, Greece<br>$131 million / km\",\"<b>Auckland<\\/b>, New Zealand<br>$855 million / km\",\"<b>Bangalore<\\/b>, India<br>$207 million / km\",\"<b>Bangkok<\\/b>, Thailand<br>$177 million / km\",\"<b>Barcelona<\\/b>, Spain<br>$126 million / km\",\"<b>Beijing<\\/b>, China<br>$221 million / km\",\"<b>Berlin<\\/b>, Germany<br>$271 million / km\",\"<b>Bilbao<\\/b>, Spain<br>$61 million / km\",\"<b>Boston<\\/b>, United States<br>$301 million / km\",\"<b>Boston<\\/b>, United States<br>$301 million / km\",\"<b>Brussels<\\/b>, Belgium<br>$266 million / km\",\"<b>Bucharest<\\/b>, Romania<br>$270 million / km\",\"<b>Budapest<\\/b>, Hungary<br>$484 million / km\",\"<b>Buenos Aires<\\/b>, Argentina<br>$232 million / km\",\"<b>Busan<\\/b>, South Korea<br>$138 million / km\",\"<b>Cairo<\\/b>, Egypt<br>$NA million / km\",\"<b>Changchun<\\/b>, China<br>$145 million / km\",\"<b>Changsha<\\/b>, China<br>$184 million / km\",\"<b>Changsha<\\/b>, China<br>$184 million / km\",\"<b>Chengdu<\\/b>, China<br>$184 million / km\",\"<b>Chennai<\\/b>, India<br>$239 million / km\",\"<b>Chiang Mai<\\/b>, Thailand<br>$138 million / km\",\"<b>Chongqing<\\/b>, China<br>$161 million / km\",\"<b>Cologne<\\/b>, Germany<br>$338 million / km\",\"<b>Copenhagen<\\/b>, Denmark<br>$225 million / km\",\"<b>Delhi<\\/b>, India<br>$190 million / km\",\"<b>Dhaka<\\/b>, Bangladesh<br>$539 million / km\",\"<b>Doha<\\/b>, Qatar<br>$726 million / km\",\"<b>Dongguan<\\/b>, China<br>$153 million / km\",\"<b>Dubai<\\/b>, United Arab Emirates<br>$304 million / km\",\"<b>Frankfurt<\\/b>, Germany<br>$180 million / km\",\"<b>Fukuoka<\\/b>, Japan<br>$367 million / km\",\"<b>Fuzhou<\\/b>, China<br>$197 million / km\",\"<b>Fuzhou<\\/b>, China<br>$197 million / km\",\"<b>Geneva<\\/b>, Switzerland<br>$121 million / km\",\"<b>Guadalajara<\\/b>, Mexico<br>$155 million / km\",\"<b>Guangzhou<\\/b>, China<br>$194 million / km\",\"<b>Guiyang<\\/b>, China<br>$158 million / km\",\"<b>Gurgaon<\\/b>, India<br>$163 million / km\",\"<b>Hamburg<\\/b>, Germany<br>$178 million / km\",\"<b>Hangzhou<\\/b>, China<br>$181 million / km\",\"<b>Hanoi<\\/b>, Vietnam<br>$289 million / km\",\"<b>Hefei<\\/b>, China<br>$186 million / km\",\"<b>Helsinki<\\/b>, Finland<br>$85 million / km\",\"<b>Ho Chi Minh City<\\/b>, Vietnam<br>$535 million / km\",\"<b>Istanbul<\\/b>, Turkey<br>$110 million / km\",\"<b>Jakarta<\\/b>, Indonesia<br>$187 million / km\",\"<b>Jeddah<\\/b>, Saudi Arabia<br>$107 million / km\",\"<b>Kaohsiung<\\/b>, Taiwan<br>$355 million / km\",\"<b>Karlsruhe<\\/b>, Germany<br>$395 million / km\",\"<b>Kharkiv<\\/b>, Ukraine<br>$426 million / km\",\"<b>Kochi<\\/b>, India<br>$207 million / km\",\"<b>Kuala Lumpur<\\/b>, Malaysia<br>$349 million / km\",\"<b>Kunming<\\/b>, China<br>$151 million / km\",\"<b>Kuwait City<\\/b>, Kuwait<br>$190 million / km\",\"<b>Kyiv<\\/b>, Ukraine<br>$248 million / km\",\"<b>Lahore<\\/b>, Pakistan<br>$223 million / km\",\"<b>Lanzhou<\\/b>, China<br>$177 million / km\",\"<b>Leipzig<\\/b>, Germany<br>$347 million / km\",\"<b>Lima<\\/b>, Peru<br>$317 million / km\",\"<b>Lisbon<\\/b>, Portugal<br>$140 million / km\",\"<b>London<\\/b>, United Kingdom<br>$720 million / km\",\"<b>Los Angeles<\\/b>, United States<br>$569 million / km\",\"<b>Lucerne<\\/b>, Switzerland<br>$100 million / km\",\"<b>Lyon<\\/b>, France<br>$160 million / km\",\"<b>Madrid<\\/b>, Spain<br>$90 million / km\",\"<b>Manila<\\/b>, Philippines<br>$407 million / km\",\"<b>Mecca<\\/b>, Saudi Arabia<br>$203 million / km\",\"<b>Melbourne<\\/b>, Australia<br>$843 million / km\",\"<b>Mexico City<\\/b>, Mexico<br>$254 million / km\",\"<b>Milan<\\/b>, Italy<br>$145 million / km\",\"<b>Moscow<\\/b>, Russia<br>$221 million / km\",\"<b>Mumbai<\\/b>, India<br>$192 million / km\",\"<b>Nanchang<\\/b>, China<br>$194 million / km\",\"<b>Nanjing<\\/b>, China<br>$197 million / km\",\"<b>Nanning<\\/b>, China<br>$181 million / km\",\"<b>New York<\\/b>, United States<br>$1792 million / km\",\"<b>Nizhniy Novgorod<\\/b>, Russia<br>$180 million / km\",\"<b>Nuremberg<\\/b>, Germany<br>$298 million / km\",\"<b>Oslo<\\/b>, Norway<br>$142 million / km\",\"<b>Ottawa<\\/b>, Canada<br>$139 million / km\",\"<b>Panama City<\\/b>, Panama<br>$204 million / km\",\"<b>Paris<\\/b>, France<br>$235 million / km\",\"<b>Perth<\\/b>, Australia<br>$151 million / km\",\"<b>Porto<\\/b>, Portugal<br>$67 million / km\",\"<b>Prague<\\/b>, Czechia<br>$249 million / km\",\"<b>Quito<\\/b>, Ecuador<br>$167 million / km\",\"<b>Riyadh<\\/b>, Saudi Arabia<br>$215 million / km\",\"<b>Rome<\\/b>, Italy<br>$190 million / km\",\"<b>Saint Petersburg<\\/b>, Russia<br>$295 million / km\",\"<b>San Francisco<\\/b>, United States<br>$619 million / km\",\"<b>San Jose<\\/b>, United States<br>$719 million / km\",\"<b>Santiago<\\/b>, Chile<br>$168 million / km\",\"<b>Seattle<\\/b>, United States<br>$344 million / km\",\"<b>Seoul<\\/b>, South Korea<br>$104 million / km\",\"<b>Shanghai<\\/b>, China<br>$162 million / km\",\"<b>Shenyang<\\/b>, China<br>$164 million / km\",\"<b>Shenzhen<\\/b>, China<br>$228 million / km\",\"<b>Singapore<\\/b>, Singapore<br>$657 million / km\",\"<b>Sofia<\\/b>, Bulgaria<br>$128 million / km\",\"<b>Stockholm<\\/b>, Sweden<br>$138 million / km\",\"<b>Suzhou<\\/b>, China<br>$165 million / km\",\"<b>Suzhou<\\/b>, China<br>$165 million / km\",\"<b>Sydney<\\/b>, Australia<br>$253 million / km\",\"<b>Taichung<\\/b>, Taiwan<br>$236 million / km\",\"<b>Tainan<\\/b>, Taiwan<br>$238 million / km\",\"<b>Taipei<\\/b>, Taiwan<br>$234 million / km\",\"<b>Taiyuan<\\/b>, China<br>$177 million / km\",\"<b>Tashkent<\\/b>, Uzbekistan<br>$158 million / km\",\"<b>Tehran<\\/b>, Iran<br>$200 million / km\",\"<b>Tianjin<\\/b>, China<br>$240 million / km\",\"<b>Tokyo<\\/b>, Japan<br>$321 million / km\",\"<b>Toronto<\\/b>, Canada<br>$465 million / km\",\"<b>Toulouse<\\/b>, France<br>$129 million / km\",\"<b>Turin<\\/b>, Italy<br>$124 million / km\",\"<b>Vancouver<\\/b>, Canada<br>$417 million / km\",\"<b>Vienna<\\/b>, Austria<br>$166 million / km\",\"<b>Warsaw<\\/b>, Poland<br>$199 million / km\",\"<b>Wenzhou<\\/b>, China<br>$93 million / km\",\"<b>Wuhan<\\/b>, China<br>$184 million / km\",\"<b>Xiamen<\\/b>, China<br>$179 million / km\",\"<b>Zhengzhou<\\/b>, China<br>$146 million / km\"],null,{\"showCoverageOnHover\":true,\"zoomToBoundsOnClick\":true,\"spiderfyOnMaxZoom\":true,\"removeOutsideVisibleBounds\":true,\"spiderLegPolylineOptions\":{\"weight\":1.5,\"color\":\"#222\",\"opacity\":0.5},\"freezeAtZoom\":false},null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[-37.8136,60.1756],\"lng\":[-123.1,174.7833]}},\"evals\":[],\"jsHooks\":[]}\r\nSee you next week!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-05-tidytuesday-2021-week-2-global-transit-costs/transit.png",
    "last_modified": "2021-01-05T13:58:41+00:00",
    "input_file": {},
    "preview_width": 7583,
    "preview_height": 5381
  },
  {
    "path": "posts/2020-12-30-tutorial-calendar-plots-using-ggplot2/",
    "title": "Tutorial: Calendar Plots using ggplot2",
    "description": "A ggplot2 recreation of an uncommon heat-map visualisation.",
    "author": [
      {
        "name": "Jack Davison",
        "url": {}
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "tutorial",
      "ggplot2"
    ],
    "contents": "\r\n\r\nContents\r\nSetting the scene\r\nCreating a calendar plot in ggplot2\r\nThe grammar of… calendars?\r\nFormatting data for a calendar plot\r\nPlotting the calendar\r\n\r\nClosing thoughts, and calendars “in the wild”\r\n\r\nIf you’re less interested in the journey and just want the finished code, you can find it here.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\ncal_df = openair::mydata %>% \r\n  mutate(day   = day(date),\r\n         wday  = wday(date, label = T), \r\n         week  = week(date), \r\n         month = month(date, label = T, abbr = F), \r\n         year  = year(date),\r\n         wday = factor(wday, c(\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"))) %>%\r\n  group_by(month, year) %>%\r\n  mutate(week = week - min(week),\r\n         wu = ws * sin(2 * pi * wd/360),\r\n         wv = ws * cos(2 * pi * wd/360)) %>%\r\n  group_by(day, wday, week, month, year) %>%\r\n  summarise(across(where(is.numeric), mean, na.rm = T)) %>%\r\n  mutate(wd = (atan2(wu, wv) * 360/2/pi) + 180, .keep = \"unused\") %>%\r\n  ungroup()\r\n\r\ndummy = crossing(wday = unique(cal_df$wday),\r\n                 week = unique(cal_df$week),\r\n                 month = unique(cal_df$month))\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  \r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(data = dummy, fill = \"grey95\", color = \"white\") +\r\n  geom_tile(aes(fill = nox), color = \"white\") +\r\n  \r\n  geom_spoke(aes(angle = ((-wd+90)/360)*2*pi), radius = .4, arrow = arrow(length = unit(.1,\"cm\"))) +\r\n  geom_point(aes(size = ws)) +\r\n\r\n\r\n  facet_wrap(~month) +\r\n  \r\n  theme_void() +\r\n  theme(axis.text.x = element_text(),\r\n        plot.title.position = \"plot\", \r\n        plot.caption.position = \"plot\",\r\n        plot.title = element_text(size = 20),\r\n        plot.subtitle = element_text(size = 15, color = \"grey40\"),\r\n        plot.caption = element_text(size = 7, color = \"grey50\", hjust = 0)) +\r\n  \r\n  labs(fill = openair::quickText(\"NOx / ppb\"),\r\n       size = openair::quickText(\"Wind Speed / m/s\"),\r\n       title = \"Nitrogen Oxides in Marylebone, London\",\r\n       subtitle = \"How concentrations of nitrogen oxides changed through the year 2000.\\n\",\r\n       caption = \"\\nData from {openair} | Visualisation by Jack Davison (@JDavison_)\") +\r\n  \r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = c(\"S\",\"S\",\"M\",\"T\",\"W\",\"T\",\"F\")) +\r\n  \r\n  scale_fill_gradientn(colors = openair::openColours(scheme = \"heat\"), \r\n                       na.value = \"grey80\") +\r\n  scale_size(range = c(0,4)) +\r\n\r\n  coord_equal()\r\n\r\n\r\n\r\nSetting the scene\r\nCalendar plots are a relatively uncommon “heat-map” style visualisation, the key benefit of which is that they are an extremely readable and familiar format, even for folks less familiar with more esoteric chart types. After all, even your nan has a calendar up in her kitchen! As an air quality scientist I am most familiar with them from the openair package, which helpfully includes a dummy data-set named mydata and a purpose-built calendarPlot() function.\r\nLet’s first consider the data set, which is defined in the openair documentation as follows:\r\n\r\nThe mydata dataset is provided as an example dataset as part of the openair package. The dataset contains hourly measurements of air pollutant concentrations, wind speed and wind direction collected at the Marylebone (London) air quality monitoring supersite between 1st January 1998 and 23rd June 2005. The data set is a tibble.\r\n\r\nIt contains a date column which contains hourly date-times starting in January 1998, and a series of numeric columns. ws and wd refer to the wind speed and wind direction, and the columns nox through pm25 correspond to concentrations of the named pollutant.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\nopenair::mydata %>% glimpse()\r\n\r\n\r\nRows: 65,533\r\nColumns: 10\r\n$ date <dttm> 1998-01-01 00:00:00, 1998-01-01 01:00:00, 1998-01-0...\r\n$ ws   <dbl> 0.60, 2.16, 2.76, 2.16, 2.40, 3.00, 3.00, 3.00, 3.36...\r\n$ wd   <int> 280, 230, 190, 170, 180, 190, 140, 170, 170, 170, 18...\r\n$ nox  <int> 285, NA, NA, 493, 468, 264, 171, 195, 137, 113, 100,...\r\n$ no2  <int> 39, NA, NA, 52, 78, 42, 38, 51, 42, 39, 34, 38, 41, ...\r\n$ o3   <int> 1, NA, 3, 3, 2, 0, 0, 0, 1, 2, 7, 8, 9, 8, 9, 9, 12,...\r\n$ pm10 <int> 29, 37, 34, 35, 34, 16, 11, 12, 12, 12, 10, 11, 13, ...\r\n$ so2  <dbl> 4.7225, NA, 6.8300, 7.6625, 8.0700, 5.5050, 4.2300, ...\r\n$ co   <dbl> 3.3725, NA, 9.6025, 10.2175, 8.9125, 3.0525, 2.2650,...\r\n$ pm25 <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...\r\n\r\nTo keep things simple, let’s consider only the NOx data from the year 2000. In openair, the calendar plot will look like this. The colour of the boxes reflect NOx concentration in ppb, the direction of the arrows reflect wind direction, and the relative size of the arrows correspond to wind speed.\r\n\r\n\r\nopenair::mydata %>% \r\n  filter(lubridate::year(date) == 2000) %>%\r\n  openair::calendarPlot(mydata = ., annotate = \"ws\")\r\n\r\n\r\n\r\n\r\nopenair’s plotting functions were written in lattice before ggplot2 came on the scene. This is directly referenced in the preface of the openair book:\r\n\r\nopenair is a product of the time it was started and used the highly capable lattice package for plotting. This was a time before ggplot2 and the ‘tidyverse’.\r\n\r\nA “DIY” version of the calendar plot will allow for a greater amount of customisation. ggplot2 is also now much more widely used than lattice, so using the former package will hopefully be more comfortable for a greater number of people.\r\nI first realised it was possible to produce calendar plots using ggplot2 after reading Roy Francis’ blog post on the subject, but I intend to take the general outline further than Roy - who made an availability chart - to create an opeanir-style heatmap calendar.\r\nCreating a calendar plot in ggplot2\r\nThe grammar of… calendars?\r\nOn first glance it could be difficult to consider how to translate a calendar layout to a ggplot2 style, but looking to the lattice/openair plot we can translate its layout to ggplot2-style language.\r\nEach month is a facet\r\nThe day of the week (Mon, Tue, Wed, etc.) is the x-axis.\r\nThe week of the month (i.e. “Week 1-4”) is the y-axis.\r\nEach day of the year will be a distinct observation, plotted using a geometry function.\r\nThe pollutant concentration (or whatever else will be heat-mapped) is a fill aesthetic.\r\nTo put pen to paper in ggplot2 one needs to use geom_*() functions. We want to plot a square, which can be achieved by the (in my experience) rarely-used geom_tile(). A simple reproducible example of a “tile-plot” is shown below.\r\n\r\n\r\ncrossing(x = 1:5, y = 1:5) %>%\r\n  mutate(z = rnorm(n = 25)) %>%\r\n  ggplot(aes(x,y, fill = z)) + \r\n  geom_tile() +\r\n  coord_equal()\r\n\r\n\r\n\r\n\r\nFormatting data for a calendar plot\r\nWe need to restructure our data for a calendar plot. An obvious first step can be the extraction of various elements of the date column, which we can achieve using the lubridate package. It has an excellent set of functions that can easily rip apart date-times and label if needed using the label argument.\r\n\r\n\r\ncal_df = openair::mydata %>% \r\n  mutate(day   = day(date),\r\n         wday  = wday(date, label = T), \r\n         week  = week(date), \r\n         month = month(date, label = T, abbr = F), \r\n         year  = year(date))\r\n\r\n\r\n\r\nThere are two further mutations needed here that purely serve to improve the aesthetics of the plot.\r\nwday is turned into a factor starting on Saturday, much like the openair calendar.\r\nweek currently corresponds to the week of the year and not the week of the month, so we convert one to the other using an intermediary group_by().\r\n\r\n\r\ncal_df = cal_df %>% \r\n  mutate(wday = factor(wday, c(\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"))) %>%\r\n  group_by(month, year) %>%\r\n  mutate(week = week - min(week))\r\n\r\n\r\n\r\nThe final step is to generate a daily average, as our data’s time resolution is simply too high for a calendar. This is a straightforward group_by()-summarise() step. Here I use a dplyr v1.0.0 feature, across(), to calculate averages for all numeric values (i.e. all of our pollutants and wind speed). This means we can re-use this same data frame for multiple calendar plots if we so wish!\r\n A Note on Wind Direction  Wind direction is a complicated thing to average, as the wind is a vector - it has both a direction (on the compass) and a magnitude (it’s speed). Here I follow this technical note, taking the vector average of the wind direction and the scalar average of the wind speed.\r\n\r\n\r\ncal_df = cal_df %>%\r\n  mutate(wu = ws * sin(2 * pi * wd/360),\r\n         wv = ws * cos(2 * pi * wd/360)) %>%\r\n  group_by(day, wday, week, month, year) %>%\r\n  summarise(across(where(is.numeric), mean, na.rm = T)) %>%\r\n  mutate(wd = (atan2(wu, wv) * 360/2/pi) + 180, .keep = \"unused\") %>%\r\n  ungroup()\r\n\r\n\r\n\r\nLet’s have a look at our data frame now:\r\n\r\n\r\nglimpse(cal_df)\r\n\r\n\r\nRows: 2,731\r\nColumns: 14\r\n$ day   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ wday  <ord> Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, S...\r\n$ week  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\r\n$ month <ord> January, January, February, March, April, May, May,...\r\n$ year  <dbl> 2000, 2005, 2003, 2003, 2000, 1999, 2004, 2002, 200...\r\n$ ws    <dbl> 2.375000, 6.466667, 3.858333, 4.425000, 3.940000, 2...\r\n$ wd    <dbl> 60.5088499, 53.6412623, 64.5840029, 3.5529869, 342....\r\n$ nox   <dbl> 210.16667, 112.50000, 204.54167, 208.62500, 224.416...\r\n$ no2   <dbl> 41.45833, 48.08333, 55.75000, 65.37500, 50.70833, 5...\r\n$ o3    <dbl> 1.500000, 6.500000, 3.083333, 3.000000, 4.833333, 8...\r\n$ pm10  <dbl> 30.37500, 20.37500, 31.41667, 36.16667, 34.45833, 4...\r\n$ so2   <dbl> 7.104167, NaN, 4.211806, 4.513889, 3.809722, 5.1099...\r\n$ co    <dbl> 2.5078473, 0.7579024, 1.4006945, 1.5520833, 2.64027...\r\n$ pm25  <dbl> 24.80000, 10.45833, 14.62500, 16.79167, 26.00000, 2...\r\n\r\nPlotting the calendar\r\nThe most basic way to plot a calendar and check that the previous steps have worked correctly is shown below:\r\n\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(aes(fill = nox)) +\r\n  facet_wrap(~month) +\r\n  scale_y_reverse()\r\n\r\n\r\n\r\n\r\nThis is obviously really awful to look at, so over a few iterations we’ll refine it!\r\nThe default ggplot2 theme can be rather unappealing, so I will start by amending the theme and labelling of this plot. The steps I take are:\r\nUse theme_void(), then override it’s removal of the x axis text using theme().\r\nAdd some labels. In the case of the colourbar I use an openair function to format the subscript in NOx, but this naturally will not always be needed.\r\nAbbreviate the weekday labels using scale_x_discrete().\r\nProvide a coordinate system, coord_equal() to ensure the tiles are square.\r\nChange the colour scheme. Here I’ve ripped the openair “heat” colour scheme, but we could have provided anything.\r\n\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  \r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(aes(fill = nox)) +\r\n\r\n  facet_wrap(~month) +\r\n  \r\n  theme_void() +\r\n  theme(axis.text.x = element_text()) +\r\n  \r\n  labs(fill = openair::quickText(\"NOx / ppb\")) +\r\n  \r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = c(\"S\",\"S\",\"M\",\"T\",\"W\",\"T\",\"F\")) +\r\n  \r\n  scale_fill_gradientn(colors = openair::openColours(scheme = \"heat\"), \r\n                       na.value = \"grey80\", \r\n                       breaks = c(100,300,500)) +\r\n  \r\n  coord_equal()\r\n\r\n\r\n\r\n\r\nOne thing that looks rather weird is the unusual shapes that have emerged from using theme_void(). We can create some dummy data to “fill out” the rest of each facet. To do so, I use purrr’s crossing() function to create a “complete” grid of tiles for each combination of month, week and wday in the cal_df data frame. I then plot this data frame using geom_tile() first to provide a back-drop to our actual calendar plot. I use color = \"white\" for both geom_tile()s to add some contrast between them.\r\n\r\n\r\ndummy = crossing(wday = unique(cal_df$wday),\r\n                 week = unique(cal_df$week),\r\n                 month = unique(cal_df$month))\r\n\r\n\r\n\r\n\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  \r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(data = dummy, fill = \"grey95\", color = \"white\") +\r\n  geom_tile(aes(fill = nox), color = \"white\") +\r\n\r\n  facet_wrap(~month) +\r\n  \r\n  theme_void() +\r\n  theme(axis.text.x = element_text()) +\r\n  \r\n  labs(fill = openair::quickText(\"NOx / ppb\")) +\r\n  \r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = c(\"S\",\"S\",\"M\",\"T\",\"W\",\"T\",\"F\")) +\r\n  \r\n  scale_fill_gradientn(colors = openair::openColours(scheme = \"heat\"), \r\n                       na.value = \"grey80\", \r\n                       breaks = c(100,300,500)) +\r\n  \r\n  coord_equal()\r\n\r\n\r\n\r\n\r\nA clear ommission from the original openair plot is that we have yet to visualise wind speed or direction. Thankfully, ggplot2 provides geom_spoke() - another geometry I do not find myself using particularly often. Because geom_spoke() uses the cos() and sin() functions to calculate angles, our wd column needs a little playing with.\r\nTo my knowledge, geom_spoke() also pivots around a centre point and cannot be readily extended backwards like the tail of a weathercock, as is the visualisation of the openair calendar. This means that a potentailly wide range of wind speeds will be represented by a relatively tiny change in the length of the arrow. Instead I choose to use a point, the size of which will be relative to wind speed instead.\r\n\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  \r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(data = dummy, fill = \"grey95\", color = \"white\") +\r\n  geom_tile(aes(fill = nox), color = \"white\") +\r\n  \r\n  geom_spoke(aes(angle = ((-wd+90)/360)*2*pi), radius = .4, arrow = arrow(length = unit(.1,\"cm\"))) +\r\n  geom_point(aes(size = ws)) +\r\n\r\n\r\n  facet_wrap(~month) +\r\n  \r\n  theme_void() +\r\n  theme(axis.text.x = element_text()) +\r\n  \r\n  labs(fill = openair::quickText(\"NOx / ppb\"),\r\n       size = openair::quickText(\"Wind Speed / m/s\")) +\r\n  \r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = c(\"S\",\"S\",\"M\",\"T\",\"W\",\"T\",\"F\")) +\r\n  \r\n  scale_fill_gradientn(colors = openair::openColours(scheme = \"heat\"), \r\n                       na.value = \"grey80\") +\r\n  scale_size(range = c(0,4)) +\r\n\r\n  coord_equal()\r\n\r\n\r\n\r\n\r\nNow it’s “ready to go” I’ll just throw a title on it for some-last minute theme-ing.\r\n\r\n\r\ncal_df %>%\r\n  filter(year == 2000) %>%\r\n  \r\n  ggplot(aes(x = wday, y = week)) +\r\n  geom_tile(data = dummy, fill = \"grey95\", color = \"white\") +\r\n  geom_tile(aes(fill = nox), color = \"white\") +\r\n  \r\n  geom_spoke(aes(angle = ((-wd+90)/360)*2*pi), radius = .4, arrow = arrow(length = unit(.1,\"cm\"))) +\r\n  geom_point(aes(size = ws)) +\r\n\r\n\r\n  facet_wrap(~month) +\r\n  \r\n  theme_void() +\r\n  theme(axis.text.x = element_text(),\r\n        plot.title.position = \"plot\", \r\n        plot.caption.position = \"plot\",\r\n        plot.title = element_text(size = 20),\r\n        plot.subtitle = element_text(size = 15, color = \"grey40\"),\r\n        plot.caption = element_text(size = 7, color = \"grey50\", hjust = 0)) +\r\n  \r\n  labs(fill = openair::quickText(\"NOx / ppb\"),\r\n       size = openair::quickText(\"Wind Speed / m/s\"),\r\n       title = \"Nitrogen Oxides in Marylebone, London\",\r\n       subtitle = \"How concentrations of nitrogen oxides changed through the year 2000.\\n\",\r\n       caption = \"\\nData from {openair} | Visualisation by Jack Davison (@JDavison_)\") +\r\n  \r\n  scale_y_reverse() +\r\n  scale_x_discrete(labels = c(\"S\",\"S\",\"M\",\"T\",\"W\",\"T\",\"F\")) +\r\n  \r\n  scale_fill_gradientn(colors = openair::openColours(scheme = \"heat\"), \r\n                       na.value = \"grey80\") +\r\n  scale_size(range = c(0,4)) +\r\n\r\n  coord_equal()\r\n\r\n\r\n\r\n\r\nClosing thoughts, and calendars “in the wild”\r\nCalendar plots are relatively niche and their applications are pretty limited - they can effectively only plot time series data at a daily resolution. At a higher resolution you need to average or sum and lose detail, and at a lower resolution (e.g. weekly, monthly, yearly data) you cannot calendarify the data in the way we have here.\r\nWhile openair is designed for air quality data, there is no reason to necessarily restrict ourselves to data from just even the wider umbrella of environmental science. For example, in a recent #TidyTuesday challenge about homelessness I noted some rather interesting calendar plots!\r\nThere was this plot, similar to what I have laid out here:\r\n\r\n\r\nToday's #TidyTuesday plot, looking at the occupancy rate of shelters in Toronto in 2019. Coming into the winter months, donations are needed now more than ever.More information here: https://t.co/pJpgVzyaE3 pic.twitter.com/k93B9EsezQ\r\n\r\n— Henry Wakefield (@henrywrover2) December 1, 2020\r\n\r\nA “GitHub-style” calendar:\r\n\r\n\r\nGot back on the #rstats #TidyTuesday train this week looking at Toronto Homeless Shelters. I made a github-style calendar showing the number of programs at or above capacity on each day. I can see some seasonal trend (esp in 2018), but not as much as I was expecting pic.twitter.com/gYnlbLQldy\r\n\r\n— Amanda Luby, PhD (@amandaluby) December 2, 2020\r\n\r\nAnd even a “calendar in disguise” which has all the elements of a calendar plot, but uses a different categorical variable (in this case a shelter provider) in place of the day of the year.\r\n\r\n\r\nToronto shelters for this week’s #TidyTuesday code: https://t.co/QK77DGL8g1#dataviz #RStats pic.twitter.com/zhNeuieP5W\r\n\r\n— Georgios Karamanis (@geokaramanis) December 2, 2020\r\n\r\nSo perhaps calendar plots aren’t going to be useful for every project, but they’re a useful one to have in your back pocket!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-30-tutorial-calendar-plots-using-ggplot2/tutorial-calendar-plots-using-ggplot2_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2020-12-31T19:47:43+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 1152
  },
  {
    "path": "posts/2020-12-29-creating-a-function-counting-coins/",
    "title": "The Minimum Coins Problem in Base R",
    "description": "An R-based solution to a common programming challenge - for a given value of money, what is the minimum number of coins to make the change?",
    "author": [
      {
        "name": "Jack Davison",
        "url": {}
      }
    ],
    "date": "2020-12-29",
    "categories": [
      "tutorial"
    ],
    "contents": "\r\n\r\nContents\r\nSetting the scene\r\nWriting a basic function\r\nExpanding the function\r\nReturning the list of coins\r\nAllowing for similar currencies\r\nThe curious case of the Canadian Dollar\r\nSaving users from themselves\r\n\r\nInsights and Visualisations\r\nClosing Thoughts\r\n\r\nSetting the scene\r\nThe internet is awash with common programming “interview-style” challenges, perhaps most famously “fizzbuzz”. A problem that I recently became aware of was the “minimum coins” puzzle, defined as such:\r\n\r\nGiven a value V, if we want to make change for V cents, and we have infinite supply of each of C = { C1, C2, .. , Cm} valued coins, what is the minimum number of coins to make the change?\r\n\r\nThe linked blog features answers for many languages - C++, Java, Python, etc. - but R is a notable omission. So I thought I’d try to generate a complementary answer in R - and go a bit further!\r\nWriting a basic function\r\nWhen I write a function, I try to write the most basic workable version of it first. My thoughts going into this were as follows:\r\nAs a first step, we’ll need to define the coins we’ll be using. As I’m British, I’ll use GBP.\r\nWe’ll then need to find the greatest value coin that itself is greater than the value, V, which we can call B.\r\nOnce we have identified B, we’ll need to subtract B from V and repeat from Step 2 until V = 0.\r\nWe’ll need to count each iteration of Steps 2-3 and return that counted value.\r\nMy attempt is given below:\r\n\r\n\r\ncount_coins = function(v){\r\n  \r\n  coins = c(1, 2, 5, 10, 20, 50, 100, 200)\r\n\r\n  result = c()\r\n  \r\n  while(v > 0){\r\n    \r\n    b = max(coins[coins <= v])\r\n    \r\n    result = append(result, b)\r\n    \r\n    v = v - b\r\n    \r\n  }\r\n  \r\n  return(length(result))\r\n\r\n}\r\n\r\n\r\n\r\nHere the available coins are stored as a vector named coins. I’ve chosen not to use decimals (i.e. 2p = 2, not .02) for a reason that will become apparent later. I then have an empty results vector which will become populated with our coins.\r\nI then use the R function while(), which loops until the condition inside is no longer met. In this case, I have specified that while V is greater than 0, the contents within should loop. Those contents identify the greatest value in the coins vector that’s greater than or equal to V, add it to our results vector, and then subtract it from V.\r\nThe function finally returns the length of the results vector. So now, in theory, we can provide our function any values in pence and it’ll return the minimum number of coins you’d need. So for example, if we use V = 8 we’d expect 3 coins - 5p + 2p + 1p.\r\n\r\n\r\ncount_coins(v = 8)\r\n\r\n\r\n[1] 3\r\n\r\nAs the UK has 10p coins, V = 10 should return 1 coin.\r\n\r\n\r\ncount_coins(v = 10)\r\n\r\n\r\n[1] 1\r\n\r\nWe could also throw it a larger amount of money - let’s say £5.12 (V = 512). I would predict £2 + £2 + £1 + 10p + 2p = 5 coins.\r\n\r\n\r\ncount_coins(v = 512)\r\n\r\n\r\n[1] 5\r\n\r\nLooking good!\r\nExpanding the function\r\nReturning the list of coins\r\nThis would hopefully tick whatever box the interviewer was looking to tick, but we can do better. If we were a frequent counter of coins, we may want additional features. For example, we may want an option to return the actual coins rather than simply the number of them. This will also be a good double-check that the function isn’t doing anything wacky behind the scenes and just happening upon the correct answer! It is also a pretty easy addition:\r\n\r\n\r\ncount_coins = function(v, return_coins = F){\r\n  \r\n  coins = c(1, 2, 5, 10, 20, 50, 100, 200)\r\n\r\n  result = c()\r\n  \r\n  while(v > 0){\r\n    \r\n    b = max(coins[coins <= v])\r\n    \r\n    result = append(result, b)\r\n    \r\n    v = v - b\r\n    \r\n  }\r\n  \r\n  if (return_coins) {\r\n    return(result)\r\n  } else {\r\n    return(length(result))\r\n  }\r\n  \r\n\r\n}\r\n\r\n\r\n\r\nLet’s revisit the arbitrary value of £5.12.\r\n\r\n\r\ncount_coins(v = 512, return_coins = F)\r\n\r\n\r\n[1] 5\r\n\r\ncount_coins(v = 512, return_coins = T)\r\n\r\n\r\n[1] 200 200 100  10   2\r\n\r\nAllowing for similar currencies\r\nAnother obvious addition could be the addition of different currencies other than GBP. There are two ways we could approach this:\r\nWe could allow users to enter their own vector of coins.\r\nWe could allow users to enter a currency code and have these values built-in, e.g. “GBP”, “USD”, “JPY”, etc.\r\nThere is no reason, however, to not do both! Such is the flexibility of writing functions in R. I will add a feature that checks to see if the user has entered a character string or numeric vector.\r\nIf the user has entered a character string, it will check it against the hard-coded coin values and give a warning if whatever they have specified is not included.\r\nIf the user has entered a numeric vector, it will set the coins vector to be equal to it. To avoid infinite loops, it will also append the number 1.\r\nFor the sake of this example, I’m going to hard-code the Great British Pound, the Euro, the US Dollar, and the Japanese Yen as four common currencies. This is the reason I did not use decimals - the Japanese Yen does not, and it would only serve to introduce awkwardness if we want to make comparisons between the different coin types (spoilers: that is coming next!).\r\n\r\n\r\ncount_coins = function(v, return_coins = F, coin_vals = \"GBP\"){\r\n  \r\n  if (is.character(coin_vals)) {\r\n    if (coin_vals %in% c(\"GBP\", \"EUR\")) {\r\n      coins = c(1, 2, 5, 10, 20, 50, 100, 200)\r\n    } else if (coin_vals == \"USD\") {\r\n      coins = c(1, 5, 10, 25, 50, 100)\r\n    } else if (coin_vals == \"JPY\") {\r\n      coins = c(1, 5, 10, 50, 100, 500)\r\n    } else {\r\n      warning(\r\n        paste0(\r\n          \"The currency '\",\r\n          coin_vals,\r\n          \"' is not supported. Supported currencies are GBP, EUR, USD, and JPY.\r\n          Alternatively, provide a vector of numeric coin values.\"\r\n        )\r\n      )\r\n      stop()\r\n    }} else if (is.numeric(coin_vals)) {\r\n    coins = append(coin_vals,1)\r\n  }\r\n  \r\n  result = c()\r\n\r\n  while(v > 0){\r\n\r\n    b = max(coins[coins <= v])\r\n\r\n    result = append(result, b)\r\n\r\n    v = v - b\r\n\r\n  }\r\n\r\n  if (return_coins) {\r\n    return(result)\r\n  } else {\r\n    return(length(result))\r\n  }\r\n\r\n\r\n}\r\n\r\n\r\n\r\nLet’s try a few examples:\r\n\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = \"EUR\")\r\n\r\n\r\n[1] 200 200 100  10   2\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = \"USD\")\r\n\r\n\r\n[1] 100 100 100 100 100  10   1   1\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = \"JPY\")\r\n\r\n\r\n[1] 500  10   1   1\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = c(5,500))\r\n\r\n\r\n[1] 500   5   5   1   1\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = \"CAD\")\r\n\r\n\r\nWarning in count_coins(v = 512, return_coins = T, coin_vals = \"CAD\"): The currency 'CAD' is not supported. Supported currencies are GBP, EUR, USD, and JPY.\r\n          Alternatively, provide a vector of numeric coin values.\r\nError in count_coins(v = 512, return_coins = T, coin_vals = \"CAD\"): \r\n\r\nThe curious case of the Canadian Dollar\r\nIn Canada and Australia the decision was made to stop minting coins below a value of 5¢. This is very sensible, but introduces an interesting problem for the function - currently, to avoid infinite recursion, we append a 1¢ (or 1p, or whatever) coin to any list - but this won’t actually reflect the situation in Canada or Australia. The Wikipedia article on the Abolition of the Canadaian penny writes:\r\n\r\nCash transactions are now rounded to the nearest 5¢. The rounding is not done on individual items but on the total bill of sale, with totals ending in 1, 2, 6, or 7 rounded down to 0 or 5, and totals ending in 3, 4, 8, or 9 rounded up.\r\n\r\nThe solution to this issue could be more complicated than it needs to be. If the question that this function serves to answer is “If I have V Canadian cents in my bank account and convert that to cash, how many coins are now in my wallet?” then the value of V we use within the while() loop will have to be some value that is divisible by five.\r\nTo overcome this, we could simply round V down to the nearest multiple of five using the floor() function, which is similar to round() but always rounds down. Rather than hard-coding a value of 5, I can just identify the smallest unit of currency inside of the coins vector and use that - this allows for any coins to be used. Note that this means I can get rid of appending the 1¢ coin to a user-defined numeric vector.\r\n\r\n\r\ncount_coins = function(v, return_coins = F, coin_vals = \"GBP\"){\r\n  \r\n  if (is.character(coin_vals)) {\r\n    if (coin_vals %in% c(\"GBP\", \"EUR\")) {\r\n      coins = c(1, 2, 5, 10, 20, 50, 100, 200)\r\n    } else if (coin_vals == \"USD\") {\r\n      coins = c(1, 5, 10, 25, 50, 100)\r\n    } else if (coin_vals == \"JPY\") {\r\n      coins = c(1, 5, 10, 50, 100, 500)\r\n    } else if (coin_vals == \"CAD\") {\r\n      coins = c(5, 10, 25, 100, 200)\r\n    } else if (coin_vals == \"AUD\") {\r\n      coins = c(5, 10, 20, 50, 100, 200)\r\n    } else {\r\n      warning(\r\n        paste0(\r\n          \"The currency '\",\r\n          coin_vals,\r\n          \"' is not supported. Supported currencies are GBP, EUR, USD, CAD, AUD and JPY.\r\n          Alternatively, provide a vector of numeric coin values.\"\r\n        )\r\n      )\r\n      stop()\r\n      \r\n  }} else if (is.numeric(coin_vals)) {\r\n    coins = coin_vals\r\n  }\r\n  \r\n  v = floor(v/min(coins))*min(coins)\r\n\r\n  result = c()\r\n\r\n  while(v > 0){\r\n\r\n    b = max(coins[coins <= v])\r\n    \r\n    result = append(result, b)\r\n\r\n    v = v - b\r\n\r\n  }\r\n\r\n  if (return_coins) {\r\n    return(result)\r\n  } else {\r\n    return(length(result))\r\n  }\r\n\r\n\r\n}\r\n\r\n\r\n\r\nLet’s test this out:\r\n\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = \"CAD\")\r\n\r\n\r\n[1] 200 200 100  10\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = c(10, 20, 60))\r\n\r\n\r\n [1] 60 60 60 60 60 60 60 60 20 10\r\n\r\nSaving users from themselves\r\nThis seems to work well, but has highlighted a danger of allowing the manual input of coin values - infinite recursion. For example, the following will loop infinitely:\r\n\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = c(9, 23, 64))\r\n\r\n\r\n\r\nI have intuited (perhaps incorrectly!) that each coin value needs to be divisible by the lowest coin value to work. So this is an appropriate list of coins and will not loop forever:\r\n\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = c(9, 27, 63))\r\n\r\n\r\n[1] 63 63 63 63 63 63 63 63\r\n\r\nOne way we can stop the function breaking is introducing a check using all() and the modulus operator %% which stops the function before the loop.\r\n\r\n\r\ncount_coins = function(v, return_coins = F, coin_vals = \"GBP\"){\r\n  \r\n  if (is.character(coin_vals)) {\r\n    if (coin_vals %in% c(\"GBP\", \"EUR\")) {\r\n      coins = c(1, 2, 5, 10, 20, 50, 100, 200)\r\n    } else if (coin_vals == \"USD\") {\r\n      coins = c(1, 5, 10, 25, 50, 100)\r\n    } else if (coin_vals == \"JPY\") {\r\n      coins = c(1, 5, 10, 50, 100, 500)\r\n    } else if (coin_vals == \"CAD\") {\r\n      coins = c(5, 10, 25, 100, 200)\r\n    } else if (coin_vals == \"AUD\") {\r\n      coins = c(5, 10, 20, 50, 100, 200)\r\n    } else {\r\n      warning(\r\n        paste0(\r\n          \"The currency '\",\r\n          coin_vals,\r\n          \"' is not supported. Supported currencies are GBP, EUR, USD, CAD, AUD and JPY.\r\n          Alternatively, provide a vector of numeric coin values.\"\r\n        )\r\n      )\r\n      stop()\r\n      \r\n  }} else if (is.numeric(coin_vals)) {\r\n    coins = coin_vals\r\n  }\r\n  \r\n  v = floor(v/min(coins))*min(coins)\r\n  \r\n  if (!all(coins %% min(coins) == 0)) {\r\n    warning(paste(\"Ensure all coins are divisible by\",\r\n                  min(coins),\r\n                  \"to avoid infinite recursion.\"))\r\n    stop()\r\n  }\r\n\r\n  result = c()\r\n\r\n  while(v > 0){\r\n\r\n    b = max(coins[coins <= v])\r\n    \r\n    if (is.null(b)) {\r\n      stop()\r\n    }\r\n\r\n    result = append(result, b)\r\n\r\n    v = v - b\r\n\r\n  }\r\n\r\n  if (return_coins) {\r\n    return(result)\r\n  } else {\r\n    return(length(result))\r\n  }\r\n\r\n\r\n}\r\n\r\n\r\n\r\nOur dodgy set of coins should now fail rather than go on indefinitely:\r\n\r\n\r\ncount_coins(v = 512, return_coins = T, coin_vals = c(9, 23, 64))\r\n\r\n\r\nWarning in count_coins(v = 512, return_coins = T, coin_vals = c(9, 23,\r\n64)): Ensure all coins are divisible by 9 to avoid infinite recursion.\r\nError in count_coins(v = 512, return_coins = T, coin_vals = c(9, 23, 64)): \r\n\r\nInsights and Visualisations\r\nWith a more fully featured function like this, we can gain some fairly pointless insights about how the values of coins affect the numbers of coins you actually have in your wallet. I’m going to create some visualisations using ggplot2 and the tidyverse. For ease, I’m going to pre-set my theme using the ggthemr package.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(ggthemr)\r\n\r\nggthemr::ggthemr(palette = \"flat\",\r\n                 text_size = 14,\r\n                 layout = \"scientific\")\r\n\r\n\r\n\r\nLet’s use our function on values from 1 to 5000 units of currency (i.e. 1p to £50, 1¢ to $50, and ¥1 to ¥5000). We’ll use the dplyr function rowwise() to facilitate this.\r\n\r\n\r\ndf = data.frame(val = 1:5000) %>%\r\n  rowwise() %>%\r\n  mutate(coin_gbp = count_coins(v = val, coin_vals = \"GBP\"),\r\n         coin_usd = count_coins(v = val, coin_vals = \"USD\"),\r\n         coin_jpy = count_coins(v = val, coin_vals = \"JPY\"))\r\n\r\n\r\n\r\nLet’s plot our findings. This should let us see which set of coin values will give us the fewest number of physical coins on our person. NB: There is currently a bug in the most recent version of ggthemr; the last line should not be necessary in future!\r\n\r\n\r\ndf %>%\r\n  pivot_longer(contains(\"coin\"),\r\n               names_to = \"currency\",\r\n               values_to = \"number\") %>%\r\n  mutate(currency = toupper(str_remove_all(currency, \"coin_\"))) %>%\r\n  ggplot(aes(x = val, y = number, color = currency)) + \r\n  geom_point(alpha = .05) +\r\n  geom_smooth(se = F) +\r\n  theme(\r\n    plot.title.position = \"plot\",\r\n    plot.caption.position = \"plot\",\r\n    legend.position = \"top\", \r\n    plot.margin = unit(rep(1,4), \"cm\")\r\n  ) +\r\n  labs(x = \"Amount of Money\", y = \"Number of Coins\",\r\n       title = \"Currency to Coinage\",\r\n       subtitle = \"How the same decimal amount of money affects the number of coins in your wallet.\",\r\n       caption = \"Visualisation by Jack Davison (@JDavison_)\",\r\n       color = \"\") +\r\n  scale_color_manual(values = ggthemr::swatch()) \r\n\r\n\r\n\r\n\r\nThe results are perhaps unsurprising. The US lacks a large coin, so for larger monetary values the number of coins rapidly increase. The GBP lies in the middle, having both a £1 and £2 coin. Of most interest is the JPY - the largest coins are ¥100 and ¥500 - ¥400 between them. This large gap magnifies a repeating pattern of a large drop every 500 units of currency.\r\nThese relationships appear to be roughly linear, so we could do some linear modelling to transform one currency into the other. The real world use of this is extremely limited, but it answers the question of “if you had X coins in the US, the US suddenly abandoned the 25¢ coin in favour of a 20¢ one and gained a $2 coin, how many coins would you likely have instead?”, a question I am sure has plagued economists for centuries!\r\n\r\n\r\neq = df %>%\r\n  pivot_longer(coin_usd:coin_jpy) %>%\r\n  mutate(name = if_else(name == \"coin_usd\", \"US Dollars/Cents\", \"Japanese Yen\")) %>%\r\n  nest_by(name) %>%\r\n  mutate(mod = list(lm(coin_gbp ~ value, data = data))) %>%\r\n  summarise(broom::tidy(mod)) %>%\r\n  select(name, term, estimate) %>%\r\n  pivot_wider(names_from = term, values_from = estimate) %>%\r\n  janitor::clean_names() %>%\r\n  mutate(equation = glue::glue(\"y = {round(intercept,2)} + {round(value,2)}x\"))\r\n\r\ndf %>%\r\n  pivot_longer(coin_usd:coin_jpy) %>%\r\n  mutate(name = if_else(name == \"coin_usd\", \"US Dollars/Cents\", \"Japanese Yen\")) %>%\r\n  ggplot(aes(x = value, y = coin_gbp)) +\r\n  geom_abline(color = swatch()[1], lty = 2, size = 1) +\r\n  geom_count(aes(color = name), alpha = .5) +\r\n  geom_smooth(method = \"lm\", color = swatch()[5]) +\r\n  labs(x = \"Number of Local Coins\", \r\n       y = \"Number of GB Coins\",\r\n       title = \"Coinage: What If?\",\r\n       subtitle = \"What if the US or Japan had coins more like the UK?\",\r\n       caption = \"Visualisation by Jack Davison (@JDavison_)\") +\r\n  geom_text(data = eq, inherit.aes = F, aes(label = equation, x = 10, y = 32.5), size = 4,\r\n            color = swatch()[5]) +\r\n  facet_grid(~name, space = \"free\", scales = \"free_x\", switch = \"x\") +\r\n  theme(legend.position = \"top\", \r\n        plot.title.position = \"plot\",\r\n        plot.caption.position = \"plot\",\r\n        strip.placement = \"outside\", \r\n        plot.margin = unit(rep(1,4), \"cm\")) +\r\n  guides(color = F) +\r\n  scale_x_continuous(breaks = seq(0,60,10)) +\r\n  scale_color_manual(values = ggthemr::swatch()) \r\n\r\n\r\n\r\n\r\nSo in this magical scenario where the global community suddenly decides to mint coins in a GBP-style, a US citizen walking around with 10 coins in his pocket would suddenly find himself with only 7. Conversely, a Japanese person would likely find themselves with 14!\r\nClosing Thoughts\r\nThis was simply an exercise in solving a basic programming challenge in Base R, taking the problem further, and then seeing if anything insightful can be seen (answer: not really anything that insightful!).\r\nIs there anything else that could be added to such a function? And is the way of dealing with AUD and CAD the best approach?\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-29-creating-a-function-counting-coins/creating-a-function-counting-coins_files/figure-html5/unnamed-chunk-17-1.png",
    "last_modified": "2021-01-05T14:22:25+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 1152
  }
]
